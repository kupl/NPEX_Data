{
    "hadoop-common_006ffd5": {
        "bug_id": "hadoop-common_006ffd5",
        "commit": "https://github.com/apache/hadoop-common/commit/006ffd5207ff585047ed8bb522e5e516997397f5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=006ffd5207ff585047ed8bb522e5e516997397f5",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -179,6 +179,8 @@ Release 2.1.2 - UNRELEASED\n     YARN-1273. Fixed Distributed-shell to account for containers that failed\n     to start. (Hitesh Shah via vinodkv)\n \n+    YARN-1032. Fixed NPE in RackResolver. (Lohit Vijayarenu via acmurthy)\n+\n Release 2.1.1-beta - 2013-09-23\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/CHANGES.txt",
                "sha": "0c3a0307fbf19277a23271d1b9cc3981adc9398f",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java?ref=006ffd5207ff585047ed8bb522e5e516997397f5",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.net.CachedDNSToSwitchMapping;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n+import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.Node;\n import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.net.ScriptBasedMapping;\n@@ -98,8 +99,15 @@ private static Node coreResolve(String hostName) {\n     List <String> tmpList = new ArrayList<String>(1);\n     tmpList.add(hostName);\n     List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);\n-    String rName = rNameList.get(0);\n-    LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n+    String rName = null;\n+    if (rNameList == null || rNameList.get(0) == null) {\n+      rName = NetworkTopology.DEFAULT_RACK;\n+      LOG.info(\"Couldn't resolve \" + hostName + \". Falling back to \"\n+          + NetworkTopology.DEFAULT_RACK);\n+    } else {\n+      rName = rNameList.get(0);\n+      LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n+    }\n     return new NodeBase(hostName, rName);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "sha": "cc2a56c3be6819cfaa5a05b6ccc38a0b54079607",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java?ref=006ffd5207ff585047ed8bb522e5e516997397f5",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "patch": "@@ -28,13 +28,16 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n+import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.Node;\n import org.junit.Assert;\n import org.junit.Test;\n \n public class TestRackResolver {\n \n   private static Log LOG = LogFactory.getLog(TestRackResolver.class);\n+  private static final String invalidHost = \"invalidHost\";\n+\n \n   public static final class MyResolver implements DNSToSwitchMapping {\n \n@@ -50,6 +53,11 @@\n       if (hostList.isEmpty()) {\n         return returnList;\n       }\n+      if (hostList.get(0).equals(invalidHost)) {\n+        // Simulate condition where resolving host returns null\n+        return null; \n+      }\n+        \n       LOG.info(\"Received resolve request for \"\n           + hostList.get(0));\n       if (hostList.get(0).equals(\"host1\")\n@@ -90,6 +98,8 @@ public void testCaching() {\n     Assert.assertEquals(\"/rack1\", node.getNetworkLocation());\n     node = RackResolver.resolve(\"host1\");\n     Assert.assertEquals(\"/rack1\", node.getNetworkLocation());\n+    node = RackResolver.resolve(invalidHost);\n+    Assert.assertEquals(NetworkTopology.DEFAULT_RACK, node.getNetworkLocation());\n   }\n \n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "sha": "70ca23c3a2e0b97fd7100ce0519947a806b43a79",
                "status": "modified"
            }
        ],
        "message": "YARN-1032. Fixed NPE in RackResolver. Contributed by Lohit Vijayarenu.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529534 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/692a361f14da347dc06a2fc924516be9943dbdc5",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRackResolver.java"
        ]
    },
    "hadoop-common_0432804": {
        "bug_id": "hadoop-common_0432804",
        "commit": "https://github.com/apache/hadoop-common/commit/04328044e122dbd0c44e534a99a01b05fa43021d",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=04328044e122dbd0c44e534a99a01b05fa43021d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -174,6 +174,7 @@ Release 0.23.2 - UNRELEASED\n     (sharad, todd via todd)\n \n   BUG FIXES\n+    HADOOP-8054 NPE with FilterFileSystem (Daryn Sharp via bobby)\n \n     HADOOP-8042  When copying a file out of HDFS, modifying it, and uploading\n     it back into HDFS, the put fails due to a CRC mismatch",
                "raw_url": "https://github.com/apache/hadoop-common/raw/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "a199c6e1b88bf677994949ae87c9cf694836c645",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java?ref=04328044e122dbd0c44e534a99a01b05fa43021d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "patch": "@@ -80,6 +80,11 @@ public FileSystem getRawFileSystem() {\n    */\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n+    // this is less than ideal, but existing filesystems sometimes neglect\n+    // to initialize the embedded filesystem\n+    if (fs.getConf() == null) {\n+      fs.initialize(name, conf);\n+    }\n     String scheme = name.getScheme();\n     if (!scheme.equals(fs.getUri().getScheme())) {\n       swapScheme = scheme;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "sha": "91ee2ae710566d54b4950413f17e01051ca0f618",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java?ref=04328044e122dbd0c44e534a99a01b05fa43021d",
                "deletions": 7,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "patch": "@@ -48,13 +48,6 @@ public LocalFileSystem(FileSystem rawLocalFileSystem) {\n     super(rawLocalFileSystem);\n   }\n     \n-  @Override\n-  public void initialize(URI uri, Configuration conf) throws IOException {\n-    super.initialize(uri, conf);\n-    // ctor didn't initialize the filtered fs\n-    getRawFileSystem().initialize(uri, conf);\n-  }\n-  \n   /** Convert a path to a File. */\n   public File pathToFile(Path path) {\n     return ((RawLocalFileSystem)fs).pathToFile(path);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "sha": "4aff81114b9befe0dc7daf6677c91e65c09356db",
                "status": "modified"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/hadoop-common/blob/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "changes": 125,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java?ref=04328044e122dbd0c44e534a99a01b05fa43021d",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "patch": "@@ -18,24 +18,39 @@\n \n package org.apache.hadoop.fs;\n \n+import static org.junit.Assert.*;\n+import static org.mockito.Matchers.*;\n+import static org.mockito.Mockito.*;\n+\n import java.io.IOException;\n import java.lang.reflect.Method;\n import java.lang.reflect.Modifier;\n+import java.net.URI;\n import java.util.EnumSet;\n import java.util.Iterator;\n \n-import junit.framework.TestCase;\n import org.apache.commons.logging.Log;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.fs.Options.CreateOpts;\n import org.apache.hadoop.fs.Options.Rename;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.util.Progressable;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n \n-public class TestFilterFileSystem extends TestCase {\n+public class TestFilterFileSystem {\n \n   private static final Log LOG = FileSystem.LOG;\n+  private static final Configuration conf = new Configuration();\n \n+  @BeforeClass\n+  public static void setup() {\n+    conf.set(\"fs.flfs.impl\", FilterLocalFileSystem.class.getName());\n+    conf.setBoolean(\"fs.flfs.impl.disable.cache\", true);\n+    conf.setBoolean(\"fs.file.impl.disable.cache\", true);\n+  }\n+  \n   public static class DontCheck {\n     public BlockLocation[] getFileBlockLocations(Path p, \n         long start, long len) { return null; }\n@@ -153,6 +168,7 @@ public void primitiveMkdir(Path f, FsPermission absolutePermission,\n     \n   }\n   \n+  @Test\n   public void testFilterFileSystem() throws Exception {\n     for (Method m : FileSystem.class.getDeclaredMethods()) {\n       if (Modifier.isStatic(m.getModifiers()))\n@@ -176,4 +192,109 @@ public void testFilterFileSystem() throws Exception {\n     }\n   }\n   \n+  @Test\n+  public void testFilterEmbedInit() throws Exception {\n+    FileSystem mockFs = createMockFs(false); // no conf = need init\n+    checkInit(new FilterFileSystem(mockFs), true);\n+  }\n+\n+  @Test\n+  public void testFilterEmbedNoInit() throws Exception {\n+    FileSystem mockFs = createMockFs(true); // has conf = skip init\n+    checkInit(new FilterFileSystem(mockFs), false);\n+  }\n+\n+  @Test\n+  public void testLocalEmbedInit() throws Exception {\n+    FileSystem mockFs = createMockFs(false); // no conf = need init\n+    checkInit(new LocalFileSystem(mockFs), true);\n+  }  \n+  \n+  @Test\n+  public void testLocalEmbedNoInit() throws Exception {\n+    FileSystem mockFs = createMockFs(true); // has conf = skip init\n+    checkInit(new LocalFileSystem(mockFs), false);\n+  }\n+  \n+  private FileSystem createMockFs(boolean useConf) {\n+    FileSystem mockFs = mock(FileSystem.class);\n+    when(mockFs.getUri()).thenReturn(URI.create(\"mock:/\"));\n+    when(mockFs.getConf()).thenReturn(useConf ? conf : null);\n+    return mockFs;\n+  }\n+\n+  @Test\n+  public void testGetLocalFsSetsConfs() throws Exception {\n+    LocalFileSystem lfs = FileSystem.getLocal(conf);\n+    checkFsConf(lfs, conf, 2);\n+  }\n+\n+  @Test\n+  public void testGetFilterLocalFsSetsConfs() throws Exception {\n+    FilterFileSystem flfs =\n+        (FilterFileSystem) FileSystem.get(URI.create(\"flfs:/\"), conf);\n+    checkFsConf(flfs, conf, 3);\n+  }\n+\n+  @Test\n+  public void testInitLocalFsSetsConfs() throws Exception {\n+    LocalFileSystem lfs = new LocalFileSystem();\n+    checkFsConf(lfs, null, 2);\n+    lfs.initialize(lfs.getUri(), conf);\n+    checkFsConf(lfs, conf, 2);\n+  }\n+\n+  @Test\n+  public void testInitFilterFsSetsEmbedConf() throws Exception {\n+    LocalFileSystem lfs = new LocalFileSystem();\n+    checkFsConf(lfs, null, 2);\n+    FilterFileSystem ffs = new FilterFileSystem(lfs);\n+    assertEquals(lfs, ffs.getRawFileSystem());\n+    checkFsConf(ffs, null, 3);\n+    ffs.initialize(URI.create(\"filter:/\"), conf);\n+    checkFsConf(ffs, conf, 3);\n+  }\n+\n+  @Test\n+  public void testInitFilterLocalFsSetsEmbedConf() throws Exception {\n+    FilterFileSystem flfs = new FilterLocalFileSystem();\n+    assertEquals(LocalFileSystem.class, flfs.getRawFileSystem().getClass());\n+    checkFsConf(flfs, null, 3);\n+    flfs.initialize(URI.create(\"flfs:/\"), conf);\n+    checkFsConf(flfs, conf, 3);\n+  }\n+\n+  private void checkInit(FilterFileSystem fs, boolean expectInit)\n+      throws Exception {\n+    URI uri = URI.create(\"filter:/\");\n+    fs.initialize(uri, conf);\n+    \n+    FileSystem embedFs = fs.getRawFileSystem();\n+    if (expectInit) {\n+      verify(embedFs, times(1)).initialize(eq(uri), eq(conf));\n+    } else {\n+      verify(embedFs, times(0)).initialize(any(URI.class), any(Configuration.class));\n+    }\n+  }\n+\n+  // check the given fs's conf, and all its filtered filesystems\n+  private void checkFsConf(FileSystem fs, Configuration conf, int expectDepth) {\n+    int depth = 0;\n+    while (true) {\n+      depth++; \n+      assertFalse(\"depth \"+depth+\">\"+expectDepth, depth > expectDepth);\n+      assertEquals(conf, fs.getConf());\n+      if (!(fs instanceof FilterFileSystem)) {\n+        break;\n+      }\n+      fs = ((FilterFileSystem) fs).getRawFileSystem();\n+    }\n+    assertEquals(expectDepth, depth);\n+  }\n+  \n+  private static class FilterLocalFileSystem extends FilterFileSystem {\n+    FilterLocalFileSystem() {\n+      super(new LocalFileSystem());\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "sha": "364f46d2a40652beb7d20a27c3755376ba95d9b8",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8054. NPE with FilterFileSystem (Daryn Sharp via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1245637 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/585a0bf5128d1477d06b6cb708aedfebc6dc4b04",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFilterFileSystem.java",
            "TestLocalFileSystem.java"
        ]
    },
    "hadoop-common_0727aa6": {
        "bug_id": "hadoop-common_0727aa6",
        "commit": "https://github.com/apache/hadoop-common/commit/0727aa6e8cb2499562ae1d35f7a4e5a923083571",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0727aa6e8cb2499562ae1d35f7a4e5a923083571/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=0727aa6e8cb2499562ae1d35f7a4e5a923083571",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -163,6 +163,9 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    MAPREDUCE-2539. Fixed NPE in getMapTaskReports in JobClient. (Robert Evans via\n+    acmurthy) \n+\n     MAPREDUCE-2531. Fixed jobcontrol to downgrade JobID. (Robert Evans via\n     acmurthy) \n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0727aa6e8cb2499562ae1d35f7a4e5a923083571/CHANGES.txt",
                "sha": "493d13037e920b46fb0b710e743a3eaf1d5f3b8f",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=0727aa6e8cb2499562ae1d35f7a4e5a923083571",
                "deletions": 20,
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -576,6 +576,8 @@ public RunningJob getJob(String jobid) throws IOException {\n     return getJob(JobID.forName(jobid));\n   }\n   \n+  private static final TaskReport[] EMPTY_TASK_REPORTS = new TaskReport[0];\n+  \n   /**\n    * Get the information of the current state of the map tasks of a job.\n    * \n@@ -584,9 +586,16 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */\n   public TaskReport[] getMapTaskReports(JobID jobId) throws IOException {\n+    return getTaskReports(jobId, TaskType.MAP);\n+  }\n+  \n+  private TaskReport[] getTaskReports(JobID jobId, TaskType type) throws IOException {\n     try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.MAP));\n+      Job j = cluster.getJob(jobId);\n+      if(j == null) {\n+        return EMPTY_TASK_REPORTS;\n+      }\n+      return TaskReport.downgradeArray(j.getTaskReports(type));\n     } catch (InterruptedException ie) {\n       throw new IOException(ie);\n     }\n@@ -606,12 +615,7 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */    \n   public TaskReport[] getReduceTaskReports(JobID jobId) throws IOException {\n-    try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.REDUCE));\n-    } catch (InterruptedException ie) {\n-      throw new IOException(ie);\n-    }\n+    return getTaskReports(jobId, TaskType.REDUCE);\n   }\n \n   /**\n@@ -622,12 +626,7 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */    \n   public TaskReport[] getCleanupTaskReports(JobID jobId) throws IOException {\n-    try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.JOB_CLEANUP));\n-    } catch (InterruptedException ie) {\n-      throw new IOException(ie);\n-    }\n+    return getTaskReports(jobId, TaskType.JOB_CLEANUP);\n   }\n \n   /**\n@@ -638,12 +637,7 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */    \n   public TaskReport[] getSetupTaskReports(JobID jobId) throws IOException {\n-    try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.JOB_SETUP));\n-    } catch (InterruptedException ie) {\n-      throw new IOException(ie);\n-    }\n+    return getTaskReports(jobId, TaskType.JOB_SETUP);\n   }\n \n   ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "245e362866fe446422f09c9425a1f3e6651e7879",
                "status": "modified"
            },
            {
                "additions": 94,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java?ref=0727aa6e8cb2499562ae1d35f7a4e5a923083571",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "patch": "@@ -0,0 +1,94 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.mapred;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import org.apache.hadoop.mapreduce.Cluster;\n+import org.junit.Test;\n+\n+public class JobClientUnitTest {\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testMapTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getMapTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testReduceTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getReduceTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testSetupTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getSetupTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testCleanupTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getCleanupTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "sha": "11873c14233814e67b9a766c486e4f8b775edd10",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-2539. Fixed NPE in getMapTaskReports in JobClient. Contributed by Robert Evans.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1130994 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/34fc2ec906ba5602d4a65b0bb2ec9fc7c4cb855f",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobClient.java"
        ]
    },
    "hadoop-common_076d285": {
        "bug_id": "hadoop-common_076d285",
        "commit": "https://github.com/apache/hadoop-common/commit/076d285ac838dbae1ee28b93a4b7e492cc31f4ff",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=076d285ac838dbae1ee28b93a4b7e492cc31f4ff",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -558,6 +558,9 @@ Release 0.23.4 - UNRELEASED\n     MAPREDUCE-4691. Historyserver can report \"Unknown job\" after RM says job\n     has completed (Robert Joseph Evans via jlowe)\n \n+    MAPREDUCE-4689. JobClient.getMapTaskReports on failed job results in NPE\n+    (jlowe via bobby)\n+\n Release 0.23.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "9e98ac96e1d250ab9b6265c43322bbd37c6f80f3",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java?ref=076d285ac838dbae1ee28b93a4b7e492cc31f4ff",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "patch": "@@ -42,6 +42,8 @@\n \n public class CompletedTask implements Task {\n \n+  private static final Counters EMPTY_COUNTERS = new Counters();\n+\n   private final TaskId taskId;\n   private final TaskInfo taskInfo;\n   private TaskReport report;\n@@ -124,7 +126,11 @@ private void constructTaskReport() {\n     report.setFinishTime(taskInfo.getFinishTime());\n     report.setTaskState(getState());\n     report.setProgress(getProgress());\n-    report.setCounters(TypeConverter.toYarn(getCounters()));\n+    Counters counters = getCounters();\n+    if (counters == null) {\n+      counters = EMPTY_COUNTERS;\n+    }\n+    report.setCounters(TypeConverter.toYarn(counters));\n     if (successfulAttempt != null) {\n       report.setSuccessfulAttempt(successfulAttempt);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "sha": "830b64f1ad364a1d9982e3dad1b0bd37f2175732",
                "status": "modified"
            },
            {
                "additions": 78,
                "blob_url": "https://github.com/apache/hadoop-common/blob/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "changes": 78,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java?ref=076d285ac838dbae1ee28b93a4b7e492cc31f4ff",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "patch": "@@ -49,6 +49,7 @@\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n import org.apache.hadoop.mapreduce.v2.api.records.JobState;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId;\n+import org.apache.hadoop.mapreduce.v2.api.records.TaskId;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskState;\n import org.apache.hadoop.mapreduce.v2.app.MRApp;\n import org.apache.hadoop.mapreduce.v2.app.job.Job;\n@@ -402,6 +403,63 @@ public void testHistoryParsingForFailedAttempts() throws Exception {\n     }\n   }\n   \n+  @Test\n+  public void testCountersForFailedTask() throws Exception {\n+    LOG.info(\"STARTING testCountersForFailedTask\");\n+    try {\n+    Configuration conf = new Configuration();\n+    conf\n+        .setClass(\n+            CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,\n+            MyResolver.class, DNSToSwitchMapping.class);\n+    RackResolver.init(conf);\n+    MRApp app = new MRAppWithHistoryWithFailedTask(2, 1, true,\n+        this.getClass().getName(), true);\n+    app.submit(conf);\n+    Job job = app.getContext().getAllJobs().values().iterator().next();\n+    JobId jobId = job.getID();\n+    app.waitForState(job, JobState.FAILED);\n+\n+    // make sure all events are flushed\n+    app.waitForState(Service.STATE.STOPPED);\n+\n+    String jobhistoryDir = JobHistoryUtils\n+        .getHistoryIntermediateDoneDirForUser(conf);\n+    JobHistory jobHistory = new JobHistory();\n+    jobHistory.init(conf);\n+\n+    JobIndexInfo jobIndexInfo = jobHistory.getJobFileInfo(jobId)\n+        .getJobIndexInfo();\n+    String jobhistoryFileName = FileNameIndexUtils\n+        .getDoneFileName(jobIndexInfo);\n+\n+    Path historyFilePath = new Path(jobhistoryDir, jobhistoryFileName);\n+    FSDataInputStream in = null;\n+    FileContext fc = null;\n+    try {\n+      fc = FileContext.getFileContext(conf);\n+      in = fc.open(fc.makeQualified(historyFilePath));\n+    } catch (IOException ioe) {\n+      LOG.info(\"Can not open history file: \" + historyFilePath, ioe);\n+      throw (new Exception(\"Can not open History File\"));\n+    }\n+\n+    JobHistoryParser parser = new JobHistoryParser(in);\n+    JobInfo jobInfo = parser.parse();\n+    Exception parseException = parser.getParseException();\n+    Assert.assertNull(\"Caught an expected exception \" + parseException,\n+        parseException);\n+    for (Map.Entry<TaskID,TaskInfo> entry : jobInfo.getAllTasks().entrySet()) {\n+      TaskId yarnTaskID = TypeConverter.toYarn(entry.getKey());\n+      CompletedTask ct = new CompletedTask(yarnTaskID, entry.getValue());\n+      Assert.assertNotNull(\"completed task report has null counters\",\n+          ct.getReport().getCounters());\n+    }\n+    } finally {\n+      LOG.info(\"FINISHED testCountersForFailedTask\");\n+    }\n+  }\n+\n   static class MRAppWithHistoryWithFailedAttempt extends MRAppWithHistory {\n \n     public MRAppWithHistoryWithFailedAttempt(int maps, int reduces, boolean autoComplete,\n@@ -422,6 +480,26 @@ protected void attemptLaunched(TaskAttemptId attemptID) {\n     }\n   }\n \n+  static class MRAppWithHistoryWithFailedTask extends MRAppWithHistory {\n+\n+    public MRAppWithHistoryWithFailedTask(int maps, int reduces, boolean autoComplete,\n+        String testName, boolean cleanOnStart) {\n+      super(maps, reduces, autoComplete, testName, cleanOnStart);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Override\n+    protected void attemptLaunched(TaskAttemptId attemptID) {\n+      if (attemptID.getTaskId().getId() == 0) {\n+        getContext().getEventHandler().handle(\n+            new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_FAILMSG));\n+      } else {\n+        getContext().getEventHandler().handle(\n+            new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_DONE));\n+      }\n+    }\n+  }\n+\n   public static void main(String[] args) throws Exception {\n     TestJobHistoryParsing t = new TestJobHistoryParsing();\n     t.testHistoryParsing();",
                "raw_url": "https://github.com/apache/hadoop-common/raw/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "sha": "f9acb1a38212081cd4aa43beb959dc58d3cd310d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4689. JobClient.getMapTaskReports on failed job results in NPE (jlowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1391679 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/9d91daeefc9e42ed5848f214c5774bdde6c9c6bb",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestCompletedTask.java"
        ]
    },
    "hadoop-common_0933689": {
        "bug_id": "hadoop-common_0933689",
        "commit": "https://github.com/apache/hadoop-common/commit/09336893d22e678afe600dd606849df36812af7e",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 3,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -220,9 +220,6 @@ Release 0.23.3 - UNRELEASED\n     HADOOP-8163. Improve ActiveStandbyElector to provide hooks for\n     fencing old active. (todd)\n \n-    HADOOP-8193. Refactor FailoverController/HAAdmin code to add an abstract\n-    class for \"target\" services. (todd)\n-\n   OPTIMIZATIONS\n \n   BUG FIXES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "f3fda7668c5ee52db3dacbecaf62f2f9cfa5b5bd",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "patch": "@@ -19,16 +19,11 @@\n \n import java.io.IOException;\n \n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.classification.InterfaceStability;\n-\n /**\n  * Indicates that the operator has specified an invalid configuration\n  * for fencing methods.\n  */\n-@InterfaceAudience.Public\n-@InterfaceStability.Evolving\n-public class BadFencingConfigurationException extends IOException {\n+class BadFencingConfigurationException extends IOException {\n   private static final long serialVersionUID = 1L;\n \n   public BadFencingConfigurationException(String msg) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "sha": "3d3b1ba53cca506a3ef8d28ade71a18c25747777",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 22,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.ha;\n \n import java.io.IOException;\n+import java.net.InetSocketAddress;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -50,21 +51,21 @@\n    * allow it to become active, eg because it triggers a log roll\n    * so the standby can learn about new blocks and leave safemode.\n    *\n-   * @param target service to make active\n+   * @param toSvc service to make active\n+   * @param toSvcName name of service to make active\n    * @param forceActive ignore toSvc if it reports that it is not ready\n    * @throws FailoverFailedException if we should avoid failover\n    */\n-  private static void preFailoverChecks(HAServiceTarget target,\n+  private static void preFailoverChecks(HAServiceProtocol toSvc,\n+                                        InetSocketAddress toSvcAddr,\n                                         boolean forceActive)\n       throws FailoverFailedException {\n     HAServiceStatus toSvcStatus;\n-    HAServiceProtocol toSvc;\n \n     try {\n-      toSvc = target.getProxy();\n       toSvcStatus = toSvc.getServiceStatus();\n     } catch (IOException e) {\n-      String msg = \"Unable to get service state for \" + target;\n+      String msg = \"Unable to get service state for \" + toSvcAddr;\n       LOG.error(msg, e);\n       throw new FailoverFailedException(msg, e);\n     }\n@@ -78,7 +79,7 @@ private static void preFailoverChecks(HAServiceTarget target,\n       String notReadyReason = toSvcStatus.getNotReadyReason();\n       if (!forceActive) {\n         throw new FailoverFailedException(\n-            target + \" is not ready to become active: \" +\n+            toSvcAddr + \" is not ready to become active: \" +\n             notReadyReason);\n       } else {\n         LOG.warn(\"Service is not ready to become active, but forcing: \" +\n@@ -102,72 +103,77 @@ private static void preFailoverChecks(HAServiceTarget target,\n    * then try to failback.\n    *\n    * @param fromSvc currently active service\n+   * @param fromSvcAddr addr of the currently active service\n    * @param toSvc service to make active\n+   * @param toSvcAddr addr of the service to make active\n+   * @param fencer for fencing fromSvc\n    * @param forceFence to fence fromSvc even if not strictly necessary\n    * @param forceActive try to make toSvc active even if it is not ready\n    * @throws FailoverFailedException if the failover fails\n    */\n-  public static void failover(HAServiceTarget fromSvc,\n-                              HAServiceTarget toSvc,\n+  public static void failover(HAServiceProtocol fromSvc,\n+                              InetSocketAddress fromSvcAddr,\n+                              HAServiceProtocol toSvc,\n+                              InetSocketAddress toSvcAddr,\n+                              NodeFencer fencer,\n                               boolean forceFence,\n                               boolean forceActive)\n       throws FailoverFailedException {\n-    Preconditions.checkArgument(fromSvc.getFencer() != null,\n-        \"failover requires a fencer\");\n-    preFailoverChecks(toSvc, forceActive);\n+    Preconditions.checkArgument(fencer != null, \"failover requires a fencer\");\n+    preFailoverChecks(toSvc, toSvcAddr, forceActive);\n \n     // Try to make fromSvc standby\n     boolean tryFence = true;\n     try {\n-      HAServiceProtocolHelper.transitionToStandby(fromSvc.getProxy());\n+      HAServiceProtocolHelper.transitionToStandby(fromSvc);\n       // We should try to fence if we failed or it was forced\n       tryFence = forceFence ? true : false;\n     } catch (ServiceFailedException sfe) {\n-      LOG.warn(\"Unable to make \" + fromSvc + \" standby (\" +\n+      LOG.warn(\"Unable to make \" + fromSvcAddr + \" standby (\" +\n           sfe.getMessage() + \")\");\n     } catch (IOException ioe) {\n-      LOG.warn(\"Unable to make \" + fromSvc +\n+      LOG.warn(\"Unable to make \" + fromSvcAddr +\n           \" standby (unable to connect)\", ioe);\n     }\n \n     // Fence fromSvc if it's required or forced by the user\n     if (tryFence) {\n-      if (!fromSvc.getFencer().fence(fromSvc)) {\n+      if (!fencer.fence(fromSvcAddr)) {\n         throw new FailoverFailedException(\"Unable to fence \" +\n-            fromSvc + \". Fencing failed.\");\n+            fromSvcAddr + \". Fencing failed.\");\n       }\n     }\n \n     // Try to make toSvc active\n     boolean failed = false;\n     Throwable cause = null;\n     try {\n-      HAServiceProtocolHelper.transitionToActive(toSvc.getProxy());\n+      HAServiceProtocolHelper.transitionToActive(toSvc);\n     } catch (ServiceFailedException sfe) {\n-      LOG.error(\"Unable to make \" + toSvc + \" active (\" +\n+      LOG.error(\"Unable to make \" + toSvcAddr + \" active (\" +\n           sfe.getMessage() + \"). Failing back.\");\n       failed = true;\n       cause = sfe;\n     } catch (IOException ioe) {\n-      LOG.error(\"Unable to make \" + toSvc +\n+      LOG.error(\"Unable to make \" + toSvcAddr +\n           \" active (unable to connect). Failing back.\", ioe);\n       failed = true;\n       cause = ioe;\n     }\n \n     // We failed to make toSvc active\n     if (failed) {\n-      String msg = \"Unable to failover to \" + toSvc;\n+      String msg = \"Unable to failover to \" + toSvcAddr;\n       // Only try to failback if we didn't fence fromSvc\n       if (!tryFence) {\n         try {\n           // Unconditionally fence toSvc in case it is still trying to\n           // become active, eg we timed out waiting for its response.\n           // Unconditionally force fromSvc to become active since it\n           // was previously active when we initiated failover.\n-          failover(toSvc, fromSvc, true, true);\n+          failover(toSvc, toSvcAddr, fromSvc, fromSvcAddr, fencer, true, true);\n         } catch (FailoverFailedException ffe) {\n-          msg += \". Failback to \" + fromSvc +\n+          msg += \". Failback to \" + fromSvcAddr +\n             \" failed (\" + ffe.getMessage() + \")\";\n           LOG.fatal(msg);\n         }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "sha": "c8878e8b73951f0f7a3290757615241c8f989d08",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.ha;\n \n+import java.net.InetSocketAddress;\n+\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configurable;\n@@ -60,6 +62,6 @@\n    * @throws BadFencingConfigurationException if the configuration was\n    *         determined to be invalid only at runtime\n    */\n-  public boolean tryFence(HAServiceTarget target, String args)\n+  public boolean tryFence(InetSocketAddress serviceAddr, String args)\n     throws BadFencingConfigurationException;\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "sha": "d8bda1402fa928ae1001a2a705709aaad03d70fb",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 11,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import java.io.IOException;\n import java.io.PrintStream;\n+import java.net.InetSocketAddress;\n import java.util.Map;\n \n import org.apache.commons.cli.Options;\n@@ -27,8 +28,11 @@\n import org.apache.commons.cli.GnuParser;\n import org.apache.commons.cli.ParseException;\n \n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n+import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.util.Tool;\n import org.apache.hadoop.util.ToolRunner;\n \n@@ -73,8 +77,6 @@\n   protected PrintStream errOut = System.err;\n   PrintStream out = System.out;\n \n-  protected abstract HAServiceTarget resolveTarget(String string);\n-\n   protected String getUsageString() {\n     return \"Usage: HAAdmin\";\n   }\n@@ -107,7 +109,7 @@ private int transitionToActive(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     HAServiceProtocolHelper.transitionToActive(proto);\n     return 0;\n   }\n@@ -120,13 +122,14 @@ private int transitionToStandby(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     HAServiceProtocolHelper.transitionToStandby(proto);\n     return 0;\n   }\n \n   private int failover(final String[] argv)\n       throws IOException, ServiceFailedException {\n+    Configuration conf = getConf();\n     boolean forceFence = false;\n     boolean forceActive = false;\n \n@@ -159,12 +162,29 @@ private int failover(final String[] argv)\n       return -1;\n     }\n \n-    HAServiceTarget fromNode = resolveTarget(args[0]);\n-    HAServiceTarget toNode = resolveTarget(args[1]);\n-    \n+    NodeFencer fencer;\n     try {\n-      FailoverController.failover(fromNode, toNode,\n-          forceFence, forceActive); \n+      fencer = NodeFencer.create(conf);\n+    } catch (BadFencingConfigurationException bfce) {\n+      errOut.println(\"failover: incorrect fencing configuration: \" + \n+          bfce.getLocalizedMessage());\n+      return -1;\n+    }\n+    if (fencer == null) {\n+      errOut.println(\"failover: no fencer configured\");\n+      return -1;\n+    }\n+\n+    InetSocketAddress addr1 = \n+      NetUtils.createSocketAddr(getServiceAddr(args[0]));\n+    InetSocketAddress addr2 = \n+      NetUtils.createSocketAddr(getServiceAddr(args[1]));\n+    HAServiceProtocol proto1 = getProtocol(args[0]);\n+    HAServiceProtocol proto2 = getProtocol(args[1]);\n+\n+    try {\n+      FailoverController.failover(proto1, addr1, proto2, addr2,\n+          fencer, forceFence, forceActive); \n       out.println(\"Failover from \"+args[0]+\" to \"+args[1]+\" successful\");\n     } catch (FailoverFailedException ffe) {\n       errOut.println(\"Failover failed: \" + ffe.getLocalizedMessage());\n@@ -181,7 +201,7 @@ private int checkHealth(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     try {\n       HAServiceProtocolHelper.monitorHealth(proto);\n     } catch (HealthCheckFailedException e) {\n@@ -199,7 +219,7 @@ private int getServiceState(final String[] argv)\n       return -1;\n     }\n \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     out.println(proto.getServiceStatus().getState());\n     return 0;\n   }\n@@ -212,6 +232,16 @@ protected String getServiceAddr(String serviceId) {\n     return serviceId;\n   }\n \n+  /**\n+   * Return a proxy to the specified target service.\n+   */\n+  protected HAServiceProtocol getProtocol(String serviceId)\n+      throws IOException {\n+    String serviceAddr = getServiceAddr(serviceId);\n+    InetSocketAddress addr = NetUtils.createSocketAddr(serviceAddr);\n+    return new HAServiceProtocolClientSideTranslatorPB(addr, getConf());\n+  }\n+\n   @Override\n   public int run(String[] argv) throws Exception {\n     try {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "sha": "a16ffb4c4004bdf89f821920605e17b78d9fe4a1",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "changes": 74,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java?ref=b1f1f5d93b21d98be1e61b777994322f57e3fda8",
                "deletions": 74,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "patch": "@@ -1,74 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.ha;\n-\n-import java.io.IOException;\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.classification.InterfaceStability;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n-import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n-\n-/**\n- * Represents a target of the client side HA administration commands.\n- */\n-@InterfaceAudience.Public\n-@InterfaceStability.Evolving\n-public abstract class HAServiceTarget {\n-\n-  /**\n-   * @return the IPC address of the target node.\n-   */\n-  public abstract InetSocketAddress getAddress();\n-\n-  /**\n-   * @return a Fencer implementation configured for this target node\n-   */\n-  public abstract NodeFencer getFencer();\n-  \n-  /**\n-   * @throws BadFencingConfigurationException if the fencing configuration\n-   * appears to be invalid. This is divorced from the above\n-   * {@link #getFencer()} method so that the configuration can be checked\n-   * during the pre-flight phase of failover.\n-   */\n-  public abstract void checkFencingConfigured()\n-      throws BadFencingConfigurationException;\n-  \n-  /**\n-   * @return a proxy to connect to the target HA Service.\n-   */\n-  public HAServiceProtocol getProxy(Configuration conf, int timeoutMs)\n-      throws IOException {\n-    Configuration confCopy = new Configuration(conf);\n-    // Lower the timeout so we quickly fail to connect\n-    confCopy.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 1);\n-    return new HAServiceProtocolClientSideTranslatorPB(\n-        getAddress(),\n-        confCopy, null, timeoutMs);\n-  }\n-\n-  /**\n-   * @return a proxy to connect to the target HA Service.\n-   */\n-  public final HAServiceProtocol getProxy() throws IOException {\n-    return getProxy(new Configuration(), 0); // default conf, timeout\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "sha": "78a2f2e4d98ab1b14b3042af340caa3d1c0d751b",
                "status": "removed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.hadoop.ha;\n \n+import java.net.InetSocketAddress;\n import java.util.List;\n import java.util.Map;\n import java.util.regex.Matcher;\n@@ -90,14 +91,14 @@ public static NodeFencer create(Configuration conf)\n     return new NodeFencer(conf);\n   }\n \n-  public boolean fence(HAServiceTarget fromSvc) {\n+  public boolean fence(InetSocketAddress serviceAddr) {\n     LOG.info(\"====== Beginning Service Fencing Process... ======\");\n     int i = 0;\n     for (FenceMethodWithArg method : methods) {\n       LOG.info(\"Trying method \" + (++i) + \"/\" + methods.size() +\": \" + method);\n       \n       try {\n-        if (method.method.tryFence(fromSvc, method.arg)) {\n+        if (method.method.tryFence(serviceAddr, method.arg)) {\n           LOG.info(\"====== Fencing successful by method \" + method + \" ======\");\n           return true;\n         }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "sha": "34a2c8b823a3ee2943d18c94431babe22275ef2d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "patch": "@@ -75,8 +75,7 @@ public void checkArgs(String args) throws BadFencingConfigurationException {\n   }\n \n   @Override\n-  public boolean tryFence(HAServiceTarget target, String cmd) {\n-    InetSocketAddress serviceAddr = target.getAddress();\n+  public boolean tryFence(InetSocketAddress serviceAddr, String cmd) {\n     List<String> cmdList = Arrays.asList(cmd.split(\"\\\\s+\"));\n \n     // Create arg list with service as the first argument",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "sha": "ca81f23a1878fa7fea3247b8e197e1150209a53c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "patch": "@@ -79,11 +79,10 @@ public void checkArgs(String argStr) throws BadFencingConfigurationException {\n   }\n \n   @Override\n-  public boolean tryFence(HAServiceTarget target, String argsStr)\n+  public boolean tryFence(InetSocketAddress serviceAddr, String argsStr)\n       throws BadFencingConfigurationException {\n \n     Args args = new Args(argsStr);\n-    InetSocketAddress serviceAddr = target.getAddress();\n     String host = serviceAddr.getHostName();\n     \n     Session session;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "sha": "00b9a83a572a3ad8a14cf2f7eab8cd013501a2ee",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java?ref=b1f1f5d93b21d98be1e61b777994322f57e3fda8",
                "deletions": 94,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "patch": "@@ -1,94 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.ha;\n-\n-import java.io.IOException;\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n-import org.apache.hadoop.security.AccessControlException;\n-import org.mockito.Mockito;\n-\n-/**\n- * Test-only implementation of {@link HAServiceTarget}, which returns\n- * a mock implementation.\n- */\n-class DummyHAService extends HAServiceTarget {\n-  HAServiceState state;\n-  HAServiceProtocol proxy;\n-  NodeFencer fencer;\n-  InetSocketAddress address;\n-\n-  DummyHAService(HAServiceState state, InetSocketAddress address) {\n-    this.state = state;\n-    this.proxy = makeMock();\n-    this.fencer = Mockito.mock(NodeFencer.class);\n-    this.address = address;\n-  }\n-  \n-  private HAServiceProtocol makeMock() {\n-    return Mockito.spy(new HAServiceProtocol() {\n-      @Override\n-      public void monitorHealth() throws HealthCheckFailedException,\n-          AccessControlException, IOException {\n-      }\n-\n-      @Override\n-      public void transitionToActive() throws ServiceFailedException,\n-          AccessControlException, IOException {\n-        state = HAServiceState.ACTIVE;\n-      }\n-\n-      @Override\n-      public void transitionToStandby() throws ServiceFailedException,\n-          AccessControlException, IOException {\n-        state = HAServiceState.STANDBY;\n-      }\n-\n-      @Override\n-      public HAServiceStatus getServiceStatus() throws IOException {\n-        HAServiceStatus ret = new HAServiceStatus(state);\n-        if (state == HAServiceState.STANDBY) {\n-          ret.setReadyToBecomeActive();\n-        }\n-        return ret;\n-      }\n-    });\n-  }\n-\n-  @Override\n-  public InetSocketAddress getAddress() {\n-    return address;\n-  }\n-\n-  @Override\n-  public HAServiceProtocol getProxy(Configuration conf, int timeout)\n-      throws IOException {\n-    return proxy;\n-  }\n-\n-  @Override\n-  public NodeFencer getFencer() {\n-    return fencer;\n-  }\n-\n-  @Override\n-  public void checkFencingConfigured() throws BadFencingConfigurationException {\n-  }\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "sha": "69c4a6fde41141b8c43c94a4efe41348718f9a32",
                "status": "removed"
            },
            {
                "additions": 214,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "changes": 358,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 144,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "patch": "@@ -24,85 +24,124 @@\n import static org.mockito.Mockito.verify;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n+import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n import org.apache.hadoop.ha.TestNodeFencer.AlwaysSucceedFencer;\n import org.apache.hadoop.ha.TestNodeFencer.AlwaysFailFencer;\n import static org.apache.hadoop.ha.TestNodeFencer.setupFencer;\n+import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.AccessControlException;\n \n import org.junit.Test;\n-import org.mockito.Mockito;\n-import org.mockito.internal.stubbing.answers.ThrowsException;\n-import org.mockito.stubbing.Answer;\n-\n import static org.junit.Assert.*;\n \n public class TestFailoverController {\n+\n   private InetSocketAddress svc1Addr = new InetSocketAddress(\"svc1\", 1234); \n-  private InetSocketAddress svc2Addr = new InetSocketAddress(\"svc2\", 5678);\n+  private InetSocketAddress svc2Addr = new InetSocketAddress(\"svc2\", 5678); \n+\n+  private class DummyService implements HAServiceProtocol {\n+    HAServiceState state;\n+\n+    DummyService(HAServiceState state) {\n+      this.state = state;\n+    }\n+\n+    @Override\n+    public void monitorHealth() throws HealthCheckFailedException, IOException {\n+      // Do nothing\n+    }\n+\n+    @Override\n+    public void transitionToActive() throws ServiceFailedException, IOException {\n+      state = HAServiceState.ACTIVE;\n+    }\n \n-  HAServiceStatus STATE_NOT_READY = new HAServiceStatus(HAServiceState.STANDBY)\n-      .setNotReadyToBecomeActive(\"injected not ready\");\n+    @Override\n+    public void transitionToStandby() throws ServiceFailedException, IOException {\n+      state = HAServiceState.STANDBY;\n+    }\n \n+    @Override\n+    public HAServiceStatus getServiceStatus() throws IOException {\n+      HAServiceStatus ret = new HAServiceStatus(state);\n+      if (state == HAServiceState.STANDBY) {\n+        ret.setReadyToBecomeActive();\n+      }\n+      return ret;\n+    }\n+    \n+    private HAServiceState getServiceState() {\n+      return state;\n+    }\n+  }\n+  \n   @Test\n   public void testFailoverAndFailback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n-    FailoverController.failover(svc1, svc2, false, false);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     assertEquals(0, TestNodeFencer.AlwaysSucceedFencer.fenceCalled);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n-    FailoverController.failover(svc2, svc1, false, false);\n+    FailoverController.failover(svc2, svc2Addr, svc1, svc1Addr, fencer, false, false);\n     assertEquals(0, TestNodeFencer.AlwaysSucceedFencer.fenceCalled);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromStandbyToStandby() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.STANDBY, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.STANDBY);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n-    FailoverController.failover(svc1, svc2, false, false);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromActiveToActive() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.ACTIVE, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.ACTIVE);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover to an already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverWithoutPermission() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new AccessControlException(\"Access denied\"))\n-        .when(svc1.proxy).getServiceStatus();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new AccessControlException(\"Access denied\"))\n-        .when(svc2.proxy).getServiceStatus();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        throw new AccessControlException(\"Access denied\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        throw new AccessControlException(\"Access denied\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover when access is denied\");\n     } catch (FailoverFailedException ffe) {\n       assertTrue(ffe.getCause().getMessage().contains(\"Access denied\"));\n@@ -112,13 +151,19 @@ public void testFailoverWithoutPermission() throws Exception {\n \n   @Test\n   public void testFailoverToUnreadyService() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doReturn(STATE_NOT_READY).when(svc2.proxy).getServiceStatus();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        HAServiceStatus ret = new HAServiceStatus(HAServiceState.STANDBY);\n+        ret.setNotReadyToBecomeActive(\"injected not ready\");\n+        return ret;\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover to a service that's not ready\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -127,88 +172,95 @@ public void testFailoverToUnreadyService() throws Exception {\n       }\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n \n     // Forcing it means we ignore readyToBecomeActive\n-    FailoverController.failover(svc1, svc2, false, true);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, true);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToUnhealthyServiceFailsAndFailsback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new HealthCheckFailedException(\"Failed!\"))\n-        .when(svc2.proxy).monitorHealth();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void monitorHealth() throws HealthCheckFailedException {\n+        throw new HealthCheckFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failover to unhealthy service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromFaultyServiceSucceeds() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToStandby();\n-\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToStandby() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     } catch (FailoverFailedException ffe) {\n       fail(\"Faulty active prevented failover\");\n     }\n \n     // svc1 still thinks it's active, that's OK, it was fenced\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(svc1, AlwaysSucceedFencer.fencedSvc);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromFaultyServiceFencingFailure() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToStandby();\n-\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToStandby() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n \n     AlwaysFailFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over even though fencing failed\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc1, AlwaysFailFencer.fencedSvc);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysFailFencer.fencedSvc);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFencingFailureDuringFailover() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n \n     AlwaysFailFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, true, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, true, false);\n       fail(\"Failed over even though fencing requested and failed\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -217,105 +269,115 @@ public void testFencingFailureDuringFailover() throws Exception {\n     // If fencing was requested and it failed we don't try to make\n     // svc2 active anyway, and we don't failback to svc1.\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc1, AlwaysFailFencer.fencedSvc);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysFailFencer.fencedSvc);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n   \n+  private HAServiceProtocol getProtocol(String target)\n+      throws IOException {\n+    InetSocketAddress addr = NetUtils.createSocketAddr(target);\n+    Configuration conf = new Configuration();\n+    // Lower the timeout so we quickly fail to connect\n+    conf.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 1);\n+    return new HAServiceProtocolClientSideTranslatorPB(addr, conf);\n+  }\n+\n   @Test\n   public void testFailoverFromNonExistantServiceWithFencer() throws Exception {\n-    DummyHAService svc1 = spy(new DummyHAService(null, svc1Addr));\n-    // Getting a proxy to a dead server will throw IOException on call,\n-    // not on creation of the proxy.\n-    HAServiceProtocol errorThrowingProxy = Mockito.mock(HAServiceProtocol.class,\n-        new ThrowsException(new IOException(\"Could not connect to host\")));\n-    Mockito.doReturn(errorThrowingProxy).when(svc1).getProxy();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    HAServiceProtocol svc1 = getProtocol(\"localhost:1234\");\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     } catch (FailoverFailedException ffe) {\n       fail(\"Non-existant active prevented failover\");\n     }\n \n     // Don't check svc1 because we can't reach it, but that's OK, it's been fenced.\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToNonExistantServiceFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = spy(new DummyHAService(null, svc2Addr));\n-    Mockito.doThrow(new IOException(\"Failed to connect\"))\n-      .when(svc2).getProxy(Mockito.<Configuration>any(),\n-          Mockito.anyInt());\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    HAServiceProtocol svc2 = getProtocol(\"localhost:1234\");\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to a non-existant standby\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToFaultyServiceFailsbackOK() throws Exception {\n-    DummyHAService svc1 = spy(new DummyHAService(HAServiceState.ACTIVE, svc1Addr));\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = spy(new DummyService(HAServiceState.ACTIVE));\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failover to already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     // svc1 went standby then back to active\n-    verify(svc1.proxy).transitionToStandby();\n-    verify(svc1.proxy).transitionToActive();\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    verify(svc1).transitionToStandby();\n+    verify(svc1).transitionToActive();\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testWeDontFailbackIfActiveWasFenced() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, true, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, true, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     // We failed to failover and did not failback because we fenced\n     // svc1 (we forced it), therefore svc1 and svc2 should be standby.\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testWeFenceOnFailbackIfTransitionToActiveFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException, IOException {\n+        throw new IOException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n     AlwaysSucceedFencer.fenceCalled = 0;\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -324,22 +386,25 @@ public void testWeFenceOnFailbackIfTransitionToActiveFails() throws Exception {\n     // We failed to failover. We did not fence svc1 because it cooperated\n     // and we didn't force it, so we failed back to svc1 and fenced svc2.\n     // Note svc2 still thinks it's active, that's OK, we fenced it.\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(svc2, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"svc2:5678\", AlwaysSucceedFencer.fencedSvc);\n   }\n \n   @Test\n   public void testFailureToFenceOnFailbackFailsTheFailback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new IOException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException, IOException {\n+        throw new IOException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n     AlwaysFailFencer.fenceCalled = 0;\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -348,30 +413,35 @@ public void testFailureToFenceOnFailbackFailsTheFailback() throws Exception {\n     // We did not fence svc1 because it cooperated and we didn't force it, \n     // we failed to failover so we fenced svc2, we failed to fence svc2\n     // so we did not failback to svc1, ie it's still standby.\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc2, AlwaysFailFencer.fencedSvc);\n+    assertEquals(\"svc2:5678\", AlwaysFailFencer.fencedSvc);\n   }\n \n   @Test\n   public void testFailbackToFaultyServiceFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToActive();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1, svc1Addr, svc2, svc2Addr, fencer, false, false);\n       fail(\"Failover to already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "sha": "6dec32c636de5317a679eb3bc609faa8168e0922",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "patch": "@@ -22,15 +22,14 @@\n import java.io.ByteArrayOutputStream;\n import java.io.IOException;\n import java.io.PrintStream;\n-import java.net.InetSocketAddress;\n \n import org.apache.commons.logging.LogFactory;\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n \n import org.junit.Before;\n import org.junit.Test;\n+import org.mockito.Mockito;\n \n import com.google.common.base.Charsets;\n import com.google.common.base.Joiner;\n@@ -41,14 +40,15 @@\n   private HAAdmin tool;\n   private ByteArrayOutputStream errOutBytes = new ByteArrayOutputStream();\n   private String errOutput;\n-\n+  private HAServiceProtocol mockProtocol;\n+  \n   @Before\n   public void setup() throws IOException {\n+    mockProtocol = Mockito.mock(HAServiceProtocol.class);\n     tool = new HAAdmin() {\n       @Override\n-      protected HAServiceTarget resolveTarget(String target) {\n-        return new DummyHAService(HAServiceState.STANDBY,\n-            new InetSocketAddress(\"dummy\", 12345));\n+      protected HAServiceProtocol getProtocol(String target) throws IOException {\n+        return mockProtocol;\n       }\n     };\n     tool.setConf(new Configuration());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "sha": "7f885d8bc2535d2b34eb4fe9e5dd961da12be685",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 28,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "patch": "@@ -26,35 +26,26 @@\n import org.apache.hadoop.conf.Configured;\n import org.junit.Before;\n import org.junit.Test;\n-import org.mockito.Mockito;\n \n import com.google.common.collect.Lists;\n \n public class TestNodeFencer {\n \n-  private HAServiceTarget MOCK_TARGET;\n-  \n-\n   @Before\n   public void clearMockState() {\n     AlwaysSucceedFencer.fenceCalled = 0;\n     AlwaysSucceedFencer.callArgs.clear();\n     AlwaysFailFencer.fenceCalled = 0;\n     AlwaysFailFencer.callArgs.clear();\n-    \n-    MOCK_TARGET = Mockito.mock(HAServiceTarget.class);\n-    Mockito.doReturn(\"my mock\").when(MOCK_TARGET).toString();\n-    Mockito.doReturn(new InetSocketAddress(\"host\", 1234))\n-        .when(MOCK_TARGET).getAddress();\n   }\n \n   @Test\n   public void testSingleFencer() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName() + \"(foo)\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(\"foo\", AlwaysSucceedFencer.callArgs.get(0));\n   }\n   \n@@ -63,7 +54,7 @@ public void testMultipleFencers() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName() + \"(foo)\\n\" +\n         AlwaysSucceedFencer.class.getName() + \"(bar)\\n\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // Only one call, since the first fencer succeeds\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n     assertEquals(\"foo\", AlwaysSucceedFencer.callArgs.get(0));\n@@ -77,12 +68,12 @@ public void testWhitespaceAndCommentsInConfig()\n         \" # the next one will always fail\\n\" +\n         \" \" + AlwaysFailFencer.class.getName() + \"(foo) # <- fails\\n\" +\n         AlwaysSucceedFencer.class.getName() + \"(bar) \\n\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // One call to each, since top fencer fails\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysFailFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysFailFencer.fencedSvc);\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(\"foo\", AlwaysFailFencer.callArgs.get(0));\n     assertEquals(\"bar\", AlwaysSucceedFencer.callArgs.get(0));\n   }\n@@ -91,41 +82,41 @@ public void testWhitespaceAndCommentsInConfig()\n   public void testArglessFencer() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName());\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // One call to each, since top fencer fails\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(null, AlwaysSucceedFencer.callArgs.get(0));\n   }\n \n   @Test\n   public void testShortNameShell() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"shell(true)\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSsh() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithUser() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(user)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithPort() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(:123)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithUserPort() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(user:123)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   public static NodeFencer setupFencer(String confStr)\n@@ -142,12 +133,12 @@ public static NodeFencer setupFencer(String confStr)\n   public static class AlwaysSucceedFencer extends Configured\n       implements FenceMethod {\n     static int fenceCalled = 0;\n-    static HAServiceTarget fencedSvc;\n+    static String fencedSvc;\n     static List<String> callArgs = Lists.newArrayList();\n \n     @Override\n-    public boolean tryFence(HAServiceTarget target, String args) {\n-      fencedSvc = target;\n+    public boolean tryFence(InetSocketAddress serviceAddr, String args) {\n+      fencedSvc = serviceAddr.getHostName() + \":\" + serviceAddr.getPort();\n       callArgs.add(args);\n       fenceCalled++;\n       return true;\n@@ -164,12 +155,12 @@ public void checkArgs(String args) {\n   public static class AlwaysFailFencer extends Configured\n       implements FenceMethod {\n     static int fenceCalled = 0;\n-    static HAServiceTarget fencedSvc;\n+    static String fencedSvc;\n     static List<String> callArgs = Lists.newArrayList();\n \n     @Override\n-    public boolean tryFence(HAServiceTarget target, String args) {\n-      fencedSvc = target;\n+    public boolean tryFence(InetSocketAddress serviceAddr, String args) {\n+      fencedSvc = serviceAddr.getHostName() + \":\" + serviceAddr.getPort();\n       callArgs.add(args);\n       fenceCalled++;\n       return false;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "sha": "5508547c0a52daff57ae2400b14a0f30c946d699",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 11,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "patch": "@@ -22,7 +22,6 @@\n import java.net.InetSocketAddress;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.util.StringUtils;\n import org.junit.Before;\n import org.junit.BeforeClass;\n@@ -33,9 +32,6 @@\n \n public class TestShellCommandFencer {\n   private ShellCommandFencer fencer = createFencer();\n-  private static final HAServiceTarget TEST_TARGET =\n-      new DummyHAService(HAServiceState.ACTIVE,\n-          new InetSocketAddress(\"host\", 1234));\n   \n   @BeforeClass\n   public static void setupLogSpy() {\n@@ -61,10 +57,11 @@ private static ShellCommandFencer createFencer() {\n    */\n   @Test\n   public void testBasicSuccessFailure() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo\"));\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"exit 1\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo\"));\n+    assertFalse(fencer.tryFence(addr, \"exit 1\"));\n     // bad path should also fail\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"xxxxxxxxxxxx\"));\n+    assertFalse(fencer.tryFence(addr, \"xxxxxxxxxxxx\"));\n   }\n   \n   @Test\n@@ -101,7 +98,8 @@ public void testCheckParensNoArgs() {\n    */\n   @Test\n   public void testStdoutLogging() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo hello\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo hello\"));\n     Mockito.verify(ShellCommandFencer.LOG).info(\n         Mockito.endsWith(\"echo hello: host:1234 hello\"));\n   }\n@@ -112,7 +110,8 @@ public void testStdoutLogging() {\n    */\n   @Test\n   public void testStderrLogging() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo hello >&2\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo hello >&2\"));\n     Mockito.verify(ShellCommandFencer.LOG).warn(\n         Mockito.endsWith(\"echo hello >&2: host:1234 hello\"));\n   }\n@@ -123,7 +122,8 @@ public void testStderrLogging() {\n    */\n   @Test\n   public void testConfAsEnvironment() {\n-    fencer.tryFence(TEST_TARGET, \"echo $in_fencing_tests\");\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    fencer.tryFence(addr, \"echo $in_fencing_tests\");\n     Mockito.verify(ShellCommandFencer.LOG).info(\n         Mockito.endsWith(\"echo $in...ing_tests: host:1234 yessir\"));\n   }\n@@ -136,7 +136,8 @@ public void testConfAsEnvironment() {\n    */\n   @Test(timeout=10000)\n   public void testSubprocessInputIsClosed() {\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"read\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertFalse(fencer.tryFence(addr, \"read\"));\n   }\n   \n   @Test",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "sha": "49bae039eccbcc7b7f43a73e64e2084bc66c9a2d",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 19,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "patch": "@@ -23,7 +23,6 @@\n \n import org.apache.commons.logging.impl.Log4JLogger;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.ha.SshFenceByTcpPort.Args;\n import org.apache.log4j.Level;\n import org.junit.Assume;\n@@ -35,25 +34,12 @@\n     ((Log4JLogger)SshFenceByTcpPort.LOG).getLogger().setLevel(Level.ALL);\n   }\n   \n-  private static String TEST_FENCING_HOST = System.getProperty(\n+  private String TEST_FENCING_HOST = System.getProperty(\n       \"test.TestSshFenceByTcpPort.host\", \"localhost\");\n-  private static final String TEST_FENCING_PORT = System.getProperty(\n+  private String TEST_FENCING_PORT = System.getProperty(\n       \"test.TestSshFenceByTcpPort.port\", \"8020\");\n-  private static final String TEST_KEYFILE = System.getProperty(\n+  private final String TEST_KEYFILE = System.getProperty(\n       \"test.TestSshFenceByTcpPort.key\");\n-  \n-  private static final InetSocketAddress TEST_ADDR =\n-    new InetSocketAddress(TEST_FENCING_HOST,\n-      Integer.valueOf(TEST_FENCING_PORT));\n-  private static final HAServiceTarget TEST_TARGET =\n-    new DummyHAService(HAServiceState.ACTIVE, TEST_ADDR);\n-  \n-  /**\n-   *  Connect to Google's DNS server - not running ssh!\n-   */\n-  private static final HAServiceTarget UNFENCEABLE_TARGET =\n-    new DummyHAService(HAServiceState.ACTIVE,\n-        new InetSocketAddress(\"8.8.8.8\", 1234));\n \n   @Test(timeout=20000)\n   public void testFence() throws BadFencingConfigurationException {\n@@ -63,7 +49,8 @@ public void testFence() throws BadFencingConfigurationException {\n     SshFenceByTcpPort fence = new SshFenceByTcpPort();\n     fence.setConf(conf);\n     assertTrue(fence.tryFence(\n-        TEST_TARGET,\n+        new InetSocketAddress(TEST_FENCING_HOST,\n+                              Integer.valueOf(TEST_FENCING_PORT)),\n         null));\n   }\n \n@@ -78,7 +65,8 @@ public void testConnectTimeout() throws BadFencingConfigurationException {\n     conf.setInt(SshFenceByTcpPort.CONF_CONNECT_TIMEOUT_KEY, 3000);\n     SshFenceByTcpPort fence = new SshFenceByTcpPort();\n     fence.setConf(conf);\n-    assertFalse(fence.tryFence(UNFENCEABLE_TARGET, \"\"));\n+    // Connect to Google's DNS server - not running ssh!\n+    assertFalse(fence.tryFence(new InetSocketAddress(\"8.8.8.8\", 1234), \"\"));\n   }\n   \n   @Test",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "sha": "554a7abca5fdd33e027dfbb305c85e2d4a1b9ba3",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "patch": "@@ -25,8 +25,8 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.ha.HAAdmin;\n-import org.apache.hadoop.ha.HAServiceTarget;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DFSUtil;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.util.ToolRunner;\n \n@@ -65,9 +65,15 @@ public void setConf(Configuration conf) {\n    * Try to map the given namenode ID to its service address.\n    */\n   @Override\n-  protected HAServiceTarget resolveTarget(String nnId) {\n+  protected String getServiceAddr(String nnId) {\n     HdfsConfiguration conf = (HdfsConfiguration)getConf();\n-    return new NNHAServiceTarget(conf, nameserviceId, nnId);\n+    String serviceAddr = \n+      DFSUtil.getNamenodeServiceAddr(conf, nameserviceId, nnId);\n+    if (serviceAddr == null) {\n+      throw new IllegalArgumentException(\n+          \"Unable to determine service address for namenode '\" + nnId + \"'\");\n+    }\n+    return serviceAddr;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "sha": "13bde2ae53391ee734fd84e669950a2eb1355f4b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java?ref=b1f1f5d93b21d98be1e61b777994322f57e3fda8",
                "deletions": 84,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "patch": "@@ -1,84 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.hdfs.tools;\n-\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.ha.BadFencingConfigurationException;\n-import org.apache.hadoop.ha.HAServiceTarget;\n-import org.apache.hadoop.ha.NodeFencer;\n-import org.apache.hadoop.hdfs.DFSUtil;\n-import org.apache.hadoop.hdfs.HdfsConfiguration;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n-import org.apache.hadoop.net.NetUtils;\n-\n-/**\n- * One of the NN NameNodes acting as the target of an administrative command\n- * (e.g. failover).\n- */\n-@InterfaceAudience.Private\n-public class NNHAServiceTarget extends HAServiceTarget {\n-\n-  private final InetSocketAddress addr;\n-  private NodeFencer fencer;\n-  private BadFencingConfigurationException fenceConfigError;\n-\n-  public NNHAServiceTarget(HdfsConfiguration conf,\n-      String nsId, String nnId) {\n-    String serviceAddr = \n-      DFSUtil.getNamenodeServiceAddr(conf, nsId, nnId);\n-    if (serviceAddr == null) {\n-      throw new IllegalArgumentException(\n-          \"Unable to determine service address for namenode '\" + nnId + \"'\");\n-    }\n-    this.addr = NetUtils.createSocketAddr(serviceAddr,\n-        NameNode.DEFAULT_PORT);\n-    try {\n-      this.fencer = NodeFencer.create(conf);\n-    } catch (BadFencingConfigurationException e) {\n-      this.fenceConfigError = e;\n-    }\n-  }\n-\n-  /**\n-   * @return the NN's IPC address.\n-   */\n-  @Override\n-  public InetSocketAddress getAddress() {\n-    return addr;\n-  }\n-\n-  @Override\n-  public void checkFencingConfigured() throws BadFencingConfigurationException {\n-    if (fenceConfigError != null) {\n-      throw fenceConfigError;\n-    }\n-  }\n-  \n-  @Override\n-  public NodeFencer getFencer() {\n-    return fencer;\n-  }\n-  \n-  @Override\n-  public String toString() {\n-    return \"NameNode at \" + addr;\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "sha": "9e8c239e7e8782fffff27109a1c2c2f0f8700028",
                "status": "removed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 12,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.hadoop.ha.HAServiceProtocol;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.ha.HAServiceStatus;\n-import org.apache.hadoop.ha.HAServiceTarget;\n import org.apache.hadoop.ha.HealthCheckFailedException;\n import org.apache.hadoop.ha.NodeFencer;\n \n@@ -80,18 +79,10 @@ private HdfsConfiguration getHAConf() {\n   public void setup() throws IOException {\n     mockProtocol = Mockito.mock(HAServiceProtocol.class);\n     tool = new DFSHAAdmin() {\n-\n       @Override\n-      protected HAServiceTarget resolveTarget(String nnId) {\n-        HAServiceTarget target = super.resolveTarget(nnId);\n-        HAServiceTarget spy = Mockito.spy(target);\n-        // OVerride the target to return our mock protocol\n-        try {\n-          Mockito.doReturn(mockProtocol).when(spy).getProxy();\n-        } catch (IOException e) {\n-          throw new AssertionError(e); // mock setup doesn't really throw\n-        }\n-        return spy;\n+      protected HAServiceProtocol getProtocol(String serviceId) throws IOException {\n+        getServiceAddr(serviceId);\n+        return mockProtocol;\n       }\n     };\n     tool.setConf(getHAConf());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "sha": "c5ba0eb7e589257272defe6615228876daebf439",
                "status": "modified"
            }
        ],
        "message": "Revert HADOOP-8193 from r1304967. Patch introduced some NPEs in a test case.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1305152 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/b1f1f5d93b21d98be1e61b777994322f57e3fda8",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFailoverController.java",
            "TestHAAdmin.java",
            "TestNodeFencer.java",
            "TestShellCommandFencer.java",
            "TestSshFenceByTcpPort.java",
            "TestDFSHAAdmin.java"
        ]
    },
    "hadoop-common_0a0dc3f": {
        "bug_id": "hadoop-common_0a0dc3f",
        "commit": "https://github.com/apache/hadoop-common/commit/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -289,6 +289,9 @@ Trunk (unreleased changes)\n     HADOOP-6991.  Fix SequenceFile::Reader to honor file lengths and call\n     openFile (cdouglas via omalley)\n \n+    HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.\n+    (Aaron T. Myers via tomwhite)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/CHANGES.txt",
                "sha": "e607a0c1062c4eef025df0df61256d286a37a9af",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/src/java/org/apache/hadoop/security/KerberosName.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/security/KerberosName.java?ref=0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/security/KerberosName.java",
                "patch": "@@ -399,9 +399,10 @@ static void printRules() throws IOException {\n   }\n \n   public static void main(String[] args) throws Exception {\n+    setConfiguration(new Configuration());\n     for(String arg: args) {\n       KerberosName name = new KerberosName(arg);\n       System.out.println(\"Name: \" + name + \" to \" + name.getShortName());\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/src/java/org/apache/hadoop/security/KerberosName.java",
                "sha": "b533cd22f77d6dbf0aaa0a42edf6338dc6d987c6",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.  Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1028938 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/22f7d55eb833d49fa0cba03b5275d4462d5b2c97",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestKerberosName.java"
        ]
    },
    "hadoop-common_1073a1c": {
        "bug_id": "hadoop-common_1073a1c",
        "commit": "https://github.com/apache/hadoop-common/commit/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=1073a1c10b125805ab301e73d1a4c0a1e1d4e34e",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -12,6 +12,9 @@ Trunk (unreleased changes)\n \n   NEW FEATURES\n \n+    HADOOP-7342. Add an utility API in FileUtil for JDK File.list\n+    avoid NPEs on File.list() (Bharath Mundlapudi via mattf)\n+\n     HADOOP-7322. Adding a util method in FileUtil for directory listing,\n     avoid NPEs on File.listFiles() (Bharath Mundlapudi via mattf)\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/CHANGES.txt",
                "sha": "16152a6c9f688f892ae747b874e7f6f7105ac83b",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/src/java/org/apache/hadoop/fs/FileUtil.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/fs/FileUtil.java?ref=1073a1c10b125805ab301e73d1a4c0a1e1d4e34e",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/fs/FileUtil.java",
                "patch": "@@ -728,4 +728,23 @@ public static void replaceFile(File src, File target) throws IOException {\n     }\n     return files;\n   }  \n+  \n+  /**\n+   * A wrapper for {@link File#list()}. This java.io API returns null \n+   * when a dir is not a directory or for any I/O error. Instead of having\n+   * null check everywhere File#list() is used, we will add utility API\n+   * to get around this problem. For the majority of cases where we prefer \n+   * an IOException to be thrown.\n+   * @param dir directory for which listing should be performed\n+   * @return list of file names or empty string list\n+   * @exception IOException for invalid directory or for a bad disk.\n+   */\n+  public static String[] list(File dir) throws IOException {\n+    String[] fileNames = dir.list();\n+    if(fileNames == null) {\n+      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n+                + dir.toString());\n+    }\n+    return fileNames;\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/src/java/org/apache/hadoop/fs/FileUtil.java",
                "sha": "86956e368c079cfa9e221c8317550404fe53a29e",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/core/org/apache/hadoop/fs/TestFileUtil.java?ref=1073a1c10b125805ab301e73d1a4c0a1e1d4e34e",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "patch": "@@ -143,6 +143,33 @@ public void testListFiles() throws IOException {\n     \t//Expected an IOException\n     }\n   }\n+\n+  @Test\n+  public void testListAPI() throws IOException {\n+    setupDirs();\n+    //Test existing files case \n+    String[] files = FileUtil.list(partitioned);\n+    Assert.assertEquals(\"Unexpected number of pre-existing files\", 2, files.length);\n+\n+    //Test existing directory with no files case \n+    File newDir = new File(tmp.getPath(),\"test\");\n+    newDir.mkdir();\n+    Assert.assertTrue(\"Failed to create test dir\", newDir.exists());\n+    files = FileUtil.list(newDir);\n+    Assert.assertEquals(\"New directory unexpectedly contains files\", 0, files.length);\n+    newDir.delete();\n+    Assert.assertFalse(\"Failed to delete test dir\", newDir.exists());\n+    \n+    //Test non-existing directory case, this throws \n+    //IOException\n+    try {\n+      files = FileUtil.list(newDir);\n+      Assert.fail(\"IOException expected on list() for non-existent dir \"\n+          + newDir.toString());\n+    } catch(IOException ioe) {\n+      //Expected an IOException\n+    }\n+  }\n   \n   @After\n   public void tearDown() throws IOException {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "sha": "65c43435b75b9fb79d8be4e247a5f8175da3bf22",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7342. Add an utility API in FileUtil for JDK File.list avoid NPEs on File.list().  Contributed by Bharath Mundlapudi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1131330 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f14ab45505c3eedc78f9671c9e43a221d28dba8d",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFileUtil.java"
        ]
    },
    "hadoop-common_11131d7": {
        "bug_id": "hadoop-common_11131d7",
        "commit": "https://github.com/apache/hadoop-common/commit/11131d72040c250f897a8d1c130b83ce1b065050",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/11131d72040c250f897a8d1c130b83ce1b065050/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=11131d72040c250f897a8d1c130b83ce1b065050",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -2198,3 +2198,5 @@ Release 0.21.0 - 2010-08-13\n \n     MAPREDUCE-1856. Extract a subset of tests for smoke (DOA) validation (cos)\n \n+    MAPREDUCE-2317. Fix a NPE in HadoopArchives.  (Devaraj K via szetszwo)\n+",
                "raw_url": "https://github.com/apache/hadoop-common/raw/11131d72040c250f897a8d1c130b83ce1b065050/CHANGES.txt",
                "sha": "9f26015d60ab79f23ebb727a7c2cdc543fbbf831",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/11131d72040c250f897a8d1c130b83ce1b065050/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/tools/org/apache/hadoop/tools/HadoopArchives.java?ref=11131d72040c250f897a8d1c130b83ce1b065050",
                "deletions": 9,
                "filename": "src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "patch": "@@ -367,16 +367,18 @@ private void writeTopLevelDirs(SequenceFile.Writer srcWriter,\n         }\n         else {\n           Path parent = p.getParent();\n-          if (allpaths.containsKey(parent.toString())) {\n-            HashSet<String> children = allpaths.get(parent.toString());\n-            children.add(p.getName());\n-          }\n-          else {\n-            HashSet<String> children = new HashSet<String>();\n-            children.add(p.getName());\n-            allpaths.put(parent.toString(), children);\n+          if (null != parent) {\n+            if (allpaths.containsKey(parent.toString())) {\n+              HashSet<String> children = allpaths.get(parent.toString());\n+              children.add(p.getName());\n+            } \n+            else {\n+              HashSet<String> children = new HashSet<String>();\n+              children.add(p.getName());\n+              allpaths.put(parent.toString(), children);\n+            }\n+            parents.add(parent);\n           }\n-          parents.add(parent);\n         }\n       }\n       justDirs = parents;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/11131d72040c250f897a8d1c130b83ce1b065050/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "sha": "41d149ea952f98e877d56de2deb3c5ba482ae120",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-2317. Fix a NPE in HadoopArchives.  Contributed by Devaraj K\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1096022 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ac3445f84c3699f5032374587c3c80087b598e0f",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestHadoopArchives.java"
        ]
    },
    "hadoop-common_17a47c1": {
        "bug_id": "hadoop-common_17a47c1",
        "commit": "https://github.com/apache/hadoop-common/commit/17a47c189460dc5183582ceb370b3f3f226fb1ae",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=17a47c189460dc5183582ceb370b3f3f226fb1ae",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -88,6 +88,9 @@ Release 2.6.0 - UNRELEASED\n     YARN-2321. NodeManager web UI can incorrectly report Pmem enforcement\n     (Leitao Guo via jlowe)\n \n+    YARN-2273. NPE in ContinuousScheduling thread when we lose a node. \n+    (Wei Yan via kasha)\n+\n Release 2.5.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/CHANGES.txt",
                "sha": "2e69a756c6415598c7f26f304b969407987ee68e",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/hadoop-common/blob/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java?ref=17a47c189460dc5183582ceb370b3f3f226fb1ae",
                "deletions": 30,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "patch": "@@ -970,37 +970,27 @@ private synchronized void nodeUpdate(RMNode nm) {\n     }\n   }\n \n-  private void continuousScheduling() {\n-    while (true) {\n-      List<NodeId> nodeIdList = new ArrayList<NodeId>(nodes.keySet());\n-      // Sort the nodes by space available on them, so that we offer\n-      // containers on emptier nodes first, facilitating an even spread. This\n-      // requires holding the scheduler lock, so that the space available on a\n-      // node doesn't change during the sort.\n-      synchronized (this) {\n-        Collections.sort(nodeIdList, nodeAvailableResourceComparator);\n-      }\n+  void continuousSchedulingAttempt() {\n+    List<NodeId> nodeIdList = new ArrayList<NodeId>(nodes.keySet());\n+    // Sort the nodes by space available on them, so that we offer\n+    // containers on emptier nodes first, facilitating an even spread. This\n+    // requires holding the scheduler lock, so that the space available on a\n+    // node doesn't change during the sort.\n+    synchronized (this) {\n+      Collections.sort(nodeIdList, nodeAvailableResourceComparator);\n+    }\n \n-      // iterate all nodes\n-      for (NodeId nodeId : nodeIdList) {\n-        if (nodes.containsKey(nodeId)) {\n-          FSSchedulerNode node = getFSSchedulerNode(nodeId);\n-          try {\n-            if (Resources.fitsIn(minimumAllocation,\n-                    node.getAvailableResource())) {\n-              attemptScheduling(node);\n-            }\n-          } catch (Throwable ex) {\n-            LOG.warn(\"Error while attempting scheduling for node \" + node +\n-                    \": \" + ex.toString(), ex);\n-          }\n-        }\n-      }\n+    // iterate all nodes\n+    for (NodeId nodeId : nodeIdList) {\n+      FSSchedulerNode node = getFSSchedulerNode(nodeId);\n       try {\n-        Thread.sleep(getContinuousSchedulingSleepMs());\n-      } catch (InterruptedException e) {\n-        LOG.warn(\"Error while doing sleep in continuous scheduling: \" +\n-                e.toString(), e);\n+        if (node != null && Resources.fitsIn(minimumAllocation,\n+            node.getAvailableResource())) {\n+          attemptScheduling(node);\n+        }\n+      } catch (Throwable ex) {\n+        LOG.error(\"Error while attempting scheduling for node \" + node +\n+            \": \" + ex.toString(), ex);\n       }\n     }\n   }\n@@ -1010,6 +1000,12 @@ private void continuousScheduling() {\n \n     @Override\n     public int compare(NodeId n1, NodeId n2) {\n+      if (!nodes.containsKey(n1)) {\n+        return 1;\n+      }\n+      if (!nodes.containsKey(n2)) {\n+        return -1;\n+      }\n       return RESOURCE_CALCULATOR.compare(clusterResource,\n               nodes.get(n2).getAvailableResource(),\n               nodes.get(n1).getAvailableResource());\n@@ -1234,7 +1230,16 @@ private synchronized void initScheduler(Configuration conf)\n           new Runnable() {\n             @Override\n             public void run() {\n-              continuousScheduling();\n+              while (!Thread.currentThread().isInterrupted()) {\n+                try {\n+                  continuousSchedulingAttempt();\n+                  Thread.sleep(getContinuousSchedulingSleepMs());\n+                } catch (InterruptedException e) {\n+                  LOG.error(\"Continuous scheduling thread interrupted. Exiting. \",\n+                      e);\n+                  return;\n+                }\n+              }\n             }\n           }\n       );",
                "raw_url": "https://github.com/apache/hadoop-common/raw/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "sha": "c0687bcbc249c470465c73ebeda16c64c623b9c6",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hadoop-common/blob/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java?ref=17a47c189460dc5183582ceb370b3f3f226fb1ae",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "patch": "@@ -2763,7 +2763,43 @@ public void testContinuousScheduling() throws Exception {\n     Assert.assertEquals(2, nodes.size());\n   }\n \n-  \n+  @Test\n+  public void testContinuousSchedulingWithNodeRemoved() throws Exception {\n+    // Disable continuous scheduling, will invoke continuous scheduling once manually\n+    scheduler.init(conf);\n+    scheduler.start();\n+    Assert.assertTrue(\"Continuous scheduling should be disabled.\",\n+        !scheduler.isContinuousSchedulingEnabled());\n+\n+    // Add two nodes\n+    RMNode node1 =\n+        MockNodes.newNodeInfo(1, Resources.createResource(8 * 1024, 8), 1,\n+            \"127.0.0.1\");\n+    NodeAddedSchedulerEvent nodeEvent1 = new NodeAddedSchedulerEvent(node1);\n+    scheduler.handle(nodeEvent1);\n+    RMNode node2 =\n+        MockNodes.newNodeInfo(1, Resources.createResource(8 * 1024, 8), 2,\n+            \"127.0.0.2\");\n+    NodeAddedSchedulerEvent nodeEvent2 = new NodeAddedSchedulerEvent(node2);\n+    scheduler.handle(nodeEvent2);\n+    Assert.assertEquals(\"We should have two alive nodes.\",\n+        2, scheduler.getNumClusterNodes());\n+\n+    // Remove one node\n+    NodeRemovedSchedulerEvent removeNode1 = new NodeRemovedSchedulerEvent(node1);\n+    scheduler.handle(removeNode1);\n+    Assert.assertEquals(\"We should only have one alive node.\",\n+        1, scheduler.getNumClusterNodes());\n+\n+    // Invoke the continuous scheduling once\n+    try {\n+      scheduler.continuousSchedulingAttempt();\n+    } catch (Exception e) {\n+      fail(\"Exception happened when doing continuous scheduling. \" +\n+        e.toString());\n+    }\n+  }\n+\n   @Test\n   public void testDontAllowUndeclaredPools() throws Exception{\n     conf.setBoolean(FairSchedulerConfiguration.ALLOW_UNDECLARED_POOLS, false);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "sha": "df157e75001fd51b7687096383deb70b4eb25d4b",
                "status": "modified"
            }
        ],
        "message": "YARN-2273. NPE in ContinuousScheduling thread when we lose a node. (Wei Yan via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612720 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/0470b4182114fd42c01517ff2ef0eacca340ae1f",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    },
    "hadoop-common_1bd18c3": {
        "bug_id": "hadoop-common_1bd18c3",
        "commit": "https://github.com/apache/hadoop-common/commit/1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -37,6 +37,9 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    MAPREDUCE-1089. Fix NPE in fair scheduler preemption when tasks are\n+    scheduled but not running. (Todd Lipcon via matei)\n+\n     MAPREDUCE-1014. Fix the libraries for common and hdfs. (omalley)\n \n     MAPREDUCE-1111. JT Jetty UI not working if we run mumak.sh ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/CHANGES.txt",
                "sha": "0571389832cb58552b200f56fa0479f7524b073d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java?ref=1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
                "deletions": 1,
                "filename": "src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "patch": "@@ -831,7 +831,11 @@ protected int tasksToPreempt(PoolSchedulable sched, long curTime) {\n     List<TaskStatus> statuses = new ArrayList<TaskStatus>();\n     for (TaskInProgress tip: tips) {\n       for (TaskAttemptID id: tip.getActiveTasks().keySet()) {\n-        statuses.add(tip.getTaskStatus(id));\n+        TaskStatus stat = tip.getTaskStatus(id);\n+        // status is null when the task has been scheduled but not yet running\n+        if (stat != null) {\n+          statuses.add(stat);\n+        }\n       }\n     }\n     return statuses;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "sha": "ae0930c46581cef366f6fcd59d3ca7408b5383e3",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1089. Fix NPE in fair scheduler preemption when tasks are  \nscheduled but not running. Contributed by Todd Lipcon.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@830821 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/a98327008f5b124724f3577bde06f2b4ddec6a18",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    },
    "hadoop-common_1f04f4b": {
        "bug_id": "hadoop-common_1f04f4b",
        "commit": "https://github.com/apache/hadoop-common/commit/1f04f4baf46b45468f0d15d16c71c769c4820827",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=1f04f4baf46b45468f0d15d16c71c769c4820827",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -324,6 +324,9 @@ Release 2.0.4-beta - UNRELEASED\n     but not in dfs.namenode.edits.dir are silently ignored.  (Arpit Agarwal\n     via szetszwo)\n \n+    HDFS-4482. ReplicationMonitor thread can exit with NPE due to the race \n+    between delete and replication of same file. (umamahesh)\n+\n Release 2.0.3-alpha - 2013-02-06\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "3b1a1e36bceb5c82791b0db68a2ddae8471b5c0a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=1f04f4baf46b45468f0d15d16c71c769c4820827",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1343,6 +1343,11 @@ static String getFullPathName(INode inode) {\n \n     // fill up the inodes in the path from this inode to root\n     for (int i = 0; i < depth; i++) {\n+      if (inode == null) {\n+        NameNode.stateChangeLog.warn(\"Could not get full path.\"\n+            + \" Corresponding file might have deleted already.\");\n+        return null;\n+      }\n       inodes[depth-i-1] = inode;\n       inode = inode.parent;\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "b11059a4bb47c865d14e439b1bbabfa418b90e2b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java?ref=1f04f4baf46b45468f0d15d16c71c769c4820827",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "patch": "@@ -282,7 +282,11 @@ String getLocalName() {\n \n   String getLocalParentDir() {\n     INode inode = isRoot() ? this : getParent();\n-    return (inode != null) ? inode.getFullPathName() : \"\";\n+    String parentDir = \"\";\n+    if (inode != null) {\n+      parentDir = inode.getFullPathName();\n+    }\n+    return (parentDir != null) ? parentDir : \"\";\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "sha": "b407a62da97b1808bbbc8d7169ecd48a9b05292a",
                "status": "modified"
            }
        ],
        "message": "HDFS-4482. ReplicationMonitor thread can exit with NPE due to the race between delete and replication of same file. Contributed by Uma Maheswara Rao G.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448708 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/57181e75b9305bf18ed50eb905d7f7a6c013b340",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSDirectory.java",
            "TestINode.java"
        ]
    },
    "hadoop-common_1f471a2": {
        "bug_id": "hadoop-common_1f471a2",
        "commit": "https://github.com/apache/hadoop-common/commit/1f471a2232362ae29911a6ca50dd4cd330faa225",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f471a2232362ae29911a6ca50dd4cd330faa225/CHANGES.txt",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=1f471a2232362ae29911a6ca50dd4cd330faa225",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -2,6 +2,19 @@ Hadoop MapReduce Change Log\n \n Trunk (unreleased changes)\n \n+  INCOMPATIBLE CHANGES\n+\n+  NEW FEATURES\n+\n+  IMPROVEMENTS\n+\n+  OPTIMIZATIONS\n+\n+  BUG FIXES\n+\n+    MAPREDUCE-1707. TaskRunner can get NPE in getting ugi from TaskTracker.\n+    (Vinod Kumar Vavilapalli)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f471a2232362ae29911a6ca50dd4cd330faa225/CHANGES.txt",
                "sha": "ba5fd8006ab37795cc780e75a98ef49216e23d22",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f471a2232362ae29911a6ca50dd4cd330faa225/src/java/org/apache/hadoop/mapred/TaskRunner.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/TaskRunner.java?ref=1f471a2232362ae29911a6ca50dd4cd330faa225",
                "deletions": 5,
                "filename": "src/java/org/apache/hadoop/mapred/TaskRunner.java",
                "patch": "@@ -177,9 +177,7 @@ public final void run() {\n \n       // We don't create any symlinks yet, so presence/absence of workDir\n       // actually on the file system doesn't matter.\n-      UserGroupInformation ugi = \n-        tracker.getRunningJob(t.getJobID()).getUGI();\n-      ugi.doAs(new PrivilegedExceptionAction<Void>() {\n+      tip.getUGI().doAs(new PrivilegedExceptionAction<Void>() {\n         public Void run() throws IOException {\n           taskDistributedCacheManager = \n             tracker.getTrackerDistributedCacheManager()\n@@ -256,8 +254,9 @@ public Void run() throws IOException {\n       }\n     } finally {\n       try{\n-        taskDistributedCacheManager.release();\n-\n+        if (taskDistributedCacheManager != null) {\n+          taskDistributedCacheManager.release();\n+        }\n       }catch(IOException ie){\n         LOG.warn(\"Error releasing caches : Cache files might not have been cleaned up\");\n       }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f471a2232362ae29911a6ca50dd4cd330faa225/src/java/org/apache/hadoop/mapred/TaskRunner.java",
                "sha": "02ae6f88b2e2eefd17bc9cded4b449adc160d224",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f471a2232362ae29911a6ca50dd4cd330faa225/src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/TaskTracker.java?ref=1f471a2232362ae29911a6ca50dd4cd330faa225",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "patch": "@@ -1156,9 +1156,11 @@ private void localizeJobJarFile(String user, JobID jobId, FileSystem localFs,\n     }\n   }\n \n-  private void launchTaskForJob(TaskInProgress tip, JobConf jobConf) throws IOException{\n+  private void launchTaskForJob(TaskInProgress tip, JobConf jobConf,\n+      UserGroupInformation ugi) throws IOException {\n     synchronized (tip) {\n       tip.setJobConf(jobConf);\n+      tip.setUGI(ugi);\n       tip.launchTask();\n     }\n   }\n@@ -2232,7 +2234,8 @@ private TaskInProgress registerTask(LaunchTaskAction action,\n   void startNewTask(TaskInProgress tip) {\n     try {\n       RunningJob rjob = localizeJob(tip);\n-      launchTaskForJob(tip, new JobConf(rjob.jobConf)); \n+      // Localization is done. Neither rjob.jobConf nor rjob.ugi can be null\n+      launchTaskForJob(tip, new JobConf(rjob.jobConf), rjob.ugi); \n     } catch (Throwable e) {\n       String msg = (\"Error initializing \" + tip.getTask().getTaskID() + \n                     \":\\n\" + StringUtils.stringifyException(e));\n@@ -2373,7 +2376,19 @@ public void run() {\n     private String debugCommand;\n     private volatile boolean slotTaken = false;\n     private TaskLauncher launcher;\n-        \n+\n+    // The ugi of the user who is running the job. This contains all the tokens\n+    // too which will be populated during job-localization\n+    private UserGroupInformation ugi;\n+\n+    UserGroupInformation getUGI() {\n+      return ugi;\n+    }\n+\n+    void setUGI(UserGroupInformation userUGI) {\n+      ugi = userUGI;\n+    }\n+\n     /**\n      */\n     public TaskInProgress(Task task, JobConf conf) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f471a2232362ae29911a6ca50dd4cd330faa225/src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "sha": "ac5d47519db14c93fc1d90edf57ae14011730fae",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1707. TaskRunner can get NPE in getting ugi from TaskTracker. Contributed by Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@940740 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/77be52c3293b52e329fd08baee7969b8e27ce943",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestTaskRunner.java"
        ]
    },
    "hadoop-common_1f653c8": {
        "bug_id": "hadoop-common_1f653c8",
        "commit": "https://github.com/apache/hadoop-common/commit/1f653c81fac98662ff276f4d65c8eeb0cecbeced",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=1f653c81fac98662ff276f4d65c8eeb0cecbeced",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -413,6 +413,8 @@ Release 2.4.0 - UNRELEASED\n     YARN-1768. Fixed error message being too verbose when killing a non-existent\n     application\n     \n+    YARN-1774. FS: Submitting to non-leaf queue throws NPE. (Anubhav Dhoot and\n+    Karthik Kambatla via kasha)\n \n Release 2.3.1 - UNRELEASED\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/CHANGES.txt",
                "sha": "9286a7ead8254cf637f41654d9c50c6958e7e261",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java?ref=1f653c81fac98662ff276f4d65c8eeb0cecbeced",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "patch": "@@ -611,9 +611,6 @@ protected synchronized void addApplication(ApplicationId applicationId,\n     RMApp rmApp = rmContext.getRMApps().get(applicationId);\n     FSLeafQueue queue = assignToQueue(rmApp, queueName, user);\n     if (queue == null) {\n-      rmContext.getDispatcher().getEventHandler().handle(\n-          new RMAppRejectedEvent(applicationId,\n-              \"Application rejected by queue placement policy\"));\n       return;\n     }\n \n@@ -679,27 +676,43 @@ protected synchronized void addApplicationAttempt(\n         new RMAppAttemptEvent(applicationAttemptId,\n             RMAppAttemptEventType.ATTEMPT_ADDED));\n   }\n-  \n+\n+  /**\n+   * Helper method that attempts to assign the app to a queue. The method is\n+   * responsible to call the appropriate event-handler if the app is rejected.\n+   */\n   @VisibleForTesting\n   FSLeafQueue assignToQueue(RMApp rmApp, String queueName, String user) {\n     FSLeafQueue queue = null;\n+    String appRejectMsg = null;\n+\n     try {\n       QueuePlacementPolicy placementPolicy = allocConf.getPlacementPolicy();\n       queueName = placementPolicy.assignAppToQueue(queueName, user);\n       if (queueName == null) {\n-        return null;\n+        appRejectMsg = \"Application rejected by queue placement policy\";\n+      } else {\n+        queue = queueMgr.getLeafQueue(queueName, true);\n+        if (queue == null) {\n+          appRejectMsg = queueName + \" is not a leaf queue\";\n+        }\n       }\n-      queue = queueMgr.getLeafQueue(queueName, true);\n-    } catch (IOException ex) {\n-      LOG.error(\"Error assigning app to queue, rejecting\", ex);\n+    } catch (IOException ioe) {\n+      appRejectMsg = \"Error assigning app to queue \" + queueName;\n     }\n-    \n+\n+    if (appRejectMsg != null && rmApp != null) {\n+      LOG.error(appRejectMsg);\n+      rmContext.getDispatcher().getEventHandler().handle(\n+          new RMAppRejectedEvent(rmApp.getApplicationId(), appRejectMsg));\n+      return null;\n+    }\n+\n     if (rmApp != null) {\n       rmApp.setQueue(queue.getName());\n     } else {\n-      LOG.warn(\"Couldn't find RM app to set queue name on\");\n+      LOG.error(\"Couldn't find RM app to set queue name on\");\n     }\n-    \n     return queue;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "sha": "3cdff7f405563bc14e3c7ac7b5522ca977a25a54",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java?ref=1f653c81fac98662ff276f4d65c8eeb0cecbeced",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "patch": "@@ -704,6 +704,22 @@ public void testAssignToQueue() throws Exception {\n     assertEquals(rmApp2.getQueue(), queue2.getName());\n     assertEquals(\"root.notdefault\", rmApp2.getQueue());\n   }\n+\n+  @Test\n+  public void testAssignToNonLeafQueueReturnsNull() throws Exception {\n+    conf.set(FairSchedulerConfiguration.USER_AS_DEFAULT_QUEUE, \"true\");\n+    scheduler.reinitialize(conf, resourceManager.getRMContext());\n+\n+    scheduler.getQueueManager().getLeafQueue(\"root.child1.granchild\", true);\n+    scheduler.getQueueManager().getLeafQueue(\"root.child2\", true);\n+\n+    RMApp rmApp1 = new MockRMApp(0, 0, RMAppState.NEW);\n+    RMApp rmApp2 = new MockRMApp(1, 1, RMAppState.NEW);\n+\n+    // Trying to assign to non leaf queue would return null\n+    assertNull(scheduler.assignToQueue(rmApp1, \"root.child1\", \"tintin\"));\n+    assertNotNull(scheduler.assignToQueue(rmApp2, \"root.child2\", \"snowy\"));\n+  }\n   \n   @Test\n   public void testQueuePlacementWithPolicy() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "sha": "d1052bb165e6ae3798697208b56814da897157ba",
                "status": "modified"
            }
        ],
        "message": "YARN-1774. FS: Submitting to non-leaf queue throws NPE. (Anubhav Dhoot and Karthik Kambatla via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1575415 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/9dae58de3bfd4941f8ec1fc353055298b399d915",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    },
    "hadoop-common_2426d84": {
        "bug_id": "hadoop-common_2426d84",
        "commit": "https://github.com/apache/hadoop-common/commit/2426d84a3a13421abb9f48c81e479e935733c3f8",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -390,6 +390,8 @@ Release 0.21.0 - Unreleased\n \n     HDFS-665. TestFileAppend2 sometimes hangs. (hairong)\n \n+    HDFS-676. Fix NPE in FSDataset.updateReplicaUnderRecovery() (shv)\n+\n Release 0.20.1 - 2009-09-01\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/CHANGES.txt",
                "sha": "5abba3a61bbc02969fd3476b07cd9bfd7ae376a4",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 19,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "patch": "@@ -1982,19 +1982,21 @@ static ReplicaRecoveryInfo initReplicaRecovery(\n     return rur.createInfo();\n   }\n \n-  /** Update a replica of a block. */\n-  synchronized void updateReplica(final Block block, final long recoveryId,\n-      final long newlength) throws IOException {\n+  @Override // FSDatasetInterface\n+  public synchronized ReplicaInfo updateReplicaUnderRecovery(\n+                                    final Block oldBlock,\n+                                    final long recoveryId,\n+                                    final long newlength) throws IOException {\n     //get replica\n-    final ReplicaInfo replica = volumeMap.get(block.getBlockId());\n-    DataNode.LOG.info(\"updateReplica: block=\" + block\n+    final ReplicaInfo replica = volumeMap.get(oldBlock.getBlockId());\n+    DataNode.LOG.info(\"updateReplica: block=\" + oldBlock\n         + \", recoveryId=\" + recoveryId\n         + \", length=\" + newlength\n         + \", replica=\" + replica);\n \n     //check replica\n     if (replica == null) {\n-      throw new ReplicaNotFoundException(block);\n+      throw new ReplicaNotFoundException(oldBlock);\n     }\n \n     //check replica state\n@@ -2007,26 +2009,18 @@ synchronized void updateReplica(final Block block, final long recoveryId,\n     checkReplicaFiles(replica);\n \n     //update replica\n-    final ReplicaInfo finalized = (ReplicaInfo)updateReplicaUnderRecovery(\n-                                    replica, recoveryId, newlength);\n+    final FinalizedReplica finalized = updateReplicaUnderRecovery(\n+        (ReplicaUnderRecovery)replica, recoveryId, newlength);\n \n     //check replica files after update\n     checkReplicaFiles(finalized);\n+    return finalized;\n   }\n \n-  @Override // FSDatasetInterface\n-  public synchronized FinalizedReplica updateReplicaUnderRecovery(\n-                                          Block oldBlock,\n+  private FinalizedReplica updateReplicaUnderRecovery(\n+                                          ReplicaUnderRecovery rur,\n                                           long recoveryId,\n                                           long newlength) throws IOException {\n-    Replica r = getReplica(oldBlock.getBlockId());\n-    if(r.getState() != ReplicaState.RUR)\n-      throw new IOException(\"Replica \" + r + \" must be under recovery.\");\n-    ReplicaUnderRecovery rur = (ReplicaUnderRecovery)r;\n-    DataNode.LOG.info(\"updateReplicaUnderRecovery: recoveryId=\" + recoveryId\n-        + \", newlength=\" + newlength\n-        + \", rur=\" + rur);\n-\n     //check recovery id\n     if (rur.getRecoveryID() != recoveryId) {\n       throw new IOException(\"rur.getRecoveryID() != recoveryId = \" + recoveryId",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "sha": "12d08168eac8d3840933a776ea336865423b2838",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "patch": "@@ -350,7 +350,8 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n   /**\n    * Update replica's generation stamp and length and finalize it.\n    */\n-  public FinalizedReplica updateReplicaUnderRecovery(Block oldBlock,\n+  public ReplicaInfo updateReplicaUnderRecovery(\n+                                          Block oldBlock,\n                                           long recoveryId,\n                                           long newLength) throws IOException;\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "sha": "462e1fffc573d6230dc402b6e34066fd7dd325f1",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 3,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "patch": "@@ -804,10 +804,10 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n     return new ReplicaRecoveryInfo(rBlock.getBlock(), ReplicaState.FINALIZED);\n   }\n \n-  @Override\n+  @Override // FSDatasetInterface\n   public FinalizedReplica updateReplicaUnderRecovery(Block oldBlock,\n-                                          long recoveryId,\n-                                          long newlength) throws IOException {\n+                                        long recoveryId,\n+                                        long newlength) throws IOException {\n     return new FinalizedReplica(\n         oldBlock.getBlockId(), newlength, recoveryId, null, null);\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "sha": "1d9fe16525aab45d39a39a2ff7358ab4650c3c0e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 3,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "patch": "@@ -234,9 +234,8 @@ public void testUpdateReplicaUnderRecovery() throws IOException {\n       FSDataset.checkReplicaFiles(rur);\n \n       //update\n-      final ReplicaInfo finalized = \n-        (ReplicaInfo)fsdataset.updateReplicaUnderRecovery(\n-            rur, recoveryid, newlength);\n+      final ReplicaInfo finalized = fsdataset.updateReplicaUnderRecovery(\n+                                                rur, recoveryid, newlength);\n \n       //check meta data after update\n       FSDataset.checkReplicaFiles(finalized);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "sha": "40d15a6f02c0cd211df29bfad0ebed066a4c5b4c",
                "status": "modified"
            }
        ],
        "message": "HDFS-676. Fix NPE in FSDataset.updateReplicaUnderRecovery(). Contributed by Konstantin Shvachko.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@823732 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f2809529a9d80906c53ae6931e84255f89ff1f8b",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestSimulatedFSDataset.java"
        ]
    },
    "hadoop-common_24275fd": {
        "bug_id": "hadoop-common_24275fd",
        "commit": "https://github.com/apache/hadoop-common/commit/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=24275fdc0f5dddee3cccadf570f7361a8cd8d5e3",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -656,6 +656,9 @@ Release 2.5.0 - UNRELEASED\n     HDFS-6552. add DN storage to a BlockInfo will not replace the different\n     storage from same DN. (Amir Langer via Arpit Agarwal)\n \n+    HDFS-6551. Rename with OVERWRITE option may throw NPE when the target\n+    file/directory is a reference INode. (jing9)\n+\n   BREAKDOWN OF HDFS-2006 SUBTASKS AND RELATED JIRAS\n \n     HDFS-6299. Protobuf for XAttr and client-side implementation. (Yi Liu via umamahesh)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "f0a84bd7d224030b1d5c5518bd0a997e33a2152d",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=24275fdc0f5dddee3cccadf570f7361a8cd8d5e3",
                "deletions": 9,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -44,7 +44,6 @@\n import org.apache.hadoop.fs.XAttrSetFlag;\n import org.apache.hadoop.fs.permission.AclEntry;\n import org.apache.hadoop.fs.permission.AclStatus;\n-import org.apache.hadoop.fs.permission.FsAction;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.fs.permission.PermissionStatus;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n@@ -891,9 +890,10 @@ boolean unprotectedRenameTo(String src, String dst, long timestamp,\n     \n     boolean undoRemoveDst = false;\n     INode removedDst = null;\n+    long removedNum = 0;\n     try {\n       if (dstInode != null) { // dst exists remove it\n-        if (removeLastINode(dstIIP) != -1) {\n+        if ((removedNum = removeLastINode(dstIIP)) != -1) {\n           removedDst = dstIIP.getLastINode();\n           undoRemoveDst = true;\n         }\n@@ -933,13 +933,15 @@ boolean unprotectedRenameTo(String src, String dst, long timestamp,\n         long filesDeleted = -1;\n         if (removedDst != null) {\n           undoRemoveDst = false;\n-          BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n-          List<INode> removedINodes = new ChunkedArrayList<INode>();\n-          filesDeleted = removedDst.cleanSubtree(Snapshot.CURRENT_STATE_ID,\n-              dstIIP.getLatestSnapshotId(), collectedBlocks, removedINodes, true)\n-              .get(Quota.NAMESPACE);\n-          getFSNamesystem().removePathAndBlocks(src, collectedBlocks,\n-              removedINodes);\n+          if (removedNum > 0) {\n+            BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n+            List<INode> removedINodes = new ChunkedArrayList<INode>();\n+            filesDeleted = removedDst.cleanSubtree(Snapshot.CURRENT_STATE_ID,\n+                dstIIP.getLatestSnapshotId(), collectedBlocks, removedINodes,\n+                true).get(Quota.NAMESPACE);\n+            getFSNamesystem().removePathAndBlocks(src, collectedBlocks,\n+                removedINodes);\n+          }\n         }\n \n         if (snapshottableDirs.size() > 0) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "dc553ab73b96d1c636cdf5fb23a33ebf4ccfec8c",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hadoop-common/blob/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java?ref=24275fdc0f5dddee3cccadf570f7361a8cd8d5e3",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "patch": "@@ -171,8 +171,6 @@ public void testRenameFromSDir2NonSDir() throws Exception {\n   private static boolean existsInDiffReport(List<DiffReportEntry> entries,\n       DiffType type, String relativePath) {\n     for (DiffReportEntry entry : entries) {\n-      System.out.println(\"DiffEntry is:\" + entry.getType() + \"\\\"\"\n-          + new String(entry.getRelativePath()) + \"\\\"\");\n       if ((entry.getType() == type)\n           && ((new String(entry.getRelativePath())).compareTo(relativePath) == 0)) {\n         return true;\n@@ -2374,4 +2372,46 @@ public void testAppendFileAfterRenameInSnapshot() throws Exception {\n     // save namespace and restart\n     restartClusterAndCheckImage(true);\n   }\n+\n+  @Test\n+  public void testRenameWithOverWrite() throws Exception {\n+    final Path root = new Path(\"/\");\n+    final Path foo = new Path(root, \"foo\");\n+    final Path file1InFoo = new Path(foo, \"file1\");\n+    final Path file2InFoo = new Path(foo, \"file2\");\n+    final Path file3InFoo = new Path(foo, \"file3\");\n+    DFSTestUtil.createFile(hdfs, file1InFoo, 1L, REPL, SEED);\n+    DFSTestUtil.createFile(hdfs, file2InFoo, 1L, REPL, SEED);\n+    DFSTestUtil.createFile(hdfs, file3InFoo, 1L, REPL, SEED);\n+    final Path bar = new Path(root, \"bar\");\n+    hdfs.mkdirs(bar);\n+\n+    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n+    // move file1 from foo to bar\n+    final Path fileInBar = new Path(bar, \"file1\");\n+    hdfs.rename(file1InFoo, fileInBar);\n+    // rename bar to newDir\n+    final Path newDir = new Path(root, \"newDir\");\n+    hdfs.rename(bar, newDir);\n+    // move file2 from foo to newDir\n+    final Path file2InNewDir = new Path(newDir, \"file2\");\n+    hdfs.rename(file2InFoo, file2InNewDir);\n+    // move file3 from foo to newDir and rename it to file1, this will overwrite\n+    // the original file1\n+    final Path file1InNewDir = new Path(newDir, \"file1\");\n+    hdfs.rename(file3InFoo, file1InNewDir, Rename.OVERWRITE);\n+    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n+\n+    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(root, \"s0\", \"s1\");\n+    LOG.info(\"DiffList is \\n\\\"\" + report.toString() + \"\\\"\");\n+    List<DiffReportEntry> entries = report.getDiffList();\n+    assertEquals(7, entries.size());\n+    assertTrue(existsInDiffReport(entries, DiffType.MODIFY, \"\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.MODIFY, foo.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, bar.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.CREATE, newDir.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file1\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file2\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file3\"));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "sha": "c88aaf2410e8b34fd0303b6546aeb9249a2e51b5",
                "status": "modified"
            }
        ],
        "message": "HDFS-6551. Rename with OVERWRITE option may throw NPE when the target file/directory is a reference INode. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603612 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f7e0a9792a93d8f8bf982295b92003f186024301",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSDirectory.java"
        ]
    },
    "hadoop-common_279d851": {
        "bug_id": "hadoop-common_279d851",
        "commit": "https://github.com/apache/hadoop-common/commit/279d851c47c6fc994affcbaf04c49ff102e96b5c",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "patch": "@@ -69,3 +69,5 @@ HDFS-5535 subtasks:\n     HDFS-5987. Fix findbugs warnings in Rolling Upgrade branch. (seztszwo via\n     Arpit Agarwal)\n \n+    HDFS-5992. Fix NPE in MD5FileUtils and update editsStored for\n+    TestOfflineEditsViewer.  (szetszwo)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "sha": "4c567902c863adaa5719b632106cb992a0247b66",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.BufferedReader;\n import java.io.File;\n import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.InputStreamReader;\n@@ -72,10 +73,6 @@ public static void verifySavedMD5(File dataFile, MD5Hash expectedMD5)\n    *   where group(1) is the md5 string and group(2) is the data file path.\n    */\n   private static Matcher readStoredMd5(File md5File) throws IOException {\n-    if (!md5File.exists()) {\n-      return null;\n-    }\n-    \n     BufferedReader reader =\n         new BufferedReader(new InputStreamReader(new FileInputStream(\n             md5File), Charsets.UTF_8));\n@@ -105,6 +102,10 @@ private static Matcher readStoredMd5(File md5File) throws IOException {\n    */\n   public static MD5Hash readStoredMd5ForFile(File dataFile) throws IOException {\n     final File md5File = getDigestFileForFile(dataFile);\n+    if (!md5File.exists()) {\n+      return null;\n+    }\n+\n     final Matcher matcher = readStoredMd5(md5File);\n     String storedHash = matcher.group(1);\n     File referencedFile = new File(matcher.group(2));\n@@ -165,6 +166,10 @@ private static void saveMD5File(File dataFile, String digestString)\n   public static void renameMD5File(File oldDataFile, File newDataFile)\n       throws IOException {\n     final File fromFile = getDigestFileForFile(oldDataFile);\n+    if (!fromFile.exists()) {\n+      throw new FileNotFoundException(fromFile + \" does not exist.\");\n+    }\n+\n     final String digestString = readStoredMd5(fromFile).group(1);\n     saveMD5File(newDataFile, digestString);\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "sha": "d87ffbf3154236c316020cdb0520469379d87d12",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "patch": "@@ -130,7 +130,10 @@ private CheckpointSignature runOperations() throws IOException {\n     DFSTestUtil.runOperations(cluster, dfs, cluster.getConfiguration(0),\n         dfs.getDefaultBlockSize(), 0);\n \n+    // OP_ROLLING_UPGRADE_START\n     cluster.getNamesystem().getEditLog().logStartRollingUpgrade(Time.now());\n+    // OP_ROLLING_UPGRADE_FINALIZE\n+    cluster.getNamesystem().getEditLog().logFinalizeRollingUpgrade(Time.now());\n \n     // Force a roll so we get an OP_END_LOG_SEGMENT txn\n     return cluster.getNameNodeRpc().rollEditLog();",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "sha": "c6364b174d0b0763c271e09fff004d0b2abdcab0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "patch": "@@ -95,6 +95,7 @@ public void testGenerated() throws IOException {\n     // edits generated by nnHelper (MiniDFSCluster), should have all op codes\n     // binary, XML, reparsed binary\n     String edits = nnHelper.generateEdits();\n+    LOG.info(\"Generated edits=\" + edits);\n     String editsParsedXml = folder.newFile(\"editsParsed.xml\").getAbsolutePath();\n     String editsReparsed = folder.newFile(\"editsParsed\").getAbsolutePath();\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "sha": "19d98ab6988a92753296f65452c519f0fb878653",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "sha": "6c17f4af4a9ec863a56a6b27dec867a38eb6210e",
                "status": "modified"
            },
            {
                "additions": 102,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "changes": 578,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 476,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "patch": "@@ -1,10 +1,6 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <EDITS>\n-<<<<<<< .working\n-  <EDITS_VERSION>-50</EDITS_VERSION>\n-=======\n-  <EDITS_VERSION>-51</EDITS_VERSION>\n->>>>>>> .merge-right.r1559304\n+  <EDITS_VERSION>-55</EDITS_VERSION>\n   <RECORD>\n     <OPCODE>OP_START_LOG_SEGMENT</OPCODE>\n     <DATA>\n@@ -17,13 +13,8 @@\n       <TXID>2</TXID>\n       <DELEGATION_KEY>\n         <KEY_ID>1</KEY_ID>\n-<<<<<<< .working\n-        <EXPIRY_DATE>1389421314720</EXPIRY_DATE>\n-        <KEY>d2a03d66ebfac521</KEY>\n-=======\n-        <EXPIRY_DATE>1390519460949</EXPIRY_DATE>\n-        <KEY>dc8d30edc97df67d</KEY>\n->>>>>>> .merge-right.r1559304\n+        <EXPIRY_DATE>1393648283650</EXPIRY_DATE>\n+        <KEY>76e6d2854a753680</KEY>\n       </DELEGATION_KEY>\n     </DATA>\n   </RECORD>\n@@ -33,13 +24,8 @@\n       <TXID>3</TXID>\n       <DELEGATION_KEY>\n         <KEY_ID>2</KEY_ID>\n-<<<<<<< .working\n-        <EXPIRY_DATE>1389421314722</EXPIRY_DATE>\n-        <KEY>ef94532092f55aef</KEY>\n-=======\n-        <EXPIRY_DATE>1390519460952</EXPIRY_DATE>\n-        <KEY>096bc20b6debed03</KEY>\n->>>>>>> .merge-right.r1559304\n+        <EXPIRY_DATE>1393648283653</EXPIRY_DATE>\n+        <KEY>939fb7b875c956cd</KEY>\n       </DELEGATION_KEY>\n     </DATA>\n   </RECORD>\n@@ -51,36 +37,18 @@\n       <INODEID>16386</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115316</MTIME>\n-      <ATIME>1388730115316</ATIME>\n-=======\n-      <MTIME>1389828264873</MTIME>\n-      <ATIME>1389828264873</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084379</MTIME>\n+      <ATIME>1392957084379</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>6</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>9</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>7</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -91,22 +59,13 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115327</MTIME>\n-      <ATIME>1388730115316</ATIME>\n-=======\n-      <MTIME>1389828265699</MTIME>\n-      <ATIME>1389828264873</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084397</MTIME>\n+      <ATIME>1392957084379</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -119,15 +78,9 @@\n       <LENGTH>0</LENGTH>\n       <SRC>/file_create</SRC>\n       <DST>/file_moved</DST>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115331</TIMESTAMP>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>8</RPC_CALLID>\n-=======\n-      <TIMESTAMP>1389828265705</TIMESTAMP>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>11</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084400</TIMESTAMP>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>9</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -136,15 +89,9 @@\n       <TXID>7</TXID>\n       <LENGTH>0</LENGTH>\n       <PATH>/file_moved</PATH>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115336</TIMESTAMP>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>9</RPC_CALLID>\n-=======\n-      <TIMESTAMP>1389828265712</TIMESTAMP>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>12</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084413</TIMESTAMP>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>10</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -154,17 +101,9 @@\n       <LENGTH>0</LENGTH>\n       <INODEID>16387</INODEID>\n       <PATH>/directory_mkdir</PATH>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115342</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828265722</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084419</TIMESTAMP>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>493</MODE>\n       </PERMISSION_STATUS>\n@@ -197,13 +136,8 @@\n       <TXID>12</TXID>\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTNAME>snapshot1</SNAPSHOTNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>14</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>17</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>15</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -213,13 +147,8 @@\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTOLDNAME>snapshot1</SNAPSHOTOLDNAME>\n       <SNAPSHOTNEWNAME>snapshot2</SNAPSHOTNEWNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>15</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>18</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>16</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -228,13 +157,8 @@\n       <TXID>14</TXID>\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTNAME>snapshot2</SNAPSHOTNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>16</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>19</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>17</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -245,36 +169,18 @@\n       <INODEID>16388</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115362</MTIME>\n-      <ATIME>1388730115362</ATIME>\n-=======\n-      <MTIME>1389828265757</MTIME>\n-      <ATIME>1389828265757</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084440</MTIME>\n+      <ATIME>1392957084440</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>17</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>20</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>18</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -285,22 +191,13 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115363</MTIME>\n-      <ATIME>1388730115362</ATIME>\n-=======\n-      <MTIME>1389828265759</MTIME>\n-      <ATIME>1389828265757</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084441</MTIME>\n+      <ATIME>1392957084440</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -356,19 +253,10 @@\n       <LENGTH>0</LENGTH>\n       <SRC>/file_create</SRC>\n       <DST>/file_moved</DST>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115378</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828265782</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084455</TIMESTAMP>\n       <OPTIONS>NONE</OPTIONS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>24</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>27</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>25</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -379,36 +267,18 @@\n       <INODEID>16389</INODEID>\n       <PATH>/file_concat_target</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115382</MTIME>\n-      <ATIME>1388730115382</ATIME>\n-=======\n-      <MTIME>1389828265787</MTIME>\n-      <ATIME>1389828265787</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084459</MTIME>\n+      <ATIME>1392957084459</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>26</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>29</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>27</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -513,13 +383,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_target</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115461</MTIME>\n-      <ATIME>1388730115382</ATIME>\n-=======\n-      <MTIME>1389828266540</MTIME>\n-      <ATIME>1389828265787</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084525</MTIME>\n+      <ATIME>1392957084459</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -539,11 +404,7 @@\n         <GENSTAMP>1003</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -557,36 +418,18 @@\n       <INODEID>16390</INODEID>\n       <PATH>/file_concat_0</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115463</MTIME>\n-      <ATIME>1388730115463</ATIME>\n-=======\n-      <MTIME>1389828266544</MTIME>\n-      <ATIME>1389828266544</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084527</MTIME>\n+      <ATIME>1392957084527</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>39</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>41</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>40</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -691,13 +534,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_0</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115477</MTIME>\n-      <ATIME>1388730115463</ATIME>\n-=======\n-      <MTIME>1389828266569</MTIME>\n-      <ATIME>1389828266544</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084542</MTIME>\n+      <ATIME>1392957084527</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -717,11 +555,7 @@\n         <GENSTAMP>1006</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -735,36 +569,18 @@\n       <INODEID>16391</INODEID>\n       <PATH>/file_concat_1</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115479</MTIME>\n-      <ATIME>1388730115479</ATIME>\n-=======\n-      <MTIME>1389828266572</MTIME>\n-      <ATIME>1389828266572</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084544</MTIME>\n+      <ATIME>1392957084544</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>51</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>53</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>52</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -869,13 +685,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_1</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115495</MTIME>\n-      <ATIME>1388730115479</ATIME>\n-=======\n-      <MTIME>1389828266599</MTIME>\n-      <ATIME>1389828266572</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084559</MTIME>\n+      <ATIME>1392957084544</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -895,11 +706,7 @@\n         <GENSTAMP>1009</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -911,22 +718,13 @@\n       <TXID>56</TXID>\n       <LENGTH>0</LENGTH>\n       <TRG>/file_concat_target</TRG>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115498</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828266603</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084561</TIMESTAMP>\n       <SOURCES>\n         <SOURCE1>/file_concat_0</SOURCE1>\n         <SOURCE2>/file_concat_1</SOURCE2>\n       </SOURCES>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>62</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>64</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>63</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -937,190 +735,37 @@\n       <INODEID>16392</INODEID>\n       <PATH>/file_symlink</PATH>\n       <VALUE>/file_concat_target</VALUE>\n-<<<<<<< .working\n-      <MTIME>1388730115502</MTIME>\n-      <ATIME>1388730115502</ATIME>\n-=======\n-      <MTIME>1389828266633</MTIME>\n-      <ATIME>1389828266633</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084564</MTIME>\n+      <ATIME>1392957084564</ATIME>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>511</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>63</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>66</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>64</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_ADD</OPCODE>\n     <DATA>\n       <TXID>58</TXID>\n-<<<<<<< .working\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-      <EXPIRY_TIME>1388816515505</EXPIRY_TIME>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_RENEW_DELEGATION_TOKEN</OPCODE>\n-    <DATA>\n-      <TXID>59</TXID>\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-      <EXPIRY_TIME>1388816515564</EXPIRY_TIME>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_CANCEL_DELEGATION_TOKEN</OPCODE>\n-    <DATA>\n-      <TXID>60</TXID>\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>61</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <OWNERNAME>szetszwo</OWNERNAME>\n-      <GROUPNAME>staff</GROUPNAME>\n-      <MODE>493</MODE>\n-      <LIMIT>9223372036854775807</LIMIT>\n-      <MAXRELATIVEEXPIRY>2305843009213693951</MAXRELATIVEEXPIRY>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>67</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_MODIFY_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>62</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <OWNERNAME>carlton</OWNERNAME>\n-      <GROUPNAME>party</GROUPNAME>\n-      <MODE>448</MODE>\n-      <LIMIT>1989</LIMIT>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>68</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>63</TXID>\n-      <ID>1</ID>\n-      <PATH>/bar</PATH>\n-      <REPLICATION>1</REPLICATION>\n-      <POOL>poolparty</POOL>\n-      <EXPIRATION>2305844397943809533</EXPIRATION>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>69</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_MODIFY_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>64</TXID>\n-      <ID>1</ID>\n-      <PATH>/bar2</PATH>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>70</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_REMOVE_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>65</TXID>\n-      <ID>1</ID>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>71</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_REMOVE_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>66</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>72</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD</OPCODE>\n-    <DATA>\n-      <TXID>67</TXID>\n-=======\n->>>>>>> .merge-right.r1559304\n       <LENGTH>0</LENGTH>\n       <INODEID>16393</INODEID>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115596</MTIME>\n-      <ATIME>1388730115596</ATIME>\n-=======\n-      <MTIME>1389828266637</MTIME>\n-      <ATIME>1389828266637</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084567</MTIME>\n+      <ATIME>1392957084567</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>73</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>67</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>65</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1175,38 +820,22 @@\n   <RECORD>\n     <OPCODE>OP_REASSIGN_LEASE</OPCODE>\n     <DATA>\n-<<<<<<< .working\n-      <TXID>73</TXID>\n-      <LEASEHOLDER>DFSClient_NONMAPREDUCE_381408282_1</LEASEHOLDER>\n-=======\n       <TXID>64</TXID>\n-      <LEASEHOLDER>DFSClient_NONMAPREDUCE_16108824_1</LEASEHOLDER>\n->>>>>>> .merge-right.r1559304\n+      <LEASEHOLDER>DFSClient_NONMAPREDUCE_-1178237747_1</LEASEHOLDER>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <NEWHOLDER>HDFS_NameNode</NEWHOLDER>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-<<<<<<< .working\n-    <OPCODE>OP_CLOSE</OPCODE>\n-    <DATA>\n-      <TXID>74</TXID>\n-=======\n     <OPCODE>OP_CLOSE</OPCODE>\n     <DATA>\n       <TXID>65</TXID>\n->>>>>>> .merge-right.r1559304\n       <LENGTH>0</LENGTH>\n       <INODEID>0</INODEID>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730118281</MTIME>\n-      <ATIME>1388730115596</ATIME>\n-=======\n-      <MTIME>1389828269751</MTIME>\n-      <ATIME>1389828266637</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957087263</MTIME>\n+      <ATIME>1392957084567</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -1216,36 +845,24 @@\n         <GENSTAMP>1011</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-<<<<<<< .working\n-    <OPCODE>OP_UPGRADE_MARKER</OPCODE>\n-    <DATA>\n-      <TXID>75</TXID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-=======\n     <OPCODE>OP_ADD_CACHE_POOL</OPCODE>\n     <DATA>\n       <TXID>66</TXID>\n       <POOLNAME>pool1</POOLNAME>\n-      <OWNERNAME>jing</OWNERNAME>\n+      <OWNERNAME>szetszwo</OWNERNAME>\n       <GROUPNAME>staff</GROUPNAME>\n       <MODE>493</MODE>\n       <LIMIT>9223372036854775807</LIMIT>\n       <MAXRELATIVEEXPIRY>2305843009213693951</MAXRELATIVEEXPIRY>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>74</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>72</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1254,8 +871,8 @@\n       <TXID>67</TXID>\n       <POOLNAME>pool1</POOLNAME>\n       <LIMIT>99</LIMIT>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>75</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>73</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1266,9 +883,9 @@\n       <PATH>/path</PATH>\n       <REPLICATION>1</REPLICATION>\n       <POOL>pool1</POOL>\n-      <EXPIRATION>2305844399041964876</EXPIRATION>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>76</RPC_CALLID>\n+      <EXPIRATION>2305844402170781554</EXPIRATION>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>74</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1277,44 +894,53 @@\n       <TXID>69</TXID>\n       <ID>1</ID>\n       <REPLICATION>2</REPLICATION>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>77</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>75</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_REMOVE_CACHE_DIRECTIVE</OPCODE>\n     <DATA>\n       <TXID>70</TXID>\n       <ID>1</ID>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>78</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>76</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_REMOVE_CACHE_POOL</OPCODE>\n     <DATA>\n       <TXID>71</TXID>\n       <POOLNAME>pool1</POOLNAME>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>79</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>77</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n->>>>>>> .merge-right.r1559304\n-    <OPCODE>OP_END_LOG_SEGMENT</OPCODE>\n+    <OPCODE>OP_SET_ACL</OPCODE>\n     <DATA>\n-<<<<<<< .working\n-      <TXID>76</TXID>\n-=======\n       <TXID>72</TXID>\n->>>>>>> .merge-right.r1559304\n+      <SRC>/file_concat_target</SRC>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-    <OPCODE>OP_SET_ACL</OPCODE>\n+    <OPCODE>OP_ROLLING_UPGRADE_START</OPCODE>\n     <DATA>\n       <TXID>73</TXID>\n-      <SRC>/file_set_acl</SRC>\n+      <STARTTIME>1392957087621</STARTTIME>\n+    </DATA>\n+  </RECORD>\n+  <RECORD>\n+    <OPCODE>OP_ROLLING_UPGRADE_FINALIZE</OPCODE>\n+    <DATA>\n+      <TXID>74</TXID>\n+      <FINALIZETIME>1392957087621</FINALIZETIME>\n+    </DATA>\n+  </RECORD>\n+  <RECORD>\n+    <OPCODE>OP_END_LOG_SEGMENT</OPCODE>\n+    <DATA>\n+      <TXID>75</TXID>\n     </DATA>\n   </RECORD>\n </EDITS>",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "sha": "b3115591d0327d924a4eb098147c8da374327d5b",
                "status": "modified"
            }
        ],
        "message": "HDFS-5992. Fix NPE in MD5FileUtils and update editsStored for TestOfflineEditsViewer.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1570690 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/65c28e1a6fea59574d6f8f6a9688df3361d56562",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestMD5FileUtils.java"
        ]
    },
    "hadoop-common_29f5383": {
        "bug_id": "hadoop-common_29f5383",
        "commit": "https://github.com/apache/hadoop-common/commit/29f5383330910e589757db7293dd6b3a094805a9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/29f5383330910e589757db7293dd6b3a094805a9/common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/common/CHANGES.txt?ref=29f5383330910e589757db7293dd6b3a094805a9",
                "deletions": 0,
                "filename": "common/CHANGES.txt",
                "patch": "@@ -346,6 +346,9 @@ Trunk (unreleased changes)\n     HADOOP-7090. Fix resource leaks in s3.INode, BloomMapFile, WritableUtils\n     and CBZip2OutputStream.  (Uma Maheswara Rao G via szetszwo)\n \n+    HADOOP-7440. HttpServer.getParameterValues throws NPE for missing\n+    parameters. (Uma Maheswara Rao G and todd via todd)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/29f5383330910e589757db7293dd6b3a094805a9/common/CHANGES.txt",
                "sha": "c84dc559c8557a4434498e93f6e7e12e1e42daef",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/29f5383330910e589757db7293dd6b3a094805a9/common/src/java/org/apache/hadoop/http/HttpServer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/common/src/java/org/apache/hadoop/http/HttpServer.java?ref=29f5383330910e589757db7293dd6b3a094805a9",
                "deletions": 0,
                "filename": "common/src/java/org/apache/hadoop/http/HttpServer.java",
                "patch": "@@ -800,6 +800,9 @@ public String getParameter(String name) {\n       public String[] getParameterValues(String name) {\n         String unquoteName = HtmlQuoting.unquoteHtmlChars(name);\n         String[] unquoteValue = rawRequest.getParameterValues(unquoteName);\n+        if (unquoteValue == null) {\n+          return null;\n+        }\n         String[] result = new String[unquoteValue.length];\n         for(int i=0; i < result.length; ++i) {\n           result[i] = HtmlQuoting.quoteHtmlChars(unquoteValue[i]);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/29f5383330910e589757db7293dd6b3a094805a9/common/src/java/org/apache/hadoop/http/HttpServer.java",
                "sha": "6d6864c63dcb62429fd757215868e1cae03c94fd",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop-common/blob/29f5383330910e589757db7293dd6b3a094805a9/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java?ref=29f5383330910e589757db7293dd6b3a094805a9",
                "deletions": 3,
                "filename": "common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "patch": "@@ -17,11 +17,12 @@\n  */\n package org.apache.hadoop.http;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.*;\n+\n+import javax.servlet.http.HttpServletRequest;\n \n import org.junit.Test;\n+import org.mockito.Mockito;\n \n public class TestHtmlQuoting {\n \n@@ -62,4 +63,28 @@ private void runRoundTrip(String str) throws Exception {\n     }\n     runRoundTrip(buffer.toString());\n   }\n+  \n+\n+  @Test\n+  public void testRequestQuoting() throws Exception {\n+    HttpServletRequest mockReq = Mockito.mock(HttpServletRequest.class);\n+    HttpServer.QuotingInputFilter.RequestQuoter quoter =\n+      new HttpServer.QuotingInputFilter.RequestQuoter(mockReq);\n+    \n+    Mockito.doReturn(\"a<b\").when(mockReq).getParameter(\"x\");\n+    assertEquals(\"Test simple param quoting\",\n+        \"a&lt;b\", quoter.getParameter(\"x\"));\n+    \n+    Mockito.doReturn(null).when(mockReq).getParameter(\"x\");\n+    assertEquals(\"Test that missing parameters dont cause NPE\",\n+        null, quoter.getParameter(\"x\"));\n+\n+    Mockito.doReturn(new String[]{\"a<b\", \"b\"}).when(mockReq).getParameterValues(\"x\");\n+    assertArrayEquals(\"Test escaping of an array\",\n+        new String[]{\"a&lt;b\", \"b\"}, quoter.getParameterValues(\"x\"));\n+\n+    Mockito.doReturn(null).when(mockReq).getParameterValues(\"x\");\n+    assertArrayEquals(\"Test that missing parameters dont cause NPE for array\",\n+        null, quoter.getParameterValues(\"x\"));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/29f5383330910e589757db7293dd6b3a094805a9/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "sha": "9fc53a3b6fb9d50beafba1b66721310239b18b8b",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop-common/blob/29f5383330910e589757db7293dd6b3a094805a9/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java?ref=29f5383330910e589757db7293dd6b3a094805a9",
                "deletions": 0,
                "filename": "common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "patch": "@@ -45,16 +45,20 @@\n import javax.servlet.http.HttpServletRequestWrapper;\n import javax.servlet.http.HttpServletResponse;\n \n+import junit.framework.Assert;\n+\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.http.HttpServer.QuotingInputFilter.RequestQuoter;\n import org.apache.hadoop.security.Groups;\n import org.apache.hadoop.security.ShellBasedUnixGroupsMapping;\n import org.apache.hadoop.security.authorize.AccessControlList;\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.Test;\n+import org.mockito.Mockito;\n \n public class TestHttpServer extends HttpServerFunctionalTest {\n   private static HttpServer server;\n@@ -379,4 +383,26 @@ public void testAuthorizationOfDefaultServlets() throws Exception {\n     }\n     myServer.stop();\n   }\n+  \n+  @Test\n+  public void testRequestQuoterWithNull() throws Exception {\n+    HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n+    Mockito.doReturn(null).when(request).getParameterValues(\"dummy\");\n+    RequestQuoter requestQuoter = new RequestQuoter(request);\n+    String[] parameterValues = requestQuoter.getParameterValues(\"dummy\");\n+    Assert.assertEquals(\"It should return null \"\n+        + \"when there are no values for the parameter\", null, parameterValues);\n+  }\n+\n+  @Test\n+  public void testRequestQuoterWithNotNull() throws Exception {\n+    HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n+    String[] values = new String[] { \"abc\", \"def\" };\n+    Mockito.doReturn(values).when(request).getParameterValues(\"dummy\");\n+    RequestQuoter requestQuoter = new RequestQuoter(request);\n+    String[] parameterValues = requestQuoter.getParameterValues(\"dummy\");\n+    Assert.assertTrue(\"It should return Parameter Values\", Arrays.equals(\n+        values, parameterValues));\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/29f5383330910e589757db7293dd6b3a094805a9/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "sha": "0a25236fc8381e3ece6424ab16a94c890afc2f0d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7440. HttpServer.getParameterValues throws NPE for missing parameters. Contributed by Uma Maheswara Rao G and Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1143212 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/9906f9949e30d3226bddc31c295d0f9fefe52986",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestHttpServer.java"
        ]
    },
    "hadoop-common_2c80616": {
        "bug_id": "hadoop-common_2c80616",
        "commit": "https://github.com/apache/hadoop-common/commit/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -232,6 +232,8 @@ Trunk (unreleased changes)\n     MAPREDUCE-2495. exit() the TaskTracker when the distributed cache cleanup\n     thread dies. (Robert Joseph Evans via cdouglas)\n \n+    MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters. (Robert Joseph Evans\n+    via cdouglas)\n \n Release 0.22.0 - Unreleased\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/CHANGES.txt",
                "sha": "267bbd5f6761ad691d7e255f6a8f1b766e31c5d1",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 6,
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -42,8 +42,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.ipc.RemoteException;\n-import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.util.Tool;\n@@ -149,7 +147,7 @@\n    * a JobProfile object to provide some info, and interacts with the\n    * remote service to provide certain functionality.\n    */\n-  class NetworkedJob implements RunningJob {\n+  static class NetworkedJob implements RunningJob {\n     Job job;\n     /**\n      * We store a JobProfile and a timestamp for when we last\n@@ -158,7 +156,7 @@\n      * has completely forgotten about the job.  (eg, 24 hours after the\n      * job completes.)\n      */\n-    public NetworkedJob(JobStatus status) throws IOException {\n+    public NetworkedJob(JobStatus status, Cluster cluster) throws IOException {\n       job = Job.getInstance(cluster, status, new JobConf(status.getJobFile()));\n     }\n \n@@ -380,7 +378,12 @@ public String toString() {\n      */\n     public Counters getCounters() throws IOException {\n       try { \n-        return Counters.downgrade(job.getCounters());\n+        Counters result = null;\n+        org.apache.hadoop.mapreduce.Counters temp = job.getCounters();\n+        if(temp != null) {\n+          result = Counters.downgrade(temp);\n+        }\n+        return result;\n       } catch (InterruptedException ie) {\n         throw new IOException(ie);\n       }\n@@ -557,7 +560,7 @@ public RunningJob getJob(JobID jobid) throws IOException {\n       if (job != null) {\n         JobStatus status = JobStatus.downgrade(job.getStatus());\n         if (status != null) {\n-          return new NetworkedJob(status);\n+          return new NetworkedJob(status, cluster);\n         } \n       }\n     } catch (InterruptedException ie) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "3b5f84bfe1fb6d4c0920e78cfbc20d3de32c28e1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/RunningJob.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/mapred/RunningJob.java",
                "patch": "@@ -193,7 +193,7 @@\n   /**\n    * Gets the counters for this job.\n    * \n-   * @return the counters for this job.\n+   * @return the counters for this job or null if the job has been retired.\n    * @throws IOException\n    */\n   public Counters getCounters() throws IOException;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "sha": "b3af4f6c98d5ebe15971387e23173e2e4a0513a8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "patch": "@@ -230,7 +230,7 @@ public Counters getJobCounters(JobID jobid)\n   \n   /**\n    * Get task completion events for the jobid, starting from fromEventId. \n-   * Returns empty aray if no events are available. \n+   * Returns empty array if no events are available. \n    * @param jobid job id \n    * @param fromEventId event id to start from.\n    * @param maxEvents the max number of events we want to look at ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "sha": "80e556bac2f3f4929cd313ea3e21c4a4839719c9",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.mapred;\n+\n+import static org.junit.Assert.*;\n+\n+import java.util.List;\n+\n+import org.apache.hadoop.mapreduce.Job;\n+import org.junit.Test;\n+import static org.mockito.Mockito.*;\n+\n+\n+public class TestNetworkedJob {\n+\n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testGetNullCounters() throws Exception {\n+    //mock creation\n+    Job mockJob = mock(Job.class);\n+    RunningJob underTest = new JobClient.NetworkedJob(mockJob); \n+\n+    when(mockJob.getCounters()).thenReturn(null);\n+    assertNull(underTest.getCounters());\n+    //verification\n+    verify(mockJob).getCounters();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "sha": "b6565e28956d7446628aae1da838b930a83313db",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 5,
                "filename": "src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java",
                "patch": "@@ -129,7 +129,7 @@ public void testFailedTaskJobStatus()\n     }\n     Assert.assertTrue(\"Task has not been started for 1 min.\", counter != 60);\n \n-    NetworkedJob networkJob = jobClient.new NetworkedJob(jInfo.getStatus());\n+    NetworkedJob networkJob = new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster);\n     TaskID tID = TaskID.downgrade(taskInfo.getTaskID());\n     TaskAttemptID taskAttID = new TaskAttemptID(tID, 0);\n     networkJob.killTask(taskAttID, false);\n@@ -245,7 +245,7 @@ public void testDirCleanupAfterTaskKilled()\n       filesStatus = ttClient.listStatus(localTaskDir, true);\n       if (filesStatus.length > 0) {\n         isTempFolderExists = true;\n-        NetworkedJob networkJob = jobClient.new NetworkedJob(jInfo.getStatus());\n+        NetworkedJob networkJob = new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster);\n         networkJob.killTask(taskAttID, false);\n         break;\n       }\n@@ -558,7 +558,7 @@ public void testAllTaskAttemptKill() throws Exception {\n             taskIdKilled = taskid.toString();\n             taskAttemptID = new TaskAttemptID(taskid, i);\n             LOG.info(\"taskAttemptid going to be killed is : \" + taskAttemptID);\n-            (jobClient.new NetworkedJob(jInfo.getStatus())).killTask(\n+            (new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)).killTask(\n                 taskAttemptID, true);\n             checkTaskCompletionEvent(taskAttemptID, jInfo);\n             break;\n@@ -568,7 +568,7 @@ public void testAllTaskAttemptKill() throws Exception {\n               LOG\n                   .info(\"taskAttemptid going to be killed is : \"\n                       + taskAttemptID);\n-              (jobClient.new NetworkedJob(jInfo.getStatus())).killTask(\n+              (new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)).killTask(\n                   taskAttemptID, true);\n               checkTaskCompletionEvent(taskAttemptID, jInfo);\n               break;\n@@ -611,7 +611,7 @@ public void checkTaskCompletionEvent(\n     int count = 0;\n     while (!match) {\n       TaskCompletionEvent[] taskCompletionEvents =\n-          jobClient.new NetworkedJob(jInfo.getStatus())\n+          new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)\n               .getTaskCompletionEvents(0);\n       for (TaskCompletionEvent taskCompletionEvent : taskCompletionEvents) {\n         if ((taskCompletionEvent.getTaskAttemptId().toString())",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java",
                "sha": "d84f41a547b710bcec7bebea5f1ed74266917589",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters.\nContributed by Robert Joseph Evans\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1127444 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/64b55c391e197a8a72d40da7be50fdcb9f8748b9",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobClient.java"
        ]
    },
    "hadoop-common_2d5ca82": {
        "bug_id": "hadoop-common_2d5ca82",
        "commit": "https://github.com/apache/hadoop-common/commit/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=2d5ca8201ccaeca6b6edb4a314e0bd01317f3710",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -663,6 +663,8 @@ Release 2.1.0-beta - 2013-07-02\n     mechanisms are enabled and thus fix YARN/MR test failures after HADOOP-9421.\n     (Daryn Sharp and Vinod Kumar Vavilapalli via vinodkv)\n \n+    YARN-845. RM crash with NPE on NODE_UPDATE (Mayank Bansal via bikas)\n+\n   BREAKDOWN OF HADOOP-8562 SUBTASKS AND RELATED JIRAS\n \n     YARN-158. Yarn creating package-info.java must not depend on sh.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/CHANGES.txt",
                "sha": "a0e3a9b75e4a6f22677ddb2bea8c2e4a0fc80283",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=2d5ca8201ccaeca6b6edb4a314e0bd01317f3710",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -801,9 +801,10 @@ private synchronized FiCaSchedulerApp getApplication(\n     if (reservedContainer != null) {\n       FiCaSchedulerApp application = \n           getApplication(reservedContainer.getApplicationAttemptId());\n-      return \n-          assignReservedContainer(application, node, reservedContainer, \n-              clusterResource); \n+      synchronized (application) {\n+        return assignReservedContainer(application, node, reservedContainer,\n+          clusterResource);\n+      }\n     }\n     \n     // Try to assign containers to applications in order",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "dbfa7444183198dd512254b8ff5daaa6b7208180",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java?ref=2d5ca8201ccaeca6b6edb4a314e0bd01317f3710",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.hadoop.yarn.api.records.Priority;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.ResourceRequest;\n+import org.apache.hadoop.yarn.exceptions.YarnRuntimeException;\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger;\n@@ -426,6 +427,16 @@ public synchronized void unreserve(FiCaSchedulerNode node, Priority priority) {\n       this.reservedContainers.remove(priority);\n     }\n     \n+    // reservedContainer should not be null here\n+    if (reservedContainer == null) {\n+      String errorMesssage =\n+          \"Application \" + getApplicationId() + \" is trying to unreserve \"\n+              + \" on node \" + node + \", currently has \"\n+              + reservedContainers.size() + \" at priority \" + priority\n+              + \"; currentReservation \" + currentReservation;\n+      LOG.warn(errorMesssage);\n+      throw new YarnRuntimeException(errorMesssage);\n+    }\n     // Reset the re-reservation count\n     resetReReservations(priority);\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "sha": "8e2020abc79259fa89228678556aead46ddddbc6",
                "status": "modified"
            }
        ],
        "message": "YARN-845. RM crash with NPE on NODE_UPDATE (Mayank Bansal via bikas)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1499886 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/e0b0ed18e41da6362ad88ba7c86f3aedfa47e6bd",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestLeafQueue.java"
        ]
    },
    "hadoop-common_2e96190": {
        "bug_id": "hadoop-common_2e96190",
        "commit": "https://github.com/apache/hadoop-common/commit/2e961906c6b086e43b502ee2a8d0b7f171d11bb2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2e961906c6b086e43b502ee2a8d0b7f171d11bb2/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=2e961906c6b086e43b502ee2a8d0b7f171d11bb2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -248,6 +248,9 @@ Release 0.23.6 - UNRELEASED\n     YARN-280. RM does not reject app submission with invalid tokens \n     (Daryn Sharp via tgraves)\n \n+    YARN-225. Proxy Link in RM UI thows NPE in Secure mode \n+    (Devaraj K via bobby)\n+\n Release 0.23.5 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2e961906c6b086e43b502ee2a8d0b7f171d11bb2/hadoop-yarn-project/CHANGES.txt",
                "sha": "ed4f895cfacc5776d9999665c87c150dbc80c7a8",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2e961906c6b086e43b502ee2a8d0b7f171d11bb2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java?ref=2e961906c6b086e43b502ee2a8d0b7f171d11bb2",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "patch": "@@ -254,11 +254,14 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n       \n       if(securityEnabled) {\n         String cookieName = getCheckCookieName(id); \n-        for(Cookie c: req.getCookies()) {\n-          if(cookieName.equals(c.getName())) {\n-            userWasWarned = true;\n-            userApproved = userApproved || Boolean.valueOf(c.getValue());\n-            break;\n+        Cookie[] cookies = req.getCookies();\n+        if (cookies != null) {\n+          for (Cookie c : cookies) {\n+            if (cookieName.equals(c.getName())) {\n+              userWasWarned = true;\n+              userApproved = userApproved || Boolean.valueOf(c.getValue());\n+              break;\n+            }\n           }\n         }\n       }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2e961906c6b086e43b502ee2a8d0b7f171d11bb2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "sha": "7f6bba14e98e09d7634d9d6cc712868c191d126a",
                "status": "modified"
            }
        ],
        "message": "YARN-225. Proxy Link in RM UI thows NPE in Secure mode (Devaraj K via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1426515 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/df7619819dc34d77f5e00b806611444d9b48ec0b",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestWebAppProxyServlet.java"
        ]
    },
    "hadoop-common_320308a": {
        "bug_id": "hadoop-common_320308a",
        "commit": "https://github.com/apache/hadoop-common/commit/320308a92cc4f4ad118ad3703c97b0cfc34a4539",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/320308a92cc4f4ad118ad3703c97b0cfc34a4539/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=320308a92cc4f4ad118ad3703c97b0cfc34a4539",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -207,6 +207,9 @@ Trunk (unreleased changes)\n     HADOOP-6549. TestDoAsEffectiveUser should use ip address of the host\n      for superuser ip check(jnp via boryas)\n \n+    HADOOP-6570. RPC#stopProxy throws NPE if getProxyEngine(proxy) returns\n+    null. (hairong)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/320308a92cc4f4ad118ad3703c97b0cfc34a4539/CHANGES.txt",
                "sha": "7354c4cec03c5288df758a25a5ba3d2d7bfb07fd",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/320308a92cc4f4ad118ad3703c97b0cfc34a4539/src/java/org/apache/hadoop/ipc/RPC.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/ipc/RPC.java?ref=320308a92cc4f4ad118ad3703c97b0cfc34a4539",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/ipc/RPC.java",
                "patch": "@@ -244,8 +244,9 @@ public static Object getProxy(Class protocol, long clientVersion,\n    * @param proxy the proxy to be stopped\n    */\n   public static void stopProxy(Object proxy) {\n-    if (proxy!=null) {\n-      getProxyEngine(proxy).stopProxy(proxy);\n+    RpcEngine rpcEngine;\n+    if (proxy!=null && (rpcEngine = getProxyEngine(proxy)) != null) {\n+      rpcEngine.stopProxy(proxy);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/320308a92cc4f4ad118ad3703c97b0cfc34a4539/src/java/org/apache/hadoop/ipc/RPC.java",
                "sha": "36874c511dcca84e6dfa25fd0967d8e5af2a6cab",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/320308a92cc4f4ad118ad3703c97b0cfc34a4539/src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/core/org/apache/hadoop/ipc/TestRPC.java?ref=320308a92cc4f4ad118ad3703c97b0cfc34a4539",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "patch": "@@ -39,6 +39,8 @@\n import org.apache.hadoop.security.authorize.Service;\n import org.apache.hadoop.security.authorize.ServiceAuthorizationManager;\n \n+import static org.mockito.Mockito.*;\n+\n /** Unit tests for RPC. */\n public class TestRPC extends TestCase {\n   private static final String ADDRESS = \"0.0.0.0\";\n@@ -392,6 +394,14 @@ public void testNoPings() throws Exception {\n     conf.setBoolean(\"ipc.client.ping\", false);\n     new TestRPC(\"testnoPings\").testCalls(conf);\n   }\n+\n+  /**\n+   * Test stopping a non-registered proxy\n+   * @throws Exception\n+   */\n+  public void testStopNonRegisteredProxy() throws Exception {\n+    RPC.stopProxy(mock(TestProtocol.class));\n+  }\n   \n   public static void main(String[] args) throws Exception {\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/320308a92cc4f4ad118ad3703c97b0cfc34a4539/src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "sha": "0bb3f8dc5c0b3c0820081e6e849e2e1f18e2add8",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6570. RPC#stopProxy throws NPE if getProxyEngine(proxy) returns null. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@911134 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/178095ad514d5b41bb0215b7d7d6ef8b852049b9",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRPC.java"
        ]
    },
    "hadoop-common_36cabb6": {
        "bug_id": "hadoop-common_36cabb6",
        "commit": "https://github.com/apache/hadoop-common/commit/36cabb654e315d62182b47c6bed6ce804a535e3e",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -509,6 +509,10 @@ Release 0.23.0 - Unreleased\n     HADOOP-7360. Preserve relative paths that do not contain globs in FsShell.\n     (Daryn Sharp and Kihwal Lee via szetszwo)\n \n+    HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the\n+    destination directory does not exist.  (John George and Daryn Sharp\n+    via szetszwo)\n+\n   OPTIMIZATIONS\n   \n     HADOOP-7333. Performance improvement in PureJavaCrc32. (Eric Caspole",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "0b0e1beb76248ba3b27554efd8cb2bd9b239c27d",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "patch": "@@ -55,6 +55,7 @@\n   protected int exitCode = 0;\n   protected int numErrors = 0;\n   protected boolean recursive = false;\n+  private int depth = 0;\n   protected ArrayList<Exception> exceptions = new ArrayList<Exception>();\n \n   private static final Log LOG = LogFactory.getLog(Command.class);\n@@ -86,6 +87,10 @@ protected boolean isRecursive() {\n     return recursive;\n   }\n \n+  protected int getDepth() {\n+    return depth;\n+  }\n+  \n   /** \n    * Execute the command on the input path\n    * \n@@ -269,6 +274,7 @@ protected void processArgument(PathData item) throws IOException {\n   protected void processPathArgument(PathData item) throws IOException {\n     // null indicates that the call is not via recursion, ie. there is\n     // no parent directory that was expanded\n+    depth = 0;\n     processPaths(null, item);\n   }\n   \n@@ -326,7 +332,12 @@ protected void processPath(PathData item) throws IOException {\n    *  @throws IOException if anything goes wrong...\n    */\n   protected void recursePath(PathData item) throws IOException {\n-    processPaths(item, item.getDirectoryContents());\n+    try {\n+      depth++;\n+      processPaths(item, item.getDirectoryContents());\n+    } finally {\n+      depth--;\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "sha": "b24d47e02b1027b5decd17b36d9ab8a71b589875",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "changes": 142,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 26,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "patch": "@@ -20,13 +20,18 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.io.InputStream;\n import java.util.LinkedList;\n \n+import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathIsDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIsNotDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathNotFoundException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n+import org.apache.hadoop.io.IOUtils;\n \n /**\n  * Provides: argument processing to ensure the destination is valid\n@@ -106,51 +111,136 @@ protected void processArguments(LinkedList<PathData> args)\n   }\n \n   @Override\n-  protected void processPaths(PathData parent, PathData ... items)\n+  protected void processPathArgument(PathData src)\n   throws IOException {\n+    if (src.stat.isDirectory() && src.fs.equals(dst.fs)) {\n+      PathData target = getTargetPath(src);\n+      String srcPath = src.fs.makeQualified(src.path).toString();\n+      String dstPath = dst.fs.makeQualified(target.path).toString();\n+      if (dstPath.equals(srcPath)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"are identical\");\n+        e.setTargetPath(dstPath.toString());\n+        throw e;\n+      }\n+      if (dstPath.startsWith(srcPath+Path.SEPARATOR)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"is a subdirectory of itself\");\n+        e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+    }\n+    super.processPathArgument(src);\n+  }\n+\n+  @Override\n+  protected void processPath(PathData src) throws IOException {\n+    processPath(src, getTargetPath(src));\n+  }\n+  \n+  /**\n+   * Called with a source and target destination pair\n+   * @param src for the operation\n+   * @param target for the operation\n+   * @throws IOException if anything goes wrong\n+   */\n+  protected void processPath(PathData src, PathData dst) throws IOException {\n+    if (src.stat.isSymlink()) {\n+      // TODO: remove when FileContext is supported, this needs to either\n+      // copy the symlink or deref the symlink\n+      throw new PathOperationException(src.toString());        \n+    } else if (src.stat.isFile()) {\n+      copyFileToTarget(src, dst);\n+    } else if (src.stat.isDirectory() && !isRecursive()) {\n+      throw new PathIsDirectoryException(src.toString());\n+    }\n+  }\n+\n+  @Override\n+  protected void recursePath(PathData src) throws IOException {\n     PathData savedDst = dst;\n     try {\n       // modify dst as we descend to append the basename of the\n       // current directory being processed\n-      if (parent != null) dst = dst.getPathDataForChild(parent);\n-      super.processPaths(parent, items);\n+      dst = getTargetPath(src);\n+      if (dst.exists) {\n+        if (!dst.stat.isDirectory()) {\n+          throw new PathIsNotDirectoryException(dst.toString());\n+        }\n+      } else {\n+        if (!dst.fs.mkdirs(dst.path)) {\n+          // too bad we have no clue what failed\n+          PathIOException e = new PathIOException(dst.toString());\n+          e.setOperation(\"mkdir\");\n+          throw e;\n+        }    \n+        dst.refreshStatus(); // need to update stat to know it exists now\n+      }      \n+      super.recursePath(src);\n     } finally {\n       dst = savedDst;\n     }\n   }\n   \n-  @Override\n-  protected void processPath(PathData src) throws IOException {\n+  protected PathData getTargetPath(PathData src) throws IOException {\n     PathData target;\n-    // if the destination is a directory, make target a child path,\n-    // else use the destination as-is\n-    if (dst.exists && dst.stat.isDirectory()) {\n+    // on the first loop, the dst may be directory or a file, so only create\n+    // a child path if dst is a dir; after recursion, it's always a dir\n+    if ((getDepth() > 0) || (dst.exists && dst.stat.isDirectory())) {\n       target = dst.getPathDataForChild(src);\n     } else {\n       target = dst;\n     }\n-    if (target.exists && !overwrite) {\n+    return target;\n+  }\n+  \n+  /**\n+   * Copies the source file to the target.\n+   * @param src item to copy\n+   * @param target where to copy the item\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyFileToTarget(PathData src, PathData target) throws IOException {\n+    copyStreamToTarget(src.fs.open(src.path), target);\n+  }\n+  \n+  /**\n+   * Copies the stream contents to a temporary file.  If the copy is\n+   * successful, the temporary file will be renamed to the real path,\n+   * else the temporary file will be deleted.\n+   * @param in the input stream for the copy\n+   * @param target where to store the contents of the stream\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyStreamToTarget(InputStream in, PathData target)\n+  throws IOException {\n+    if (target.exists && (target.stat.isDirectory() || !overwrite)) {\n       throw new PathExistsException(target.toString());\n     }\n-\n-    try { \n-      // invoke processPath with both a source and resolved target\n-      processPath(src, target);\n-    } catch (PathIOException e) {\n-      // add the target unless it already has one\n-      if (e.getTargetPath() == null) {\n+    PathData tempFile = null;\n+    try {\n+      tempFile = target.createTempFile(target+\"._COPYING_\");\n+      FSDataOutputStream out = target.fs.create(tempFile.path, true);\n+      IOUtils.copyBytes(in, out, getConf(), true);\n+      // the rename method with an option to delete the target is deprecated\n+      if (target.exists && !target.fs.delete(target.path, false)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(target.toString());\n+        e.setOperation(\"delete\");\n+        throw e;\n+      }\n+      if (!tempFile.fs.rename(tempFile.path, target.path)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(tempFile.toString());\n+        e.setOperation(\"rename\");\n         e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+      tempFile = null;\n+    } finally {\n+      if (tempFile != null) {\n+        tempFile.fs.delete(tempFile.path, false);\n       }\n-      throw e;\n     }\n   }\n-\n-  /**\n-   * Called with a source and target destination pair\n-   * @param src for the operation\n-   * @param target for the operation\n-   * @throws IOException if anything goes wrong\n-   */\n-  protected abstract void processPath(PathData src, PathData target)\n-  throws IOException;\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "sha": "6b3b40389f9f72528cdc248ee23af4af559df101",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 73,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "patch": "@@ -26,13 +26,7 @@\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.fs.ChecksumFileSystem;\n-import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileUtil;\n-import org.apache.hadoop.fs.LocalFileSystem;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n-import org.apache.hadoop.io.IOUtils;\n \n /** Various commands for copy files */\n @InterfaceAudience.Private\n@@ -95,18 +89,10 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       CommandFormat cf = new CommandFormat(2, Integer.MAX_VALUE, \"f\");\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n+      // should have a -r option\n+      setRecursive(true);\n       getRemoteDestination(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      if (!FileUtil.copy(src.fs, src.path, target.fs, target.path, false, overwrite, getConf())) {\n-        // we have no idea what the error is...  FileUtils masks it and in\n-        // some cases won't even report an error\n-        throw new PathIOException(src.toString());\n-      }\n-    }\n   }\n   \n   /** \n@@ -126,7 +112,6 @@ protected void processPath(PathData src, PathData target)\n      * It must be at least three characters long, required by\n      * {@link java.io.File#createTempFile(String, String, File)}.\n      */\n-    private static final String COPYTOLOCAL_PREFIX = \"_copyToLocal_\";\n     private boolean copyCrc;\n     private boolean verifyChecksum;\n \n@@ -144,7 +129,7 @@ protected void processOptions(LinkedList<String> args)\n     }\n \n     @Override\n-    protected void processPath(PathData src, PathData target)\n+    protected void copyFileToTarget(PathData src, PathData target)\n     throws IOException {\n       src.fs.setVerifyChecksum(verifyChecksum);\n \n@@ -153,41 +138,10 @@ protected void processPath(PathData src, PathData target)\n         copyCrc = false;\n       }      \n \n-      if (src.stat.isFile()) {\n-        // copy the file and maybe its crc\n-        copyFileToLocal(src, target);\n-        if (copyCrc) {\n-          copyFileToLocal(src.getChecksumFile(), target.getChecksumFile());\n-        }\n-      } else if (src.stat.isDirectory()) {\n-        // create the remote directory structure locally\n-        if (!target.toFile().mkdirs()) {\n-          throw new PathIOException(target.toString());\n-        }\n-      } else {\n-        throw new PathOperationException(src.toString());\n-      }\n-    }\n-\n-    private void copyFileToLocal(PathData src, PathData target)\n-    throws IOException {\n-      File targetFile = target.toFile();\n-      File tmpFile = FileUtil.createLocalTempFile(\n-          targetFile, COPYTOLOCAL_PREFIX, true);\n-      // too bad we can't tell exactly why it failed...\n-      if (!FileUtil.copy(src.fs, src.path, tmpFile, false, getConf())) {\n-        PathIOException e = new PathIOException(src.toString());\n-        e.setOperation(\"copy\");\n-        e.setTargetPath(tmpFile.toString());\n-        throw e;\n-      }\n-\n-      // too bad we can't tell exactly why it failed...\n-      if (!tmpFile.renameTo(targetFile)) {\n-        PathIOException e = new PathIOException(tmpFile.toString());\n-        e.setOperation(\"rename\");\n-        e.setTargetPath(targetFile.toString());\n-        throw e;\n+      super.copyFileToTarget(src, target);\n+      if (copyCrc) {\n+        // should we delete real file if crc copy fails?\n+        super.copyFileToTarget(src.getChecksumFile(), target.getChecksumFile());\n       }\n     }\n   }\n@@ -208,6 +162,8 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n       getRemoteDestination(args);\n+      // should have a -r option\n+      setRecursive(true);\n     }\n \n     // commands operating on local paths have no need for glob expansion\n@@ -223,30 +179,11 @@ protected void processArguments(LinkedList<PathData> args)\n     throws IOException {\n       // NOTE: this logic should be better, mimics previous implementation\n       if (args.size() == 1 && args.get(0).toString().equals(\"-\")) {\n-        if (dst.exists && !overwrite) {\n-          throw new PathExistsException(dst.toString());\n-        }\n-        copyFromStdin();\n+        copyStreamToTarget(System.in, getTargetPath(args.get(0)));\n         return;\n       }\n       super.processArguments(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      target.fs.copyFromLocalFile(false, overwrite, src.path, target.path);\n-    }\n-\n-    /** Copies from stdin to the destination file. */\n-    protected void copyFromStdin() throws IOException {\n-      FSDataOutputStream out = dst.fs.create(dst.path); \n-      try {\n-        IOUtils.copyBytes(System.in, out, getConf(), false);\n-      } finally {\n-        out.close();\n-      }\n-    }\n   }\n \n   public static class CopyFromLocal extends Put {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "sha": "066e5fdb899df7f517fb508451caf3a86f7caf68",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "patch": "@@ -182,6 +182,19 @@ public PathData getChecksumFile() throws IOException {\n     return new PathData(srcFs.getRawFileSystem(), srcPath.toString());\n   }\n \n+  /**\n+   * Returns a temporary file for this PathData with the given extension.\n+   * The file will be deleted on exit.\n+   * @param extension for the temporary file\n+   * @return PathData\n+   * @throws IOException shouldn't happen\n+   */\n+  public PathData createTempFile(String extension) throws IOException {\n+    PathData tmpFile = new PathData(fs, uri+\"._COPYING_\");\n+    fs.deleteOnExit(tmpFile.path);\n+    return tmpFile;\n+  }\n+\n   /**\n    * Returns a list of PathData objects of the items contained in the given\n    * directory.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "sha": "a3c88f1f2af2736442e7db320919298356c681b4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the destination directory does not exist.  Contributed by John George and Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195760 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f3215940dc0fbbf0c75e744ede43605a42580d5a",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestPathData.java"
        ]
    },
    "hadoop-common_400ef68": {
        "bug_id": "hadoop-common_400ef68",
        "commit": "https://github.com/apache/hadoop-common/commit/400ef68cbb5eae73ca0d80aecd0889256048330e",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=400ef68cbb5eae73ca0d80aecd0889256048330e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -234,6 +234,10 @@ Trunk (Unreleased)\n     HADOOP-8815. RandomDatum needs to override hashCode().\n     (Brandon Li via suresh)\n \n+    HADOOP-8436. NPE In getLocalPathForWrite ( path, conf ) when the\n+    required context item is not configured\n+    (Brahma Reddy Battula via harsh)\n+\n   OPTIMIZATIONS\n \n     HADOOP-7761. Improve the performance of raw comparisons. (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "5b4066b80f22163dc57d54001f70fd088847f806",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java?ref=400ef68cbb5eae73ca0d80aecd0889256048330e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "patch": "@@ -265,6 +265,9 @@ public AllocatorPerContext(String contextCfgItemName) {\n     private synchronized void confChanged(Configuration conf) \n         throws IOException {\n       String newLocalDirs = conf.get(contextCfgItemName);\n+      if (null == newLocalDirs) {\n+        throw new IOException(contextCfgItemName + \" not configured\");\n+      }\n       if (!newLocalDirs.equals(savedLocalDirs)) {\n         localDirs = StringUtils.getTrimmedStrings(newLocalDirs);\n         localFS = FileSystem.getLocal(conf);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "sha": "16a1c99b5c0ca566a6bce500f9ecdc44063a5bff",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop-common/blob/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java?ref=400ef68cbb5eae73ca0d80aecd0889256048330e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "patch": "@@ -293,6 +293,23 @@ public void testLocalPathForWriteDirCreation() throws IOException {\n     }\n   }\n \n+  /*\n+   * Test when mapred.local.dir not configured and called\n+   * getLocalPathForWrite\n+   */\n+  @Test\n+  public void testShouldNotthrowNPE() throws Exception {\n+    Configuration conf1 = new Configuration();\n+    try {\n+      dirAllocator.getLocalPathForWrite(\"/test\", conf1);\n+      fail(\"Exception not thrown when \" + CONTEXT + \" is not set\");\n+    } catch (IOException e) {\n+      assertEquals(CONTEXT + \" not configured\", e.getMessage());\n+    } catch (NullPointerException e) {\n+      fail(\"Lack of configuration should not have thrown an NPE.\");\n+    }\n+  }\n+\n   /** Test no side effect files are left over. After creating a temp\n    * temp file, remove both the temp file and its parent. Verify that\n    * no files or directories are left over as can happen when File objects",
                "raw_url": "https://github.com/apache/hadoop-common/raw/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "sha": "7a4618e650b26f9419ce26022863a3a8c45027ae",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8436. NPE In getLocalPathForWrite ( path, conf ) when the required context item is not configured. Contributed by Brahma Reddy Battula. (harsh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1389799 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d90e5090ba0df64546f2aaa7ddd9907fc337280b",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestLocalDirAllocator.java"
        ]
    },
    "hadoop-common_463f416": {
        "bug_id": "hadoop-common_463f416",
        "commit": "https://github.com/apache/hadoop-common/commit/463f416a07473540f866e1e26365a77e6aa18490",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=463f416a07473540f866e1e26365a77e6aa18490",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -744,6 +744,8 @@ Release 2.4.0 - UNRELEASED\n     HDFS-5756. hadoopRzOptionsSetByteBufferPool does not accept NULL argument,\n     contrary to docs. (cmccabe via wang)\n \n+    HDFS-5747. Fix NPEs in BlockManager. (Arpit Agarwal)\n+\n   BREAKDOWN OF HDFS-2832 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4985. Add storage type to the protocol and expose it in block report",
                "raw_url": "https://github.com/apache/hadoop-common/raw/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "af95486f1e8220955073a84f69feeca9fced908d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java?ref=463f416a07473540f866e1e26365a77e6aa18490",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java",
                "patch": "@@ -324,12 +324,14 @@ void addReplicaIfNotPresent(DatanodeStorageInfo storage,\n     Iterator<ReplicaUnderConstruction> it = replicas.iterator();\n     while (it.hasNext()) {\n       ReplicaUnderConstruction r = it.next();\n-      if(r.getExpectedStorageLocation() == storage) {\n+      DatanodeStorageInfo expectedLocation = r.getExpectedStorageLocation();\n+      if(expectedLocation == storage) {\n         // Record the gen stamp from the report\n         r.setGenerationStamp(block.getGenerationStamp());\n         return;\n-      } else if (r.getExpectedStorageLocation().getDatanodeDescriptor() ==\n-          storage.getDatanodeDescriptor()) {\n+      } else if (expectedLocation != null &&\n+                 expectedLocation.getDatanodeDescriptor() ==\n+                     storage.getDatanodeDescriptor()) {\n \n         // The Datanode reported that the block is on a different storage\n         // than the one chosen by BlockPlacementPolicy. This can occur as",
                "raw_url": "https://github.com/apache/hadoop-common/raw/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java",
                "sha": "1161077f49ddffb2f3294549d3f9322cf319f6fb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=463f416a07473540f866e1e26365a77e6aa18490",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -547,8 +547,8 @@ private void startCommonServices(Configuration conf) throws IOException {\n   }\n   \n   private void stopCommonServices() {\n-    if(namesystem != null) namesystem.close();\n     if(rpcServer != null) rpcServer.stop();\n+    if(namesystem != null) namesystem.close();\n     if (pauseMonitor != null) pauseMonitor.stop();\n     if (plugins != null) {\n       for (ServicePlugin p : plugins) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "eb3755bdc4fda9bebbbf35ea7da30826fec2f3fa",
                "status": "modified"
            }
        ],
        "message": "HDFS-5747. Fix NPEs in BlockManager. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1557289 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/42f3f2c5a95f0cb71f96bfbe83ba5915740eb9fd",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBlockInfoUnderConstruction.java"
        ]
    },
    "hadoop-common_4b28c4e": {
        "bug_id": "hadoop-common_4b28c4e",
        "commit": "https://github.com/apache/hadoop-common/commit/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -214,6 +214,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-2518. The t flag is missing in distcp help message.  (Wei Yongjun\n     via szetszwo)\n \n+    MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters. (Robert Joseph Evans\n+    via cdouglas)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/CHANGES.txt",
                "sha": "e1eb16d5a672f5955bcf791150d2caaf76949320",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "deletions": 6,
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -42,8 +42,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.ipc.RemoteException;\n-import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.util.Tool;\n@@ -149,7 +147,7 @@\n    * a JobProfile object to provide some info, and interacts with the\n    * remote service to provide certain functionality.\n    */\n-  class NetworkedJob implements RunningJob {\n+  static class NetworkedJob implements RunningJob {\n     Job job;\n     /**\n      * We store a JobProfile and a timestamp for when we last\n@@ -158,7 +156,7 @@\n      * has completely forgotten about the job.  (eg, 24 hours after the\n      * job completes.)\n      */\n-    public NetworkedJob(JobStatus status) throws IOException {\n+    public NetworkedJob(JobStatus status, Cluster cluster) throws IOException {\n       job = Job.getInstance(cluster, status, new JobConf(status.getJobFile()));\n     }\n \n@@ -380,7 +378,12 @@ public String toString() {\n      */\n     public Counters getCounters() throws IOException {\n       try { \n-        return Counters.downgrade(job.getCounters());\n+        Counters result = null;\n+        org.apache.hadoop.mapreduce.Counters temp = job.getCounters();\n+        if(temp != null) {\n+          result = Counters.downgrade(temp);\n+        }\n+        return result;\n       } catch (InterruptedException ie) {\n         throw new IOException(ie);\n       }\n@@ -557,7 +560,7 @@ public RunningJob getJob(JobID jobid) throws IOException {\n       if (job != null) {\n         JobStatus status = JobStatus.downgrade(job.getStatus());\n         if (status != null) {\n-          return new NetworkedJob(status);\n+          return new NetworkedJob(status, cluster);\n         } \n       }\n     } catch (InterruptedException ie) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "3b5f84bfe1fb6d4c0920e78cfbc20d3de32c28e1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/RunningJob.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/mapred/RunningJob.java",
                "patch": "@@ -193,7 +193,7 @@\n   /**\n    * Gets the counters for this job.\n    * \n-   * @return the counters for this job.\n+   * @return the counters for this job or null if the job has been retired.\n    * @throws IOException\n    */\n   public Counters getCounters() throws IOException;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "sha": "b3af4f6c98d5ebe15971387e23173e2e4a0513a8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "patch": "@@ -230,7 +230,7 @@ public Counters getJobCounters(JobID jobid)\n   \n   /**\n    * Get task completion events for the jobid, starting from fromEventId. \n-   * Returns empty aray if no events are available. \n+   * Returns empty array if no events are available. \n    * @param jobid job id \n    * @param fromEventId event id to start from.\n    * @param maxEvents the max number of events we want to look at ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "sha": "80e556bac2f3f4929cd313ea3e21c4a4839719c9",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.mapred;\n+\n+import static org.junit.Assert.*;\n+\n+import java.util.List;\n+\n+import org.apache.hadoop.mapreduce.Job;\n+import org.junit.Test;\n+import static org.mockito.Mockito.*;\n+\n+\n+public class TestNetworkedJob {\n+\n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testGetNullCounters() throws Exception {\n+    //mock creation\n+    Job mockJob = mock(Job.class);\n+    RunningJob underTest = new JobClient.NetworkedJob(mockJob); \n+\n+    when(mockJob.getCounters()).thenReturn(null);\n+    assertNull(underTest.getCounters());\n+    //verification\n+    verify(mockJob).getCounters();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "sha": "b6565e28956d7446628aae1da838b930a83313db",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters.\nContributed by Robert Joseph Evans\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1125578 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/921c7920150466cb545dca153e05f0edeacd3754",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobClient.java"
        ]
    },
    "hadoop-common_4bcaa45": {
        "bug_id": "hadoop-common_4bcaa45",
        "commit": "https://github.com/apache/hadoop-common/commit/4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -326,6 +326,8 @@ Trunk (Unreleased)\n \n     HADOOP-10431. Change visibility of KeyStore.Options getter methods to public. (tucu)\n \n+    HADOOP-10583. bin/hadoop key throws NPE with no args and assorted other fixups. (clamb via tucu)\n+\n   OPTIMIZATIONS\n \n     HADOOP-7761. Improve the performance of raw comparisons. (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "36fe52b7b5d307bfa4020b5af7d58dc7c841997c",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java?ref=4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
                "deletions": 33,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "patch": "@@ -27,9 +27,7 @@\n import java.security.NoSuchAlgorithmException;\n import java.text.MessageFormat;\n import java.util.Date;\n-import java.util.LinkedHashMap;\n import java.util.List;\n-import java.util.Map;\n \n import com.google.gson.stream.JsonReader;\n import com.google.gson.stream.JsonWriter;\n@@ -176,22 +174,26 @@ protected int addVersion() {\n     protected byte[] serialize() throws IOException {\n       ByteArrayOutputStream buffer = new ByteArrayOutputStream();\n       JsonWriter writer = new JsonWriter(new OutputStreamWriter(buffer));\n-      writer.beginObject();\n-      if (cipher != null) {\n-        writer.name(CIPHER_FIELD).value(cipher);\n-      }\n-      if (bitLength != 0) {\n-        writer.name(BIT_LENGTH_FIELD).value(bitLength);\n-      }\n-      if (created != null) {\n-        writer.name(CREATED_FIELD).value(created.getTime());\n-      }\n-      if (description != null) {\n-        writer.name(DESCRIPTION_FIELD).value(description);\n+      try {\n+        writer.beginObject();\n+        if (cipher != null) {\n+          writer.name(CIPHER_FIELD).value(cipher);\n+        }\n+        if (bitLength != 0) {\n+          writer.name(BIT_LENGTH_FIELD).value(bitLength);\n+        }\n+        if (created != null) {\n+          writer.name(CREATED_FIELD).value(created.getTime());\n+        }\n+        if (description != null) {\n+          writer.name(DESCRIPTION_FIELD).value(description);\n+        }\n+        writer.name(VERSIONS_FIELD).value(versions);\n+        writer.endObject();\n+        writer.flush();\n+      } finally {\n+        writer.close();\n       }\n-      writer.name(VERSIONS_FIELD).value(versions);\n-      writer.endObject();\n-      writer.flush();\n       return buffer.toByteArray();\n     }\n \n@@ -207,23 +209,27 @@ protected Metadata(byte[] bytes) throws IOException {\n       int versions = 0;\n       String description = null;\n       JsonReader reader = new JsonReader(new InputStreamReader\n-          (new ByteArrayInputStream(bytes)));\n-      reader.beginObject();\n-      while (reader.hasNext()) {\n-        String field = reader.nextName();\n-        if (CIPHER_FIELD.equals(field)) {\n-          cipher = reader.nextString();\n-        } else if (BIT_LENGTH_FIELD.equals(field)) {\n-          bitLength = reader.nextInt();\n-        } else if (CREATED_FIELD.equals(field)) {\n-          created = new Date(reader.nextLong());\n-        } else if (VERSIONS_FIELD.equals(field)) {\n-          versions = reader.nextInt();\n-        } else if (DESCRIPTION_FIELD.equals(field)) {\n-          description = reader.nextString();\n+        (new ByteArrayInputStream(bytes)));\n+      try {\n+        reader.beginObject();\n+        while (reader.hasNext()) {\n+          String field = reader.nextName();\n+          if (CIPHER_FIELD.equals(field)) {\n+            cipher = reader.nextString();\n+          } else if (BIT_LENGTH_FIELD.equals(field)) {\n+            bitLength = reader.nextInt();\n+          } else if (CREATED_FIELD.equals(field)) {\n+            created = new Date(reader.nextLong());\n+          } else if (VERSIONS_FIELD.equals(field)) {\n+            versions = reader.nextInt();\n+          } else if (DESCRIPTION_FIELD.equals(field)) {\n+            description = reader.nextString();\n+          }\n         }\n+        reader.endObject();\n+      } finally {\n+        reader.close();\n       }\n-      reader.endObject();\n       this.cipher = cipher;\n       this.bitLength = bitLength;\n       this.created = created;\n@@ -310,7 +316,6 @@ public abstract KeyVersion getKeyVersion(String versionName\n    */\n   public abstract List<String> getKeys() throws IOException;\n \n-\n   /**\n    * Get key metadata in bulk.\n    * @param names the names of the keys to get",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "sha": "0b031c0493b8f6cc2961f2ecd54b3d74ddc54ef8",
                "status": "modified"
            },
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "changes": 168,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java?ref=4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
                "deletions": 80,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "patch": "@@ -23,9 +23,6 @@\n import java.security.InvalidParameterException;\n import java.security.NoSuchAlgorithmException;\n import java.util.List;\n-import java.util.Map;\n-\n-import javax.crypto.KeyGenerator;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n@@ -93,41 +90,54 @@ public int run(String[] args) throws Exception {\n    */\n   private int init(String[] args) throws IOException {\n     for (int i = 0; i < args.length; i++) { // parse command line\n+      boolean moreTokens = (i < args.length - 1);\n       if (args[i].equals(\"create\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new CreateCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new DeleteCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new RollCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n-      } else if (args[i].equals(\"list\")) {\n+      } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n-      } else if (args[i].equals(\"--size\")) {\n+      } else if (\"--size\".equals(args[i]) && moreTokens) {\n         getConf().set(KeyProvider.DEFAULT_BITLENGTH_NAME, args[++i]);\n-      } else if (args[i].equals(\"--cipher\")) {\n+      } else if (\"--cipher\".equals(args[i]) && moreTokens) {\n         getConf().set(KeyProvider.DEFAULT_CIPHER_NAME, args[++i]);\n-      } else if (args[i].equals(\"--provider\")) {\n+      } else if (\"--provider\".equals(args[i]) && moreTokens) {\n         userSuppliedProvider = true;\n         getConf().set(KeyProviderFactory.KEY_PROVIDER_PATH, args[++i]);\n-      } else if (args[i].equals(\"--metadata\")) {\n+      } else if (\"--metadata\".equals(args[i])) {\n         getConf().setBoolean(LIST_METADATA, true);\n-      } else if (args[i].equals(\"-i\") || (args[i].equals(\"--interactive\"))) {\n+      } else if (\"-i\".equals(args[i]) || (\"--interactive\".equals(args[i]))) {\n         interactive = true;\n-      } else if (args[i].equals(\"--help\")) {\n+      } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n         return -1;\n       } else {\n@@ -136,15 +146,20 @@ private int init(String[] args) throws IOException {\n         return -1;\n       }\n     }\n+\n+    if (command == null) {\n+      printKeyShellUsage();\n+      return -1;\n+    }\n+\n     return 0;\n   }\n \n   private void printKeyShellUsage() {\n     out.println(USAGE_PREFIX + COMMANDS);\n     if (command != null) {\n       out.println(command.getUsage());\n-    }\n-    else {\n+    } else {\n       out.println(\"=========================================================\" +\n       \t\t\"======\");\n       out.println(CreateCommand.USAGE + \":\\n\\n\" + CreateCommand.DESC);\n@@ -174,8 +189,7 @@ protected KeyProvider getKeyProvider() {\n         providers = KeyProviderFactory.getProviders(getConf());\n         if (userSuppliedProvider) {\n           provider = providers.get(0);\n-        }\n-        else {\n+        } else {\n           for (KeyProvider p : providers) {\n             if (!p.isTransient()) {\n               provider = p;\n@@ -190,7 +204,7 @@ protected KeyProvider getKeyProvider() {\n     }\n \n     protected void printProviderWritten() {\n-        out.println(provider.getClass().getName() + \" has been updated.\");\n+        out.println(provider + \" has been updated.\");\n     }\n \n     protected void warnIfTransientProvider() {\n@@ -206,12 +220,12 @@ protected void warnIfTransientProvider() {\n \n   private class ListCommand extends Command {\n     public static final String USAGE =\n-        \"list [--provider] [--metadata] [--help]\";\n+        \"list [--provider <provider>] [--metadata] [--help]\";\n     public static final String DESC =\n-        \"The list subcommand displays the keynames contained within \\n\" +\n-        \"a particular provider - as configured in core-site.xml or \" +\n-        \"indicated\\nthrough the --provider argument.\\n\" +\n-        \"If the --metadata option is used, the keys metadata will be printed\";\n+        \"The list subcommand displays the keynames contained within\\n\" +\n+        \"a particular provider as configured in core-site.xml or\\n\" +\n+        \"specified with the --provider argument. --metadata displays\\n\" +\n+        \"the metadata.\";\n \n     private boolean metadata = false;\n \n@@ -220,9 +234,9 @@ public boolean validate() {\n       provider = getKeyProvider();\n       if (provider == null) {\n         out.println(\"There are no non-transient KeyProviders configured.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\\n\"\n-            + \"to use. If you want to list a transient provider then you\\n\"\n-            + \"you MUST use the --provider argument.\");\n+          + \"Use the --provider option to specify a provider. If you\\n\"\n+          + \"want to list a transient provider then you must use the\\n\"\n+          + \"--provider argument.\");\n         rc = false;\n       }\n       metadata = getConf().getBoolean(LIST_METADATA, false);\n@@ -231,12 +245,12 @@ public boolean validate() {\n \n     public void execute() throws IOException {\n       try {\n-        List<String> keys = provider.getKeys();\n-        out.println(\"Listing keys for KeyProvider: \" + provider.toString());\n+        final List<String> keys = provider.getKeys();\n+        out.println(\"Listing keys for KeyProvider: \" + provider);\n         if (metadata) {\n-          Metadata[] meta =\n+          final Metadata[] meta =\n             provider.getKeysMetadata(keys.toArray(new String[keys.size()]));\n-          for(int i=0; i < meta.length; ++i) {\n+          for (int i = 0; i < meta.length; ++i) {\n             out.println(keys.get(i) + \" : \" + meta[i]);\n           }\n         } else {\n@@ -245,7 +259,7 @@ public void execute() throws IOException {\n           }\n         }\n       } catch (IOException e) {\n-        out.println(\"Cannot list keys for KeyProvider: \" + provider.toString()\n+        out.println(\"Cannot list keys for KeyProvider: \" + provider\n             + \": \" + e.getMessage());\n         throw e;\n       }\n@@ -258,11 +272,10 @@ public String getUsage() {\n   }\n \n   private class RollCommand extends Command {\n-    public static final String USAGE = \"roll <keyname> [--provider] [--help]\";\n+    public static final String USAGE = \"roll <keyname> [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The roll subcommand creates a new version of the key specified\\n\" +\n-        \"through the <keyname> argument within the provider indicated using\\n\" +\n-        \"the --provider argument\";\n+      \"The roll subcommand creates a new version for the specified key\\n\" +\n+      \"within the provider indicated using the --provider argument\\n\";\n \n     String keyName = null;\n \n@@ -274,39 +287,37 @@ public boolean validate() {\n       boolean rc = true;\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\n\"\n-            + \"Key will not be rolled.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\"\n-            + \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. The key\\n\" +\n+          \"has not been rolled. Use the --provider option to specify\\n\" +\n+          \"a provider.\");\n         rc = false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-            \"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"Please provide a <keyname>.\\n\" +\n+          \"See the usage description by using --help.\");\n         rc = false;\n       }\n       return rc;\n     }\n \n     public void execute() throws NoSuchAlgorithmException, IOException {\n       try {\n-        Metadata md = provider.getMetadata(keyName);\n         warnIfTransientProvider();\n         out.println(\"Rolling key version from KeyProvider: \"\n-            + provider.toString() + \" for key name: \" + keyName);\n+            + provider + \"\\n  for key name: \" + keyName);\n         try {\n           provider.rollNewVersion(keyName);\n           out.println(keyName + \" has been successfully rolled.\");\n           provider.flush();\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider.toString());\n+              + provider);\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider.toString());\n+            + provider);\n         throw e1;\n       }\n     }\n@@ -318,11 +329,11 @@ public String getUsage() {\n   }\n \n   private class DeleteCommand extends Command {\n-    public static final String USAGE = \"delete <keyname> [--provider] [--help]\";\n+    public static final String USAGE = \"delete <keyname> [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The delete subcommand deletes all of the versions of the key\\n\" +\n-        \"specified as the <keyname> argument from within the provider\\n\" +\n-        \"indicated through the --provider argument\";\n+        \"The delete subcommand deletes all versions of the key\\n\" +\n+        \"specified by the <keyname> argument from within the\\n\" +\n+        \"provider specified --provider.\";\n \n     String keyName = null;\n     boolean cont = true;\n@@ -335,23 +346,21 @@ public DeleteCommand(String keyName) {\n     public boolean validate() {\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\n\"\n-            + \"Nothing will be deleted.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\"\n-            + \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. Nothing\\n\"\n+          + \"was deleted. Use the --provider option to specify a provider.\");\n         return false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-            \"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"There is no keyName specified. Please specify a \" +\n+            \"<keyname>. See the usage description with --help.\");\n         return false;\n       }\n       if (interactive) {\n         try {\n           cont = ToolRunner\n               .confirmPrompt(\"You are about to DELETE all versions of \"\n-                  + \"the key: \" + keyName + \" from KeyProvider \"\n-                  + provider.toString() + \". Continue?:\");\n+                  + \" key: \" + keyName + \" from KeyProvider \"\n+                  + provider + \". Continue?:\");\n           if (!cont) {\n             out.println(\"Nothing has been be deleted.\");\n           }\n@@ -367,15 +376,15 @@ public boolean validate() {\n     public void execute() throws IOException {\n       warnIfTransientProvider();\n       out.println(\"Deleting key: \" + keyName + \" from KeyProvider: \"\n-          + provider.toString());\n+          + provider);\n       if (cont) {\n         try {\n           provider.deleteKey(keyName);\n           out.println(keyName + \" has been successfully deleted.\");\n           provider.flush();\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \"has NOT been deleted.\");\n+          out.println(keyName + \" has not been deleted.\");\n           throw e;\n         }\n       }\n@@ -388,16 +397,16 @@ public String getUsage() {\n   }\n \n   private class CreateCommand extends Command {\n-    public static final String USAGE = \"create <keyname> [--cipher] \" +\n-    \t\t\"[--size] [--provider] [--help]\";\n+    public static final String USAGE =\n+      \"create <keyname> [--cipher <cipher>] [--size <size>]\\n\" +\n+      \"                     [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The create subcommand creates a new key for the name specified\\n\" +\n-        \"as the <keyname> argument within the provider indicated through\\n\" +\n-        \"the --provider argument. You may also indicate the specific\\n\" +\n-        \"cipher through the --cipher argument. The default for cipher is\\n\" +\n-        \"currently \\\"AES/CTR/NoPadding\\\". The default keysize is \\\"256\\\".\\n\" +\n-        \"You may also indicate the requested key length through the --size\\n\" +\n-        \"argument.\";\n+      \"The create subcommand creates a new key for the name specified\\n\" +\n+      \"by the <keyname> argument within the provider specified by the\\n\" +\n+      \"--provider argument. You may specify a cipher with the --cipher\\n\" +\n+      \"argument. The default cipher is currently \\\"AES/CTR/NoPadding\\\".\\n\" +\n+      \"The default keysize is 256. You may specify the requested key\\n\" +\n+      \"length using the --size argument.\\n\";\n \n     String keyName = null;\n \n@@ -409,15 +418,14 @@ public boolean validate() {\n       boolean rc = true;\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\nKey\" +\n-        \t\t\" will not be created.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\" +\n-            \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. No key\\n\" +\n+          \" was created. You can use the --provider option to specify\\n\" +\n+          \" a provider to use.\");\n         rc = false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-        \t\t\"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"Please provide a <keyname>. See the usage description\" +\n+          \" with --help.\");\n         rc = false;\n       }\n       return rc;\n@@ -432,13 +440,13 @@ public void execute() throws IOException, NoSuchAlgorithmException {\n         provider.flush();\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "sha": "3d56640e11e4f674a7c09c65b47b7208373fd599",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java?ref=4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "patch": "@@ -126,7 +126,6 @@ public KeyProvider createProvider(URI providerName, Configuration conf)\n     return o;\n   }\n \n-\n   public static String checkNotEmpty(String s, String name)\n       throws IllegalArgumentException {\n     checkNotNull(s, name);\n@@ -140,6 +139,13 @@ public static String checkNotEmpty(String s, String name)\n   private String kmsUrl;\n   private SSLFactory sslFactory;\n \n+  @Override\n+  public String toString() {\n+    final StringBuilder sb = new StringBuilder(\"KMSClientProvider[\");\n+    sb.append(kmsUrl).append(\"]\");\n+    return sb.toString();\n+  }\n+\n   public KMSClientProvider(URI uri, Configuration conf) throws IOException {\n     Path path = unnestUri(uri);\n     URL url = path.toUri().toURL();\n@@ -515,5 +521,4 @@ public void flush() throws IOException {\n   public static String buildVersionName(String name, int version) {\n     return KeyProvider.buildVersionName(name, version);\n   }\n-\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "sha": "ff30f86de377f4a763aec5ccd62d532101d99311",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java?ref=4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "patch": "@@ -121,7 +121,7 @@ public void testInvalidKeySize() throws Exception {\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n     assertEquals(-1, rc);\n-    assertTrue(outContent.toString().contains(\"key1 has NOT been created.\"));\n+    assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n   @Test\n@@ -134,7 +134,7 @@ public void testInvalidCipher() throws Exception {\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n     assertEquals(-1, rc);\n-    assertTrue(outContent.toString().contains(\"key1 has NOT been created.\"));\n+    assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "sha": "ae6938eb68ce2fe2f963d70438e8c0959a4edd1d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10583. bin/hadoop key throws NPE with no args and assorted other fixups. (clamb via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594320 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d63d557df4d42fdbe313bf1c397a206740120670",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestKeyProvider.java",
            "TestKeyShell.java"
        ]
    },
    "hadoop-common_56288ad": {
        "bug_id": "hadoop-common_56288ad",
        "commit": "https://github.com/apache/hadoop-common/commit/56288adc4f5e6c922053c84c131c03f9ea29e79e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=56288adc4f5e6c922053c84c131c03f9ea29e79e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -139,6 +139,9 @@ Trunk (Unreleased)\n \n     MAPREDUCE-5717. Task pings are interpreted as task progress (jlowe)\n \n+    MAPREDUCE-5867. Fix NPE in KillAMPreemptionPolicy related to \n+    ProportionalCapacityPreemptionPolicy (Sunil G via devaraj)\n+\n Release 2.5.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "c2a4f14514d0bd184f92e05ba595fadc2966b1b3",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java?ref=56288adc4f5e6c922053c84c131c03f9ea29e79e",
                "deletions": 6,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "patch": "@@ -29,7 +29,9 @@\n import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.PreemptionContainer;\n+import org.apache.hadoop.yarn.api.records.PreemptionContract;\n import org.apache.hadoop.yarn.api.records.PreemptionMessage;\n+import org.apache.hadoop.yarn.api.records.StrictPreemptionContract;\n import org.apache.hadoop.yarn.event.EventHandler;\n \n /**\n@@ -52,13 +54,18 @@ public void init(AppContext context) {\n   public void preempt(Context ctxt, PreemptionMessage preemptionRequests) {\n     // for both strict and negotiable preemption requests kill the\n     // container\n-    for (PreemptionContainer c :\n-        preemptionRequests.getStrictContract().getContainers()) {\n-      killContainer(ctxt, c);\n+    StrictPreemptionContract strictContract = preemptionRequests\n+        .getStrictContract();\n+    if (strictContract != null) {\n+      for (PreemptionContainer c : strictContract.getContainers()) {\n+        killContainer(ctxt, c);\n+      }\n     }\n-    for (PreemptionContainer c :\n-         preemptionRequests.getContract().getContainers()) {\n-       killContainer(ctxt, c);\n+    PreemptionContract contract = preemptionRequests.getContract();\n+    if (contract != null) {\n+      for (PreemptionContainer c : contract.getContainers()) {\n+        killContainer(ctxt, c);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "sha": "09237aaa297f82b2a60ecb2f5ba9f0d620130094",
                "status": "modified"
            },
            {
                "additions": 144,
                "blob_url": "https://github.com/apache/hadoop-common/blob/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "changes": 144,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java?ref=56288adc4f5e6c922053c84c131c03f9ea29e79e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "patch": "@@ -0,0 +1,144 @@\n+/**\r\n+ * Licensed to the Apache Software Foundation (ASF) under one\r\n+ * or more contributor license agreements.  See the NOTICE file\r\n+ * distributed with this work for additional information\r\n+ * regarding copyright ownership.  The ASF licenses this file\r\n+ * to you under the Apache License, Version 2.0 (the\r\n+ * \"License\"); you may not use this file except in compliance\r\n+ * with the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *     http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+package org.apache.hadoop.mapreduce.v2.app;\r\n+\r\n+import static org.mockito.Matchers.any;\r\n+import static org.mockito.Mockito.mock;\r\n+import static org.mockito.Mockito.times;\r\n+import static org.mockito.Mockito.verify;\r\n+import static org.mockito.Mockito.when;\r\n+\r\n+import java.util.ArrayList;\r\n+import java.util.HashSet;\r\n+import java.util.List;\r\n+import java.util.Set;\r\n+\r\n+import org.apache.hadoop.mapreduce.v2.api.records.TaskType;\r\n+import org.apache.hadoop.mapreduce.v2.app.MRAppMaster.RunningAppContext;\r\n+import org.apache.hadoop.mapreduce.v2.app.job.event.JobCounterUpdateEvent;\r\n+import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent;\r\n+import org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy;\r\n+import org.apache.hadoop.mapreduce.v2.app.rm.preemption.KillAMPreemptionPolicy;\r\n+import org.apache.hadoop.mapreduce.v2.util.MRBuilderUtils;\r\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\r\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\r\n+import org.apache.hadoop.yarn.api.records.Container;\r\n+import org.apache.hadoop.yarn.api.records.ContainerId;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionContainer;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionContract;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionMessage;\r\n+import org.apache.hadoop.yarn.api.records.StrictPreemptionContract;\r\n+import org.apache.hadoop.yarn.event.EventHandler;\r\n+import org.apache.hadoop.yarn.factories.RecordFactory;\r\n+import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\r\n+import org.junit.Test;\r\n+\r\n+public class TestKillAMPreemptionPolicy {\r\n+  private final RecordFactory recordFactory = RecordFactoryProvider\r\n+      .getRecordFactory(null);\r\n+\r\n+  @SuppressWarnings(\"unchecked\")\r\n+  @Test\r\n+  public void testKillAMPreemptPolicy() {\r\n+\r\n+    ApplicationId appId = ApplicationId.newInstance(123456789, 1);\r\n+    ContainerId container = ContainerId.newInstance(\r\n+        ApplicationAttemptId.newInstance(appId, 1), 1);\r\n+    AMPreemptionPolicy.Context mPctxt = mock(AMPreemptionPolicy.Context.class);\r\n+    when(mPctxt.getTaskAttempt(any(ContainerId.class))).thenReturn(\r\n+        MRBuilderUtils.newTaskAttemptId(MRBuilderUtils.newTaskId(\r\n+            MRBuilderUtils.newJobId(appId, 1), 1, TaskType.MAP), 0));\r\n+    List<Container> p = new ArrayList<Container>();\r\n+    p.add(Container.newInstance(container, null, null, null, null, null));\r\n+    when(mPctxt.getContainers(any(TaskType.class))).thenReturn(p);\r\n+\r\n+    KillAMPreemptionPolicy policy = new KillAMPreemptionPolicy();\r\n+\r\n+    // strictContract is null & contract is null\r\n+    RunningAppContext mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    PreemptionMessage pM = getPreemptionMessage(false, false, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(0)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(0)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is not null & contract is null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(true, false, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is null & contract is not null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(false, true, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is not null & contract is not null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(true, true, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(4)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(4)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+  }\r\n+\r\n+  private RunningAppContext getRunningAppContext() {\r\n+    RunningAppContext mActxt = mock(RunningAppContext.class);\r\n+    EventHandler<?> eventHandler = mock(EventHandler.class);\r\n+    when(mActxt.getEventHandler()).thenReturn(eventHandler);\r\n+    return mActxt;\r\n+  }\r\n+\r\n+  private PreemptionMessage getPreemptionMessage(boolean strictContract,\r\n+      boolean contract, final ContainerId container) {\r\n+    PreemptionMessage preemptionMessage = recordFactory\r\n+        .newRecordInstance(PreemptionMessage.class);\r\n+    Set<PreemptionContainer> cntrs = new HashSet<PreemptionContainer>();\r\n+    PreemptionContainer preemptContainer = recordFactory\r\n+        .newRecordInstance(PreemptionContainer.class);\r\n+    preemptContainer.setId(container);\r\n+    cntrs.add(preemptContainer);\r\n+    if (strictContract) {\r\n+      StrictPreemptionContract set = recordFactory\r\n+          .newRecordInstance(StrictPreemptionContract.class);\r\n+      set.setContainers(cntrs);\r\n+      preemptionMessage.setStrictContract(set);\r\n+    }\r\n+    if (contract) {\r\n+      PreemptionContract preemptContract = recordFactory\r\n+          .newRecordInstance(PreemptionContract.class);\r\n+      preemptContract.setContainers(cntrs);\r\n+      preemptionMessage.setContract(preemptContract);\r\n+    }\r\n+    return preemptionMessage;\r\n+  }\r\n+\r\n+}\r",
                "raw_url": "https://github.com/apache/hadoop-common/raw/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "sha": "fa930ae1262be2ed4687cc09456a5d80496c7b0d",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-5867. Fix NPE in KillAMPreemptionPolicy related to ProportionalCapacityPreemptionPolicy. Contributed by Sunil G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595754 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/8544b0c207a058fe3f22f20e303e80ba16fd1557",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestKillAMPreemptionPolicy.java"
        ]
    },
    "hadoop-common_5a79a63": {
        "bug_id": "hadoop-common_5a79a63",
        "commit": "https://github.com/apache/hadoop-common/commit/5a79a633a96797443600d9437cf79cb4dcbf1a11",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=5a79a633a96797443600d9437cf79cb4dcbf1a11",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -539,6 +539,9 @@ Release 2.4.0 - UNRELEASED\n     YARN-1670. Fixed a bug in log-aggregation that can cause the writer to write\n     more log-data than the log-length that it records. (Mit Desai via vinodk)\n \n+    YARN-1849. Fixed NPE in ResourceTrackerService#registerNodeManager for UAM\n+    (Karthik Kambatla via jianhe )\n+\n Release 2.3.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/CHANGES.txt",
                "sha": "cfb0052a201002073a5a8de6ed7ca0adea4dea93",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=5a79a633a96797443600d9437cf79cb4dcbf1a11",
                "deletions": 24,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.hadoop.service.AbstractService;\n import org.apache.hadoop.util.VersionUtil;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerState;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n@@ -187,12 +188,51 @@ protected void serviceStop() throws Exception {\n     super.serviceStop();\n   }\n \n+  /**\n+   * Helper method to handle received ContainerStatus. If this corresponds to\n+   * the completion of a master-container of a managed AM,\n+   * we call the handler for RMAppAttemptContainerFinishedEvent.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  @VisibleForTesting\n+  void handleContainerStatus(ContainerStatus containerStatus) {\n+    ApplicationAttemptId appAttemptId =\n+        containerStatus.getContainerId().getApplicationAttemptId();\n+    RMApp rmApp =\n+        rmContext.getRMApps().get(appAttemptId.getApplicationId());\n+    if (rmApp == null) {\n+      LOG.error(\"Received finished container : \"\n+          + containerStatus.getContainerId()\n+          + \"for unknown application \" + appAttemptId.getApplicationId()\n+          + \" Skipping.\");\n+      return;\n+    }\n+\n+    if (rmApp.getApplicationSubmissionContext().getUnmanagedAM()) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Ignoring container completion status for unmanaged AM\"\n+            + rmApp.getApplicationId());\n+      }\n+      return;\n+    }\n+\n+    RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n+    Container masterContainer = rmAppAttempt.getMasterContainer();\n+    if (masterContainer.getId().equals(containerStatus.getContainerId())\n+        && containerStatus.getState() == ContainerState.COMPLETE) {\n+      // sending master container finished event.\n+      RMAppAttemptContainerFinishedEvent evt =\n+          new RMAppAttemptContainerFinishedEvent(appAttemptId,\n+              containerStatus);\n+      rmContext.getDispatcher().getEventHandler().handle(evt);\n+    }\n+  }\n+\n   @SuppressWarnings(\"unchecked\")\n   @Override\n   public RegisterNodeManagerResponse registerNodeManager(\n       RegisterNodeManagerRequest request) throws YarnException,\n       IOException {\n-\n     NodeId nodeId = request.getNodeId();\n     String host = nodeId.getHost();\n     int cmPort = nodeId.getPort();\n@@ -204,29 +244,7 @@ public RegisterNodeManagerResponse registerNodeManager(\n       LOG.info(\"received container statuses on node manager register :\"\n           + request.getContainerStatuses());\n       for (ContainerStatus containerStatus : request.getContainerStatuses()) {\n-        ApplicationAttemptId appAttemptId =\n-            containerStatus.getContainerId().getApplicationAttemptId();\n-        RMApp rmApp =\n-            rmContext.getRMApps().get(appAttemptId.getApplicationId());\n-        if (rmApp != null) {\n-          RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n-          if (rmAppAttempt != null) {\n-            if (rmAppAttempt.getMasterContainer().getId()\n-                .equals(containerStatus.getContainerId())\n-                && containerStatus.getState() == ContainerState.COMPLETE) {\n-              // sending master container finished event.\n-              RMAppAttemptContainerFinishedEvent evt =\n-                  new RMAppAttemptContainerFinishedEvent(appAttemptId,\n-                      containerStatus);\n-              rmContext.getDispatcher().getEventHandler().handle(evt);\n-            }\n-          }\n-        } else {\n-          LOG.error(\"Received finished container :\"\n-              + containerStatus.getContainerId()\n-              + \" for non existing application :\"\n-              + appAttemptId.getApplicationId());\n-        }\n+        handleContainerStatus(containerStatus);\n       }\n     }\n     RegisterNodeManagerResponse response = recordFactory",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "1d4032048e468a9652093acc18cf0c0b82007a70",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java?ref=5a79a633a96797443600d9437cf79cb4dcbf1a11",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "patch": "@@ -35,9 +35,11 @@\n \n import javax.crypto.SecretKey;\n \n+import com.google.common.annotations.VisibleForTesting;\n import org.apache.commons.lang.StringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.security.Credentials;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -629,7 +631,9 @@ public Container getMasterContainer() {\n     }\n   }\n \n-  private void setMasterContainer(Container container) {\n+  @InterfaceAudience.Private\n+  @VisibleForTesting\n+  public void setMasterContainer(Container container) {\n     masterContainer = container;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "sha": "3e90ec8ec1d5aaec6619f357ceaadecbb7bca194",
                "status": "modified"
            },
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java?ref=5a79a633a96797443600d9437cf79cb4dcbf1a11",
                "deletions": 13,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "patch": "@@ -26,8 +26,6 @@\n import java.util.HashMap;\n import java.util.List;\n \n-import org.junit.Assert;\n-\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.metrics2.MetricsSystem;\n@@ -45,21 +43,29 @@\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.Dispatcher;\n import org.apache.hadoop.yarn.event.DrainDispatcher;\n+import org.apache.hadoop.yarn.event.Event;\n import org.apache.hadoop.yarn.event.EventHandler;\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse;\n import org.apache.hadoop.yarn.server.api.records.NodeAction;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n import org.apache.hadoop.yarn.util.Records;\n import org.apache.hadoop.yarn.util.YarnVersionInfo;\n+\n import org.junit.After;\n+import org.junit.Assert;\n import org.junit.Test;\n \n import static org.junit.Assert.assertEquals;\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.spy;\n+import static org.mockito.Mockito.verify;\n \n public class TestResourceTrackerService {\n \n@@ -468,26 +474,64 @@ private void checkUnealthyNMCount(MockRM rm, MockNM nm1, boolean health,\n         ClusterMetrics.getMetrics().getUnhealthyNMs());\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n   @Test\n-  public void testNodeRegistrationWithContainers() throws Exception {\n-    rm = new MockRM();\n-    rm.init(new YarnConfiguration());\n+  public void testHandleContainerStatusInvalidCompletions() throws Exception {\n+    rm = new MockRM(new YarnConfiguration());\n     rm.start();\n-    RMApp app = rm.submitApp(1024);\n \n-    MockNM nm = rm.registerNode(\"host1:1234\", 8192);\n-    nm.nodeHeartbeat(true);\n+    EventHandler handler =\n+        spy(rm.getRMContext().getDispatcher().getEventHandler());\n \n-    // Register node with some container statuses\n+    // Case 1: Unmanaged AM\n+    RMApp app = rm.submitApp(1024, true);\n+\n+    // Case 1.1: AppAttemptId is null\n     ContainerStatus status = ContainerStatus.newInstance(\n         ContainerId.newInstance(ApplicationAttemptId.newInstance(\n             app.getApplicationId(), 2), 1),\n         ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    rm.getResourceTrackerService().handleContainerStatus(status);\n+    verify(handler, never()).handle((Event) any());\n+\n+    // Case 1.2: Master container is null\n+    RMAppAttemptImpl currentAttempt =\n+        (RMAppAttemptImpl) app.getCurrentAppAttempt();\n+    currentAttempt.setMasterContainer(null);\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(currentAttempt.getAppAttemptId(), 0),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    rm.getResourceTrackerService().handleContainerStatus(status);\n+    verify(handler, never()).handle((Event)any());\n \n-    // The following shouldn't throw NPE\n-    nm.registerNode(Collections.singletonList(status));\n-    assertEquals(\"Incorrect number of nodes\", 1,\n-        rm.getRMContext().getRMNodes().size());\n+    // Case 2: Managed AM\n+    app = rm.submitApp(1024);\n+\n+    // Case 2.1: AppAttemptId is null\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(ApplicationAttemptId.newInstance(\n+            app.getApplicationId(), 2), 1),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    try {\n+      rm.getResourceTrackerService().handleContainerStatus(status);\n+    } catch (Exception e) {\n+      // expected - ignore\n+    }\n+    verify(handler, never()).handle((Event)any());\n+\n+    // Case 2.2: Master container is null\n+    currentAttempt =\n+        (RMAppAttemptImpl) app.getCurrentAppAttempt();\n+    currentAttempt.setMasterContainer(null);\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(currentAttempt.getAppAttemptId(), 0),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    try {\n+      rm.getResourceTrackerService().handleContainerStatus(status);\n+    } catch (Exception e) {\n+      // expected - ignore\n+    }\n+    verify(handler, never()).handle((Event)any());\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "sha": "2f16b85699d3705be3743e4d54b069b42fab2e3d",
                "status": "modified"
            }
        ],
        "message": "YARN-1849. Fixed NPE in ResourceTrackerService#registerNodeManager for UAM. Contributed by Karthik Kambatla\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1580077 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f3bd13bf8b31785c787632de80b90c81d30a6ff1",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop-common_5eb5803": {
        "bug_id": "hadoop-common_5eb5803",
        "commit": "https://github.com/apache/hadoop-common/commit/5eb5803c3158502e02096a9aa42fa8520eb413f6",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java?ref=5eb5803c3158502e02096a9aa42fa8520eb413f6",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "patch": "@@ -202,8 +202,12 @@ public static String uriToString(URI[] uris){\n   }\n   \n   /**\n-   * \n    * @param str\n+   *          The string array to be parsed into an URI array.\n+   * @return <tt>null</tt> if str is <tt>null</tt>, else the URI array\n+   *         equivalent to str.\n+   * @throws IllegalArgumentException\n+   *           If any string in str violates RFC&nbsp;2396.\n    */\n   public static URI[] stringToURI(String[] str){\n     if (str == null) \n@@ -213,9 +217,8 @@ public static String uriToString(URI[] uris){\n       try{\n         uris[i] = new URI(str[i]);\n       }catch(URISyntaxException ur){\n-        System.out.println(\"Exception in specified URI's \" + StringUtils.stringifyException(ur));\n-        //making sure its asssigned to null in case of an error\n-        uris[i] = null;\n+        throw new IllegalArgumentException(\n+            \"Failed to create uri for \" + str[i], ur);\n       }\n     }\n     return uris;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "sha": "67a8f82d9382cfd19d0c803144248c213359a0be",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java?ref=5eb5803c3158502e02096a9aa42fa8520eb413f6",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "patch": "@@ -269,6 +269,17 @@ public void testCamelize() {\n     assertEquals(\"Yy\", StringUtils.camelize(\"yY\"));\n     assertEquals(\"Zz\", StringUtils.camelize(\"zZ\"));\n   }\n+  \n+  @Test\n+  public void testStringToURI() {\n+    String[] str = new String[] { \"file://\" };\n+    try {\n+      StringUtils.stringToURI(str);\n+      fail(\"Ignoring URISyntaxException while creating URI from string file://\");\n+    } catch (IllegalArgumentException iae) {\n+      assertEquals(\"Failed to create uri for file://\", iae.getMessage());\n+    }\n+  }\n \n   // Benchmark for StringUtils split\n   public static void main(String []args) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "sha": "fc90984608fb1a2630184419a840177223e9cb12",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=5eb5803c3158502e02096a9aa42fa8520eb413f6",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -236,6 +236,9 @@ Branch-2 ( Unreleased changes )\n     HADOOP-8499. Lower min.user.id to 500 for the tests.\n     (Colin Patrick McCabe via eli)\n \n+    MAPREDUCE-4395. Possible NPE at ClientDistributedCacheManager\n+    #determineTimestamps (Bhallamudi via bobby)\n+\n Release 2.0.0-alpha - 05-23-2012\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "db352254f14824b21d8b6f84f401ed03722e859e",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4395. Possible NPE at ClientDistributedCacheManager#determineTimestamps (Bhallamudi via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362052 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/9f9af6ed5506f0ab0cd0a67b989f8b7f5032b058",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestStringUtils.java"
        ]
    },
    "hadoop-common_67055a5": {
        "bug_id": "hadoop-common_67055a5",
        "commit": "https://github.com/apache/hadoop-common/commit/67055a5c86a325cca505a0a53e3f0d04d80378ab",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=67055a5c86a325cca505a0a53e3f0d04d80378ab",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1695,6 +1695,8 @@ Release 0.23.0 - Unreleased\n     MAPREDUCE-2788. Normalize resource requests in FifoScheduler\n     appropriately. (Ahmed Radwan via acmurthy) \n \n+    MAPREDUCE-2693. Fix NPE in job-blacklisting. (Hitesh Shah via acmurthy) \n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "22917ec8c83d7882471017e315f2646fb1a47dea",
                "status": "modified"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/hadoop-common/blob/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "changes": 160,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java?ref=67055a5c86a325cca505a0a53e3f0d04d80378ab",
                "deletions": 37,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "patch": "@@ -509,18 +509,6 @@ void addMap(ContainerRequestEvent event) {\n         request = new ContainerRequest(event, PRIORITY_FAST_FAIL_MAP);\n       } else {\n         for (String host : event.getHosts()) {\n-          //host comes from data splitLocations which are hostnames. Containers\n-          // use IP addresses.\n-          //TODO Temporary fix for locality. Use resolvers from h-common. \n-          // Cache to make this more efficient ?\n-          InetAddress addr = null;\n-          try {\n-            addr = InetAddress.getByName(host);\n-          } catch (UnknownHostException e) {\n-            LOG.warn(\"Unable to resolve host to IP for host [: \" + host + \"]\");\n-          }\n-          if (addr != null) //Fallback to host if resolve fails.\n-            host = addr.getHostAddress();\n           LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n           if (list == null) {\n             list = new LinkedList<TaskAttemptId>();\n@@ -557,26 +545,101 @@ private void assign(List<Container> allocatedContainers) {\n       while (it.hasNext()) {\n         Container allocated = it.next();\n         LOG.info(\"Assigning container \" + allocated);\n-        ContainerRequest assigned = assign(allocated);\n-          \n-        if (assigned != null) {\n-          // Update resource requests\n-          decContainerReq(assigned);\n+        \n+        // check if allocated container meets memory requirements \n+        // and whether we have any scheduled tasks that need \n+        // a container to be assigned\n+        boolean isAssignable = true;\n+        Priority priority = allocated.getPriority();\n+        if (PRIORITY_FAST_FAIL_MAP.equals(priority) \n+            || PRIORITY_MAP.equals(priority)) {\n+          if (allocated.getResource().getMemory() < mapResourceReqt\n+              || maps.isEmpty()) {\n+            LOG.info(\"Cannot assign container \" + allocated \n+                + \" for a map as either \"\n+                + \" container memory less than required \" + mapResourceReqt\n+                + \" or no pending map tasks - maps.isEmpty=\" \n+                + maps.isEmpty()); \n+            isAssignable = false; \n+          }\n+        } \n+        else if (PRIORITY_REDUCE.equals(priority)) {\n+          if (allocated.getResource().getMemory() < reduceResourceReqt\n+              || reduces.isEmpty()) {\n+            LOG.info(\"Cannot assign container \" + allocated \n+                + \" for a reduce as either \"\n+                + \" container memory less than required \" + reduceResourceReqt\n+                + \" or no pending reduce tasks - reduces.isEmpty=\" \n+                + reduces.isEmpty()); \n+            isAssignable = false;\n+          }\n+        }          \n+        \n+        boolean blackListed = false;         \n+        ContainerRequest assigned = null;\n+        \n+        if (isAssignable) {\n+          // do not assign if allocated container is on a  \n+          // blacklisted host\n+          blackListed = isNodeBlacklisted(allocated.getNodeId().getHost());\n+          if (blackListed) {\n+            // we need to request for a new container \n+            // and release the current one\n+            LOG.info(\"Got allocated container on a blacklisted \"\n+                + \" host. Releasing container \" + allocated);\n+\n+            // find the request matching this allocated container \n+            // and replace it with a new one \n+            ContainerRequest toBeReplacedReq = \n+                getContainerReqToReplace(allocated);\n+            if (toBeReplacedReq != null) {\n+              LOG.info(\"Placing a new container request for task attempt \" \n+                  + toBeReplacedReq.attemptID);\n+              ContainerRequest newReq = \n+                  getFilteredContainerRequest(toBeReplacedReq);\n+              decContainerReq(toBeReplacedReq);\n+              if (toBeReplacedReq.attemptID.getTaskId().getTaskType() ==\n+                  TaskType.MAP) {\n+                maps.put(newReq.attemptID, newReq);\n+              }\n+              else {\n+                reduces.put(newReq.attemptID, newReq);\n+              }\n+              addContainerReq(newReq);\n+            }\n+            else {\n+              LOG.info(\"Could not map allocated container to a valid request.\"\n+                  + \" Releasing allocated container \" + allocated);\n+            }\n+          }\n+          else {\n+            assigned = assign(allocated);\n+            if (assigned != null) {\n+              // Update resource requests\n+              decContainerReq(assigned);\n \n-          // send the container-assigned event to task attempt\n-          eventHandler.handle(new TaskAttemptContainerAssignedEvent(\n-              assigned.attemptID, allocated));\n+              // send the container-assigned event to task attempt\n+              eventHandler.handle(new TaskAttemptContainerAssignedEvent(\n+                  assigned.attemptID, allocated));\n \n-          assignedRequests.add(allocated.getId(), assigned.attemptID);\n-          \n-          LOG.info(\"Assigned container (\" + allocated + \") \" +\n-              \" to task \" + assigned.attemptID +\n-              \" on node \" + allocated.getNodeId().toString());\n-        } else {\n-          //not assigned to any request, release the container\n-          LOG.info(\"Releasing unassigned and invalid container \" + allocated\n-              + \". RM has gone crazy, someone go look!\"\n-              + \" Hey RM, if you are so rich, go donate to non-profits!\");\n+              assignedRequests.add(allocated.getId(), assigned.attemptID);\n+\n+              LOG.info(\"Assigned container (\" + allocated + \") \" +\n+                  \" to task \" + assigned.attemptID +\n+                  \" on node \" + allocated.getNodeId().toString());\n+            }\n+            else {\n+              //not assigned to any request, release the container\n+              LOG.info(\"Releasing unassigned and invalid container \" \n+                  + allocated + \". RM has gone crazy, someone go look!\"\n+                  + \" Hey RM, if you are so rich, go donate to non-profits!\");\n+            }\n+          }\n+        }\n+        \n+        // release container if it was blacklisted \n+        // or if we could not assign it \n+        if (blackListed || assigned == null) {\n           containersReleased++;\n           release(allocated.getId());\n         }\n@@ -604,12 +667,37 @@ private ContainerRequest assign(Container allocated) {\n       return assigned;\n     }\n     \n+    private ContainerRequest getContainerReqToReplace(Container allocated) {\n+      Priority priority = allocated.getPriority();\n+      ContainerRequest toBeReplaced = null;\n+      if (PRIORITY_FAST_FAIL_MAP.equals(priority) \n+          || PRIORITY_MAP.equals(priority)) {\n+        // allocated container was for a map\n+        String host = allocated.getNodeId().getHost();\n+        LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n+        if (list != null && list.size() > 0) {\n+          TaskAttemptId tId = list.removeLast();\n+          if (maps.containsKey(tId)) {\n+            toBeReplaced = maps.remove(tId);\n+          }\n+        }\n+        else {\n+          TaskAttemptId tId = maps.keySet().iterator().next();\n+          toBeReplaced = maps.remove(tId);          \n+        }        \n+      }\n+      else if (PRIORITY_REDUCE.equals(priority)) {\n+        TaskAttemptId tId = reduces.keySet().iterator().next();\n+        toBeReplaced = reduces.remove(tId);    \n+      }\n+      return toBeReplaced;\n+    }\n+    \n     \n     private ContainerRequest assignToFailedMap(Container allocated) {\n       //try to assign to earlierFailedMaps if present\n       ContainerRequest assigned = null;\n-      while (assigned == null && earlierFailedMaps.size() > 0 && \n-          allocated.getResource().getMemory() >= mapResourceReqt) {\n+      while (assigned == null && earlierFailedMaps.size() > 0) {\n         TaskAttemptId tId = earlierFailedMaps.removeFirst();\n         if (maps.containsKey(tId)) {\n           assigned = maps.remove(tId);\n@@ -627,8 +715,7 @@ private ContainerRequest assignToFailedMap(Container allocated) {\n     private ContainerRequest assignToReduce(Container allocated) {\n       ContainerRequest assigned = null;\n       //try to assign to reduces if present\n-      if (assigned == null && reduces.size() > 0\n-          && allocated.getResource().getMemory() >= reduceResourceReqt) {\n+      if (assigned == null && reduces.size() > 0) {\n         TaskAttemptId tId = reduces.keySet().iterator().next();\n         assigned = reduces.remove(tId);\n         LOG.info(\"Assigned to reduce\");\n@@ -640,9 +727,8 @@ private ContainerRequest assignToMap(Container allocated) {\n     //try to assign to maps if present \n       //first by host, then by rack, followed by *\n       ContainerRequest assigned = null;\n-      while (assigned == null && maps.size() > 0\n-          && allocated.getResource().getMemory() >= mapResourceReqt) {\n-        String host = getHost(allocated.getNodeId().toString());\n+      while (assigned == null && maps.size() > 0) {\n+        String host = allocated.getNodeId().getHost();\n         LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n         while (list != null && list.size() > 0) {\n           LOG.info(\"Host matched to the request list \" + host);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "sha": "fc45b8f11b285a1bb0fc475c184f61b8e1c65175",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/hadoop-common/blob/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java?ref=67055a5c86a325cca505a0a53e3f0d04d80378ab",
                "deletions": 11,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.hadoop.mapreduce.v2.app.rm;\n \n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.HashSet;\n@@ -63,7 +65,7 @@\n   //Key->ResourceName (e.g., hostname, rackname, *)\n   //Value->Map\n   //Key->Resource Capability\n-  //Value->ResourceReqeust\n+  //Value->ResourceRequest\n   private final Map<Priority, Map<String, Map<Resource, ResourceRequest>>>\n   remoteRequestsTable =\n       new TreeMap<Priority, Map<String, Map<Resource, ResourceRequest>>>();\n@@ -87,14 +89,22 @@ public RMContainerRequestor(ClientService clientService, AppContext context) {\n     final String[] racks;\n     //final boolean earlierAttemptFailed;\n     final Priority priority;\n+    \n     public ContainerRequest(ContainerRequestEvent event, Priority priority) {\n-      this.attemptID = event.getAttemptID();\n-      this.capability = event.getCapability();\n-      this.hosts = event.getHosts();\n-      this.racks = event.getRacks();\n-      //this.earlierAttemptFailed = event.getEarlierAttemptFailed();\n+      this(event.getAttemptID(), event.getCapability(), event.getHosts(),\n+          event.getRacks(), priority);\n+    }\n+    \n+    public ContainerRequest(TaskAttemptId attemptID,\n+        Resource capability, String[] hosts, String[] racks, \n+        Priority priority) {\n+      this.attemptID = attemptID;\n+      this.capability = capability;\n+      this.hosts = hosts;\n+      this.racks = racks;\n       this.priority = priority;\n     }\n+    \n   }\n \n   @Override\n@@ -149,14 +159,37 @@ protected void containerFailedOnHost(String hostName) {\n       //remove all the requests corresponding to this hostname\n       for (Map<String, Map<Resource, ResourceRequest>> remoteRequests \n           : remoteRequestsTable.values()){\n-        //remove from host\n-        Map<Resource, ResourceRequest> reqMap = remoteRequests.remove(hostName);\n+        //remove from host if no pending allocations\n+        boolean foundAll = true;\n+        Map<Resource, ResourceRequest> reqMap = remoteRequests.get(hostName);\n         if (reqMap != null) {\n           for (ResourceRequest req : reqMap.values()) {\n-            ask.remove(req);\n+            if (!ask.remove(req)) {\n+              foundAll = false;\n+            }\n+            else {\n+              // if ask already sent to RM, we can try and overwrite it if possible.\n+              // send a new ask to RM with numContainers\n+              // specified for the blacklisted host to be 0.\n+              ResourceRequest zeroedRequest = BuilderUtils.newResourceRequest(req);\n+              zeroedRequest.setNumContainers(0);\n+              // to be sent to RM on next heartbeat\n+              ask.add(zeroedRequest);\n+            }\n           }\n+          // if all requests were still in ask queue\n+          // we can remove this request\n+          if (foundAll) {\n+            remoteRequests.remove(hostName);\n+          }     \n         }\n-        //TODO: remove from rack\n+        // TODO handling of rack blacklisting\n+        // Removing from rack should be dependent on no. of failures within the rack \n+        // Blacklisting a rack on the basis of a single node's blacklisting \n+        // may be overly aggressive. \n+        // Node failures could be co-related with other failures on the same rack \n+        // but we probably need a better approach at trying to decide how and when \n+        // to blacklist a rack\n       }\n     } else {\n       nodeFailures.put(hostName, failures);\n@@ -171,7 +204,9 @@ protected void addContainerReq(ContainerRequest req) {\n     // Create resource requests\n     for (String host : req.hosts) {\n       // Data-local\n-      addResourceRequest(req.priority, host, req.capability);\n+      if (!isNodeBlacklisted(host)) {\n+        addResourceRequest(req.priority, host, req.capability);\n+      }      \n     }\n \n     // Nothing Rack-local for now\n@@ -234,6 +269,14 @@ private void decResourceRequest(Priority priority, String resourceName,\n     Map<String, Map<Resource, ResourceRequest>> remoteRequests =\n       this.remoteRequestsTable.get(priority);\n     Map<Resource, ResourceRequest> reqMap = remoteRequests.get(resourceName);\n+    if (reqMap == null) {\n+      // as we modify the resource requests by filtering out blacklisted hosts \n+      // when they are added, this value may be null when being \n+      // decremented\n+      LOG.debug(\"Not decrementing resource as \" + resourceName\n+          + \" is not present in request table\");\n+      return;\n+    }\n     ResourceRequest remoteRequest = reqMap.get(capability);\n \n     LOG.info(\"BEFORE decResourceRequest:\" + \" applicationId=\" + applicationId.getId()\n@@ -267,4 +310,23 @@ protected void release(ContainerId containerId) {\n     release.add(containerId);\n   }\n   \n+  protected boolean isNodeBlacklisted(String hostname) {\n+    if (!nodeBlacklistingEnabled) {\n+      return false;\n+    }\n+    return blacklistedNodes.contains(hostname);\n+  }\n+  \n+  protected ContainerRequest getFilteredContainerRequest(ContainerRequest orig) {\n+    ArrayList<String> newHosts = new ArrayList<String>();\n+    for (String host : orig.hosts) {\n+      if (!isNodeBlacklisted(host)) {\n+        newHosts.add(host);      \n+      }\n+    }\n+    String[] hosts = newHosts.toArray(new String[newHosts.size()]);\n+    ContainerRequest newReq = new ContainerRequest(orig.attemptID, orig.capability,\n+        hosts, orig.racks, orig.priority); \n+    return newReq;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "sha": "cfedde2229aadb4355bc2ea93f25d3d13d83e858",
                "status": "modified"
            },
            {
                "additions": 121,
                "blob_url": "https://github.com/apache/hadoop-common/blob/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "changes": 121,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java?ref=67055a5c86a325cca505a0a53e3f0d04d80378ab",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.MRJobConfig;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n import org.apache.hadoop.mapreduce.v2.api.records.JobReport;\n import org.apache.hadoop.mapreduce.v2.api.records.JobState;\n@@ -44,6 +45,7 @@\n import org.apache.hadoop.mapreduce.v2.app.job.Job;\n import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptContainerAssignedEvent;\n import org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl;\n+import org.apache.hadoop.mapreduce.v2.app.rm.ContainerFailedEvent;\n import org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent;\n import org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator;\n import org.apache.hadoop.mapreduce.v2.util.MRBuilderUtils;\n@@ -478,6 +480,105 @@ public void testReportedAppProgressWithOnlyMaps() throws Exception {\n     Assert.assertEquals(100.0f, app.getProgress(), 0.0);\n   }\n \n+  @Test\n+  public void testBlackListedNodes() throws Exception {\n+    \n+    LOG.info(\"Running testBlackListedNodes\");\n+\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);\n+    conf.setInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 1);\n+    \n+    MyResourceManager rm = new MyResourceManager(conf);\n+    rm.start();\n+    DrainDispatcher dispatcher = (DrainDispatcher) rm.getRMContext()\n+        .getDispatcher();\n+\n+    // Submit the application\n+    RMApp app = rm.submitApp(1024);\n+    dispatcher.await();\n+\n+    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\n+    amNodeManager.nodeHeartbeat(true);\n+    dispatcher.await();\n+\n+    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt()\n+        .getAppAttemptId();\n+    rm.sendAMLaunched(appAttemptId);\n+    dispatcher.await();\n+    \n+    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\n+    Job mockJob = mock(Job.class);\n+    when(mockJob.getReport()).thenReturn(\n+        MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING,\n+            0, 0, 0, 0, 0, 0, \"jobfile\"));\n+    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf,\n+        appAttemptId, mockJob);\n+\n+    // add resources to scheduler\n+    MockNM nodeManager1 = rm.registerNode(\"h1:1234\", 10240);\n+    MockNM nodeManager2 = rm.registerNode(\"h2:1234\", 10240);\n+    MockNM nodeManager3 = rm.registerNode(\"h3:1234\", 10240);\n+    dispatcher.await();\n+\n+    // create the container request\n+    ContainerRequestEvent event1 = createReq(jobId, 1, 1024,\n+        new String[] { \"h1\" });\n+    allocator.sendRequest(event1);\n+\n+    // send 1 more request with different resource req\n+    ContainerRequestEvent event2 = createReq(jobId, 2, 1024,\n+        new String[] { \"h2\" });\n+    allocator.sendRequest(event2);\n+\n+    // send another request with different resource and priority\n+    ContainerRequestEvent event3 = createReq(jobId, 3, 1024,\n+        new String[] { \"h3\" });\n+    allocator.sendRequest(event3);\n+\n+    // this tells the scheduler about the requests\n+    // as nodes are not added, no allocations\n+    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\n+\n+    // Send events to blacklist nodes h1 and h2\n+    ContainerFailedEvent f1 = createFailEvent(jobId, 1, \"h1\", false);            \n+    allocator.sendFailure(f1);\n+    ContainerFailedEvent f2 = createFailEvent(jobId, 1, \"h2\", false);            \n+    allocator.sendFailure(f2);\n+\n+    // update resources in scheduler\n+    nodeManager1.nodeHeartbeat(true); // Node heartbeat\n+    nodeManager2.nodeHeartbeat(true); // Node heartbeat\n+    dispatcher.await();\n+\n+    assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());    \n+\n+    // mark h1/h2 as bad nodes\n+    nodeManager1.nodeHeartbeat(false);\n+    nodeManager2.nodeHeartbeat(false);\n+    dispatcher.await();\n+\n+    assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());    \n+\n+    nodeManager3.nodeHeartbeat(true); // Node heartbeat\n+    assigned = allocator.schedule();    \n+    dispatcher.await();\n+        \n+    Assert.assertTrue(\"No of assignments must be 3\", assigned.size() == 3);\n+    \n+    // validate that all containers are assigned to h3\n+    for (TaskAttemptContainerAssignedEvent assig : assigned) {\n+      Assert.assertTrue(\"Assigned container host not correct\", \"h3\".equals(assig\n+          .getContainer().getNodeId().getHost()));\n+    }\n+  }\n+  \n   private static class MyFifoScheduler extends FifoScheduler {\n \n     public MyFifoScheduler(RMContext rmContext) {\n@@ -534,6 +635,19 @@ private ContainerRequestEvent createReq(JobId jobId, int taskAttemptId,\n         new String[] { NetworkTopology.DEFAULT_RACK });\n   }\n \n+  private ContainerFailedEvent createFailEvent(JobId jobId, int taskAttemptId,\n+      String host, boolean reduce) {\n+    TaskId taskId;\n+    if (reduce) {\n+      taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.REDUCE);\n+    } else {\n+      taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);\n+    }\n+    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId,\n+        taskAttemptId);\n+    return new ContainerFailedEvent(attemptId, host);    \n+  }\n+  \n   private void checkAssignments(ContainerRequestEvent[] requests,\n       List<TaskAttemptContainerAssignedEvent> assignments,\n       boolean checkHostMatch) {\n@@ -653,6 +767,10 @@ public void sendRequests(List<ContainerRequestEvent> reqs) {\n       }\n     }\n \n+    public void sendFailure(ContainerFailedEvent f) {\n+      super.handle(f);\n+    }\n+    \n     // API to be used by tests\n     public List<TaskAttemptContainerAssignedEvent> schedule() {\n       // run the scheduler\n@@ -672,6 +790,7 @@ public void sendRequests(List<ContainerRequestEvent> reqs) {\n     protected void startAllocatorThread() {\n       // override to NOT start thread\n     }\n+        \n   }\n \n   public static void main(String[] args) throws Exception {\n@@ -681,5 +800,7 @@ public static void main(String[] args) throws Exception {\n     t.testMapReduceScheduling();\n     t.testReportedAppProgress();\n     t.testReportedAppProgressWithOnlyMaps();\n+    t.testBlackListedNodes();\n   }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "sha": "dfbae8092c0b8f0e576ea16ce911cf9eb47e4e40",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-2693. Fix NPE in job-blacklisting. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186529 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/07d7bb056fa9369ff5c1abac5a5b41109587c659",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRMContainerAllocator.java"
        ]
    },
    "hadoop-common_6802695": {
        "bug_id": "hadoop-common_6802695",
        "commit": "https://github.com/apache/hadoop-common/commit/6802695868b11e9a403384eb37ab9aa5661e90d1",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6802695868b11e9a403384eb37ab9aa5661e90d1/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=6802695868b11e9a403384eb37ab9aa5661e90d1",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -455,6 +455,8 @@ Release 0.22.0 - Unreleased\n \n     HADOOP-7046. Fix Findbugs warning in Configuration. (Po Cheung via shv)\n \n+    HADOOP-7118. Fix NPE in Configuration.writeXml (todd)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6802695868b11e9a403384eb37ab9aa5661e90d1/CHANGES.txt",
                "sha": "18504560f655dd8c1cf667f83c94190fcf02c6b3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6802695868b11e9a403384eb37ab9aa5661e90d1/src/java/org/apache/hadoop/conf/Configuration.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/conf/Configuration.java?ref=6802695868b11e9a403384eb37ab9aa5661e90d1",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/conf/Configuration.java",
                "patch": "@@ -1620,6 +1620,7 @@ private synchronized Document asXmlDocument() throws IOException {\n     Element conf = doc.createElement(\"configuration\");\n     doc.appendChild(conf);\n     conf.appendChild(doc.createTextNode(\"\\n\"));\n+    getProps(); // ensure properties is set\n     for (Enumeration e = properties.keys(); e.hasMoreElements();) {\n       String name = (String)e.nextElement();\n       Object object = properties.get(name);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6802695868b11e9a403384eb37ab9aa5661e90d1/src/java/org/apache/hadoop/conf/Configuration.java",
                "sha": "c05acf36208020f5eca5fee3f4573c6e2292ace9",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6802695868b11e9a403384eb37ab9aa5661e90d1/src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/core/org/apache/hadoop/conf/TestConfiguration.java?ref=6802695868b11e9a403384eb37ab9aa5661e90d1",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.conf;\n \n import java.io.BufferedWriter;\n+import java.io.ByteArrayOutputStream;\n import java.io.File;\n import java.io.FileWriter;\n import java.io.IOException;\n@@ -255,6 +256,16 @@ public void testToString() throws IOException {\n     assertEquals(expectedOutput, conf.toString());\n   }\n   \n+  public void testWriteXml() throws IOException {\n+    Configuration conf = new Configuration();\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(); \n+    conf.writeXml(baos);\n+    String result = baos.toString();\n+    assertTrue(\"Result has proper header\", result.startsWith(\n+        \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\" standalone=\\\"no\\\"?><configuration>\"));\n+    assertTrue(\"Result has proper footer\", result.endsWith(\"</configuration>\"));\n+  }\n+  \n   public void testIncludes() throws Exception {\n     tearDown();\n     System.out.println(\"XXX testIncludes\");",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6802695868b11e9a403384eb37ab9aa5661e90d1/src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "sha": "fc9deef34ff7c4cc1cf51427257e6731420180b4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7118. Fix NPE in Configuration.writeXml. Contributed by Todd Lipcon\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1063613 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d0f318899ac44fc592900b184f6d59acae4ef74c",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestConfiguration.java"
        ]
    },
    "hadoop-common_6bc32f2": {
        "bug_id": "hadoop-common_6bc32f2",
        "commit": "https://github.com/apache/hadoop-common/commit/6bc32f2f31f21e86e408e2361a0962f4043411bf",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6bc32f2f31f21e86e408e2361a0962f4043411bf/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=6bc32f2f31f21e86e408e2361a0962f4043411bf",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1697,6 +1697,9 @@ Release 0.23.0 - Unreleased\n \n     MAPREDUCE-2693. Fix NPE in job-blacklisting. (Hitesh Shah via acmurthy) \n \n+    MAPREDUCE-3208. Fix NPE task/container log appenders. (liangzhwa via\n+    acmurthy) \n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6bc32f2f31f21e86e408e2361a0962f4043411bf/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "cf1c7d1696a3da0f1ddf516536a82303b8f196ac",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6bc32f2f31f21e86e408e2361a0962f4043411bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java?ref=6bc32f2f31f21e86e408e2361a0962f4043411bf",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java",
                "patch": "@@ -93,7 +93,9 @@ public void append(LoggingEvent event) {\n   }\n   \n   public void flush() {\n-    qw.flush();\n+    if (qw != null) {\n+      qw.flush();\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6bc32f2f31f21e86e408e2361a0962f4043411bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java",
                "sha": "0b79837f62d57a93f88a1b45a3ef9795a2ee0f0a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6bc32f2f31f21e86e408e2361a0962f4043411bf/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java?ref=6bc32f2f31f21e86e408e2361a0962f4043411bf",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java",
                "patch": "@@ -65,7 +65,9 @@ public void append(LoggingEvent event) {\n   }\n   \n   public void flush() {\n-    qw.flush();\n+    if (qw != null) {\n+      qw.flush();\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6bc32f2f31f21e86e408e2361a0962f4043411bf/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java",
                "sha": "7f09c175b6b234a8624d6a04433ef23345393fb9",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3208. Fix NPE task/container log appenders. Contributed by liangzhwa.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186542 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/67055a5c86a325cca505a0a53e3f0d04d80378ab",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestTaskLogAppender.java"
        ]
    },
    "hadoop-common_6c1e091": {
        "bug_id": "hadoop-common_6c1e091",
        "commit": "https://github.com/apache/hadoop-common/commit/6c1e091d998ed6980226bc2ddbe095d44e56a5fb",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=6c1e091d998ed6980226bc2ddbe095d44e56a5fb",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -280,6 +280,8 @@ Release 2.4.0 - UNRELEASED\n     MAPREDUCE-5724. JobHistoryServer does not start if HDFS is not running. \n     (tucu)\n \n+    MAPREDUCE-5729. mapred job -list throws NPE (kasha)\n+\n Release 2.3.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "88b0ff8ec1ae84287873f6f5ba86abadc8e0a6ed",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java?ref=6c1e091d998ed6980226bc2ddbe095d44e56a5fb",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "patch": "@@ -43,6 +43,7 @@\n import org.apache.hadoop.mapreduce.v2.util.MRApps;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ApplicationReport;\n+import org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport;\n import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.NodeReport;\n import org.apache.hadoop.yarn.api.records.QueueACL;\n@@ -445,11 +446,18 @@ public static JobStatus fromYarn(ApplicationReport application,\n     jobStatus.setStartTime(application.getStartTime());\n     jobStatus.setFinishTime(application.getFinishTime());\n     jobStatus.setFailureInfo(application.getDiagnostics());\n-    jobStatus.setNeededMem(application.getApplicationResourceUsageReport().getNeededResources().getMemory());\n-    jobStatus.setNumReservedSlots(application.getApplicationResourceUsageReport().getNumReservedContainers());\n-    jobStatus.setNumUsedSlots(application.getApplicationResourceUsageReport().getNumUsedContainers());\n-    jobStatus.setReservedMem(application.getApplicationResourceUsageReport().getReservedResources().getMemory());\n-    jobStatus.setUsedMem(application.getApplicationResourceUsageReport().getUsedResources().getMemory());\n+    ApplicationResourceUsageReport resourceUsageReport =\n+        application.getApplicationResourceUsageReport();\n+    if (resourceUsageReport != null) {\n+      jobStatus.setNeededMem(\n+          resourceUsageReport.getNeededResources().getMemory());\n+      jobStatus.setNumReservedSlots(\n+          resourceUsageReport.getNumReservedContainers());\n+      jobStatus.setNumUsedSlots(resourceUsageReport.getNumUsedContainers());\n+      jobStatus.setReservedMem(\n+          resourceUsageReport.getReservedResources().getMemory());\n+      jobStatus.setUsedMem(resourceUsageReport.getUsedResources().getMemory());\n+    }\n     return jobStatus;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "sha": "6b4aa4ed1e40a57ce9558d146e02b40b60bc35ea",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java?ref=6c1e091d998ed6980226bc2ddbe095d44e56a5fb",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "patch": "@@ -23,8 +23,6 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import junit.framework.Assert;\n-\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.mapreduce.JobStatus.State;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n@@ -40,6 +38,7 @@\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n import org.apache.hadoop.yarn.util.Records;\n+import org.junit.Assert;\n import org.junit.Test;\n import org.mockito.Mockito;\n \n@@ -112,6 +111,14 @@ public void testFromYarnApplicationReport() {\n     when(mockReport.getUser()).thenReturn(\"dummy-user\");\n     when(mockReport.getQueue()).thenReturn(\"dummy-queue\");\n     String jobFile = \"dummy-path/job.xml\";\n+\n+    try {\n+      JobStatus status = TypeConverter.fromYarn(mockReport, jobFile);\n+    } catch (NullPointerException npe) {\n+      Assert.fail(\"Type converstion from YARN fails for jobs without \" +\n+          \"ApplicationUsageReport\");\n+    }\n+\n     ApplicationResourceUsageReport appUsageRpt = Records\n         .newRecord(ApplicationResourceUsageReport.class);\n     Resource r = Records.newRecord(Resource.class);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "sha": "cc42b9c220f4b7704379054d7d90025117aed5d5",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-5729. mapred job -list throws NPE (kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1559811 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/69419b49d6740ffa46ddace7b4ee6f09205e77ae",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestTypeConverter.java"
        ]
    },
    "hadoop-common_6f01132": {
        "bug_id": "hadoop-common_6f01132",
        "commit": "https://github.com/apache/hadoop-common/commit/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt?ref=6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "patch": "@@ -51,3 +51,6 @@ fs-encryption (Unreleased)\n   BUG FIXES\n \n     HADOOP-10871. incorrect prototype in OpensslSecureRandom.c (cmccabe)\n+\n+    HADOOP-10886. CryptoCodec#getCodecclasses throws NPE when configurations not \n+    loaded. (umamahesh)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "sha": "498307e6bc7804db4ecdbc177b6561be579f7195",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java?ref=6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "patch": "@@ -45,14 +45,21 @@\n   \n   /**\n    * Get crypto codec for specified algorithm/mode/padding.\n-   * @param conf the configuration\n-   * @param CipherSuite algorithm/mode/padding\n-   * @return CryptoCodec the codec object\n+   * \n+   * @param conf\n+   *          the configuration\n+   * @param CipherSuite\n+   *          algorithm/mode/padding\n+   * @return CryptoCodec the codec object. Null value will be returned if no\n+   *         crypto codec classes with cipher suite configured.\n    */\n   public static CryptoCodec getInstance(Configuration conf, \n       CipherSuite cipherSuite) {\n     List<Class<? extends CryptoCodec>> klasses = getCodecClasses(\n         conf, cipherSuite);\n+    if (klasses == null) {\n+      return null;\n+    }\n     CryptoCodec codec = null;\n     for (Class<? extends CryptoCodec> klass : klasses) {\n       try {\n@@ -80,10 +87,13 @@ public static CryptoCodec getInstance(Configuration conf,\n   }\n   \n   /**\n-   * Get crypto codec for algorithm/mode/padding in config value \n+   * Get crypto codec for algorithm/mode/padding in config value\n    * hadoop.security.crypto.cipher.suite\n-   * @param conf the configuration\n-   * @return CryptoCodec the codec object\n+   * \n+   * @param conf\n+   *          the configuration\n+   * @return CryptoCodec the codec object Null value will be returned if no\n+   *         crypto codec classes with cipher suite configured.\n    */\n   public static CryptoCodec getInstance(Configuration conf) {\n     String name = conf.get(HADOOP_SECURITY_CRYPTO_CIPHER_SUITE_KEY, \n@@ -97,6 +107,10 @@ public static CryptoCodec getInstance(Configuration conf) {\n     String configName = HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX + \n         cipherSuite.getConfigSuffix();\n     String codecString = conf.get(configName);\n+    if (codecString == null) {\n+      LOG.warn(\"No crypto codec classes with cipher suite configured.\");\n+      return null;\n+    }\n     for (String c : Splitter.on(',').trimResults().omitEmptyStrings().\n         split(codecString)) {\n       try {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "sha": "9de7f95200f5f44e440619269fbc2ae1135f599d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java?ref=6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "patch": "@@ -25,6 +25,7 @@\n import java.io.OutputStream;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.LocalFileSystem;\n@@ -50,6 +51,11 @@ public static void init() throws Exception {\n     conf = new Configuration(false);\n     conf.set(\"fs.file.impl\", LocalFileSystem.class.getName());\n     fileSys = FileSystem.getLocal(conf);\n+    conf.set(\n+        CommonConfigurationKeysPublic.HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX\n+            + CipherSuite.AES_CTR_NOPADDING.getConfigSuffix(),\n+        OpensslAesCtrCryptoCodec.class.getName() + \",\"\n+            + JceAesCtrCryptoCodec.class.getName());\n     codec = CryptoCodec.getInstance(conf);\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "sha": "765a364faa6d5cffff9bd1a811caf51bc51b0cc5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java?ref=6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "patch": "@@ -598,7 +598,9 @@ public DFSClient(URI nameNodeUri, ClientProtocol rpcNamenode,\n         DFSUtil.getRandom().nextInt()  + \"_\" + Thread.currentThread().getId();\n     this.codec = CryptoCodec.getInstance(conf);\n     this.cipherSuites = Lists.newArrayListWithCapacity(1);\n-    cipherSuites.add(codec.getCipherSuite());\n+    if (codec != null) {\n+      cipherSuites.add(codec.getCipherSuite());\n+    }\n     provider = DFSUtil.createKeyProviderCryptoExtension(conf);\n     if (provider == null) {\n       LOG.info(\"No KeyProvider found.\");\n@@ -1333,9 +1335,12 @@ public HdfsDataInputStream createWrappedInputStream(DFSInputStream dfsis)\n     if (feInfo != null) {\n       // File is encrypted, wrap the stream in a crypto stream.\n       KeyVersion decrypted = decryptEncryptedDataEncryptionKey(feInfo);\n+      CryptoCodec codec = CryptoCodec\n+          .getInstance(conf, feInfo.getCipherSuite());\n+      Preconditions.checkNotNull(codec == null,\n+          \"No crypto codec classes with cipher suite configured.\");\n       final CryptoInputStream cryptoIn =\n-          new CryptoInputStream(dfsis, CryptoCodec.getInstance(conf, \n-              feInfo.getCipherSuite()), decrypted.getMaterial(),\n+          new CryptoInputStream(dfsis, codec, decrypted.getMaterial(),\n               feInfo.getIV());\n       return new HdfsDataInputStream(cryptoIn);\n     } else {\n@@ -1361,6 +1366,8 @@ public HdfsDataOutputStream createWrappedOutputStream(DFSOutputStream dfsos,\n       FileSystem.Statistics statistics, long startPos) throws IOException {\n     final FileEncryptionInfo feInfo = dfsos.getFileEncryptionInfo();\n     if (feInfo != null) {\n+      Preconditions.checkNotNull(codec == null,\n+          \"No crypto codec classes with cipher suite configured.\");\n       // File is encrypted, wrap the stream in a crypto stream.\n       KeyVersion decrypted = decryptEncryptedDataEncryptionKey(feInfo);\n       final CryptoOutputStream cryptoOut =",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "sha": "d3532607c84b80f9588043f78c1202a625bc1db9",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java?ref=6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "patch": "@@ -38,7 +38,6 @@\n import java.util.EnumSet;\n import java.util.List;\n import java.util.Random;\n-import java.util.concurrent.CancellationException;\n \n import org.apache.commons.lang.ArrayUtils;\n import org.apache.commons.logging.impl.Log4JLogger;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "sha": "b71cc32fb80e93c51153e29045cbaa020c1bf461",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10886. CryptoCodec#getCodecclasses throws NPE when configurations not loaded. Contributed by Uma Maheswara Rao G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1615523 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/e2140fb7ac61ed23b1e4974d8e4a4c0b89074520",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestCryptoCodec.java"
        ]
    },
    "hadoop-common_740bc98": {
        "bug_id": "hadoop-common_740bc98",
        "commit": "https://github.com/apache/hadoop-common/commit/740bc9868518a3e9d98f1b4c5da37772c481b71f",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=740bc9868518a3e9d98f1b4c5da37772c481b71f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -222,6 +222,9 @@ Release 2.4.1 - UNRELEASED\n     YARN-1957. Consider the max capacity of the queue when computing the ideal\n     capacity for preemption. (Carlo Curino via cdouglas)\n \n+    YARN-1986. In Fifo Scheduler, node heartbeat in between creating app and\n+    attempt causes NPE (Hong Zhiguo via Sandy Ryza)\n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/CHANGES.txt",
                "sha": "d82cd482a1fb2a3384b3b6a8bfc544f343ee50ae",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java?ref=740bc9868518a3e9d98f1b4c5da37772c481b71f",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "patch": "@@ -360,7 +360,8 @@ private FiCaSchedulerNode getNode(NodeId nodeId) {\n     return nodes.get(nodeId);\n   }\n \n-  private synchronized void addApplication(ApplicationId applicationId,\n+  @VisibleForTesting\n+  public synchronized void addApplication(ApplicationId applicationId,\n       String queue, String user) {\n     SchedulerApplication application =\n         new SchedulerApplication(DEFAULT_QUEUE, user);\n@@ -372,7 +373,8 @@ private synchronized void addApplication(ApplicationId applicationId,\n         .handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n   }\n \n-  private synchronized void\n+  @VisibleForTesting\n+  public synchronized void\n       addApplicationAttempt(ApplicationAttemptId appAttemptId,\n           boolean transferStateFromPreviousAttempt) {\n     SchedulerApplication application =\n@@ -458,6 +460,9 @@ private void assignContainers(FiCaSchedulerNode node) {\n         .entrySet()) {\n       FiCaSchedulerApp application =\n           (FiCaSchedulerApp) e.getValue().getCurrentAppAttempt();\n+      if (application == null) {\n+        continue;\n+      }\n       LOG.debug(\"pre-assignContainers\");\n       application.showRequests();\n       synchronized (application) {\n@@ -497,6 +502,9 @@ private void assignContainers(FiCaSchedulerNode node) {\n     for (SchedulerApplication application : applications.values()) {\n       FiCaSchedulerApp attempt =\n           (FiCaSchedulerApp) application.getCurrentAppAttempt();\n+      if (attempt == null) {\n+        continue;\n+      }\n       attempt.setHeadroom(Resources.subtract(clusterResource, usedResource));\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "sha": "21fcdecf4f9ef9fa2ee555996ba746252012e656",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop-common/blob/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java?ref=740bc9868518a3e9d98f1b4c5da37772c481b71f",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "patch": "@@ -52,6 +52,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n+import org.apache.hadoop.yarn.util.resource.Resources;\n import org.apache.log4j.Level;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n@@ -66,7 +67,7 @@\n   \n   private final int GB = 1024;\n   private static YarnConfiguration conf;\n-  \n+\n   @BeforeClass\n   public static void setup() {\n     conf = new YarnConfiguration();\n@@ -213,6 +214,32 @@ public void test() throws Exception {\n     rm.stop();\n   }\n \n+  @Test\n+  public void testNodeUpdateBeforeAppAttemptInit() throws Exception {\n+    FifoScheduler scheduler = new FifoScheduler();\n+    MockRM rm = new MockRM(conf);\n+    scheduler.reinitialize(conf, rm.getRMContext());\n+\n+    RMNode node = MockNodes.newNodeInfo(1,\n+            Resources.createResource(1024, 4), 1, \"127.0.0.1\");\n+    scheduler.handle(new NodeAddedSchedulerEvent(node));\n+\n+    ApplicationId appId = ApplicationId.newInstance(0, 1);\n+    scheduler.addApplication(appId, \"queue1\", \"user1\");\n+\n+    NodeUpdateSchedulerEvent updateEvent = new NodeUpdateSchedulerEvent(node);\n+    try {\n+      scheduler.handle(updateEvent);\n+    } catch (NullPointerException e) {\n+        Assert.fail();\n+    }\n+\n+    ApplicationAttemptId attId = ApplicationAttemptId.newInstance(appId, 1);\n+    scheduler.addApplicationAttempt(attId, false);\n+\n+    rm.stop();\n+  }\n+\n   private void testMinimumAllocation(YarnConfiguration conf, int testAlloc)\n       throws Exception {\n     MockRM rm = new MockRM(conf);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "sha": "fcd5041e4257d4cc1ce64ae90278aba09576f14d",
                "status": "modified"
            }
        ],
        "message": "YARN-1986. In Fifo Scheduler, node heartbeat in between creating app and attempt causes NPE (Hong Zhiguo via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594476 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ebaafb216fa02a4f3640357ce1c7cfddac5fcbae",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFifoScheduler.java"
        ]
    },
    "hadoop-common_755572a": {
        "bug_id": "hadoop-common_755572a",
        "commit": "https://github.com/apache/hadoop-common/commit/755572a81a5ff1eeef41242abb050124b6146533",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/755572a81a5ff1eeef41242abb050124b6146533/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=755572a81a5ff1eeef41242abb050124b6146533",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1833,6 +1833,8 @@ Release 0.23.0 - Unreleased\n     MAPREDUCE-3258. Fixed AM & JobHistory web-ui to display counters properly.\n     (Siddharth Seth via acmurthy)\n \n+    MAPREDUCE-3290. Fixed a NPE in ClientRMService. (acmurthy) \n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/755572a81a5ff1eeef41242abb050124b6146533/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "c9d61a6ec79ccd272013d739e90575c699f2c4a3",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop-common/blob/755572a81a5ff1eeef41242abb050124b6146533/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java?ref=755572a81a5ff1eeef41242abb050124b6146533",
                "deletions": 4,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "patch": "@@ -59,6 +59,7 @@\n import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;\n import org.apache.hadoop.yarn.api.records.NodeReport;\n import org.apache.hadoop.yarn.api.records.QueueInfo;\n+import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.YarnClusterMetrics;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.YarnRemoteException;\n@@ -67,10 +68,12 @@\n import org.apache.hadoop.yarn.ipc.RPCUtil;\n import org.apache.hadoop.yarn.ipc.YarnRPC;\n import org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger.AuditConstants;\n+import org.apache.hadoop.yarn.server.resourcemanager.resource.Resources;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.security.authorize.RMPolicyProvider;\n import org.apache.hadoop.yarn.server.security.ApplicationACLsManager;\n@@ -396,10 +399,18 @@ private NodeReport createNodeReports(RMNode rmNode) {\n     report.setRackName(rmNode.getRackName());\n     report.setCapability(rmNode.getTotalCapability());\n     report.setNodeHealthStatus(rmNode.getNodeHealthStatus());\n-    org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport schedulerNodeReport = scheduler\n-        .getNodeReport(rmNode.getNodeID());\n-    report.setUsed(schedulerNodeReport.getUsedResource());\n-    report.setNumContainers(schedulerNodeReport.getNumContainers());\n+    \n+    SchedulerNodeReport schedulerNodeReport = \n+        scheduler.getNodeReport(rmNode.getNodeID());\n+    Resource used = Resources.none();\n+    int numContainers = 0;\n+    if (schedulerNodeReport != null) {\n+      used = schedulerNodeReport.getUsedResource();\n+      numContainers = schedulerNodeReport.getNumContainers();\n+    } \n+    report.setUsed(used);\n+    report.setNumContainers(numContainers);\n+\n     return report;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/755572a81a5ff1eeef41242abb050124b6146533/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "sha": "b19c1c17c5e6656c934d4d92ceb72cd6120b6042",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3290. Fixed a NPE in ClientRMService.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190162 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/38f94704dfb6ecde5856d4027b82dfe2f825fe9d",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestClientRMService.java"
        ]
    },
    "hadoop-common_770463b": {
        "bug_id": "hadoop-common_770463b",
        "commit": "https://github.com/apache/hadoop-common/commit/770463b3df0bacea4f598f7c73a3d976a7a3b937",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "patch": "@@ -45,3 +45,5 @@ IMPROVEMENTS:\n \n     HDFS-5390. Send one incremental block report per storage directory.\n     (Arpit Agarwal)\n+\n+    HDFS-5401. Fix NPE in Directory Scanner. (Arpit Agarwal)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "sha": "cd139d4845e203e0125d72371d510352118fa6bc",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n+import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.Block;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n@@ -180,10 +181,11 @@ public String toString() {\n     }\n   }\n   \n-  void reportBadBlocks(ExtendedBlock block) {\n+  void reportBadBlocks(ExtendedBlock block,\n+                       String storageUuid, StorageType storageType) {\n     checkBlock(block);\n     for (BPServiceActor actor : bpServices) {\n-      actor.reportBadBlocks(block);\n+      actor.reportBadBlocks(block, storageUuid, storageType);\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "sha": "5d584616df335258b426878141001405130adb92",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.hdfs.DFSUtil;\n+import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n@@ -237,12 +238,18 @@ void scheduleBlockReport(long delay) {\n     resetBlockReportTime = true; // reset future BRs for randomness\n   }\n \n-  void reportBadBlocks(ExtendedBlock block) {\n+  void reportBadBlocks(ExtendedBlock block,\n+      String storageUuid, StorageType storageType) {\n     if (bpRegistration == null) {\n       return;\n     }\n     DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n-    LocatedBlock[] blocks = { new LocatedBlock(block, dnArr) }; \n+    String[] uuids = { storageUuid };\n+    StorageType[] types = { storageType };\n+    // TODO: Corrupt flag is set to false for compatibility. We can probably\n+    // set it to true here.\n+    LocatedBlock[] blocks = {\n+        new LocatedBlock(block, dnArr, uuids, types, -1, false) };\n     \n     try {\n       bpNamenode.reportBadBlocks(blocks);  ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "sha": "172fb0fc30ec6cd3d5fdbe7dc2459e8c7d25cf48",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "patch": "@@ -559,7 +559,9 @@ public void notifyNamenodeDeletedBlock(ExtendedBlock block, String storageUuid)\n    */\n   public void reportBadBlocks(ExtendedBlock block) throws IOException{\n     BPOfferService bpos = getBPOSForBlock(block);\n-    bpos.reportBadBlocks(block);\n+    FsVolumeSpi volume = getFSDataset().getVolume(block);\n+    bpos.reportBadBlocks(\n+        block, volume.getStorageID(), volume.getStorageType());\n   }\n \n   /**\n@@ -1265,8 +1267,10 @@ private void transferBlock(ExtendedBlock block, DatanodeInfo xferTargets[])\n     // Check if NN recorded length matches on-disk length \n     long onDiskLength = data.getLength(block);\n     if (block.getNumBytes() > onDiskLength) {\n+      FsVolumeSpi volume = getFSDataset().getVolume(block);\n       // Shorter on-disk len indicates corruption so report NN the corrupt block\n-      bpos.reportBadBlocks(block);\n+      bpos.reportBadBlocks(\n+          block, volume.getStorageID(), volume.getStorageType());\n       LOG.warn(\"Can't replicate block \" + block\n           + \" because on-disk length \" + onDiskLength \n           + \" is shorter than NameNode recorded length \" + block.getNumBytes());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "sha": "318d2f3705a05cad964d1b9b68ed52b7f4f3237a",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 16,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -198,7 +198,9 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)\n   //                 two maps. This might require some refactoring\n   //                 rewrite of FsDatasetImpl.\n   final ReplicaMap volumeMap;\n-  final Map<FsVolumeImpl, ReplicaMap> perVolumeReplicaMap;\n+\n+  // Map from StorageID to ReplicaMap.\n+  final Map<String, ReplicaMap> perVolumeReplicaMap;\n \n \n   // Used for synchronizing access to usage stats\n@@ -249,7 +251,7 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)\n       LOG.info(\"Added volume - \" + dir + \", StorageType: \" + storageType);\n     }\n     volumeMap = new ReplicaMap(this);\n-    perVolumeReplicaMap = new HashMap<FsVolumeImpl, ReplicaMap>();\n+    perVolumeReplicaMap = new HashMap<String, ReplicaMap>();\n \n     @SuppressWarnings(\"unchecked\")\n     final VolumeChoosingPolicy<FsVolumeImpl> blockChooserImpl =\n@@ -628,7 +630,7 @@ private synchronized ReplicaBeingWritten append(String bpid,\n     \n     // Replace finalized replica by a RBW replica in replicas map\n     volumeMap.add(bpid, newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(bpid, newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(bpid, newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n@@ -759,7 +761,7 @@ public synchronized ReplicaInPipeline createRbw(ExtendedBlock b)\n     ReplicaBeingWritten newReplicaInfo = new ReplicaBeingWritten(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     return newReplicaInfo;\n   }\n   \n@@ -878,7 +880,7 @@ public synchronized ReplicaInPipeline convertTemporaryToRbw(\n     rbw.setBytesAcked(visible);\n     // overwrite the RBW in the volume map\n     volumeMap.add(b.getBlockPoolId(), rbw);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), rbw);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), rbw);\n     return rbw;\n   }\n \n@@ -898,7 +900,7 @@ public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n     ReplicaInPipeline newReplicaInfo = new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n@@ -967,7 +969,8 @@ private synchronized FinalizedReplica finalizeReplica(String bpid,\n       newReplicaInfo = new FinalizedReplica(replicaInfo, v, dest.getParentFile());\n     }\n     volumeMap.add(bpid, newReplicaInfo);\n-    perVolumeReplicaMap.get(newReplicaInfo.getVolume()).add(bpid, newReplicaInfo);\n+    perVolumeReplicaMap.get(newReplicaInfo.getVolume().getStorageID())\n+        .add(bpid, newReplicaInfo);\n     return newReplicaInfo;\n   }\n \n@@ -981,7 +984,7 @@ public synchronized void unfinalizeBlock(ExtendedBlock b) throws IOException {\n     if (replicaInfo != null && replicaInfo.getState() == ReplicaState.TEMPORARY) {\n       // remove from volumeMap\n       volumeMap.remove(b.getBlockPoolId(), b.getLocalBlock());\n-      perVolumeReplicaMap.get((FsVolumeImpl) replicaInfo.getVolume())\n+      perVolumeReplicaMap.get(replicaInfo.getVolume().getStorageID())\n           .remove(b.getBlockPoolId(), b.getLocalBlock());\n       \n       // delete the on-disk temp file\n@@ -1064,7 +1067,7 @@ public BlockListAsLongs getBlockReport(String bpid) {\n         new HashMap<String, BlockListAsLongs>();\n \n     for (FsVolumeImpl v : getVolumes()) {\n-      ReplicaMap rMap = perVolumeReplicaMap.get(v);\n+      ReplicaMap rMap = perVolumeReplicaMap.get(v.getStorageID());\n       BlockListAsLongs blockList = getBlockReportWithReplicaMap(bpid, rMap);\n       blockReportMap.put(v.getStorageID(), blockList);\n     }\n@@ -1212,7 +1215,7 @@ public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n-        perVolumeReplicaMap.get(v).remove(bpid, invalidBlks[i]);\n+        perVolumeReplicaMap.get(v.getStorageID()).remove(bpid, invalidBlks[i]);\n       }\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n@@ -1274,7 +1277,8 @@ public void checkDataDir() throws DiskErrorException {\n               LOG.warn(\"Removing replica \" + bpid + \":\" + b.getBlockId()\n                   + \" on failed volume \" + fv.getCurrentDir().getAbsolutePath());\n               ib.remove();\n-              perVolumeReplicaMap.get(fv).remove(bpid, b.getBlockId());\n+              perVolumeReplicaMap.get(fv.getStorageID())\n+                  .remove(bpid, b.getBlockId());\n               removedBlocks++;\n             }\n           }\n@@ -1391,8 +1395,7 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n           // Block is in memory and not on the disk\n           // Remove the block from volumeMap\n           volumeMap.remove(bpid, blockId);\n-          perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume())\n-              .remove(bpid, blockId);\n+          perVolumeReplicaMap.get(vol.getStorageID()).remove(bpid, blockId);\n           final DataBlockScanner blockScanner = datanode.getBlockScanner();\n           if (blockScanner != null) {\n             blockScanner.deleteBlock(bpid, new Block(blockId));\n@@ -1416,8 +1419,8 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n         ReplicaInfo diskBlockInfo = new FinalizedReplica(blockId, \n             diskFile.length(), diskGS, vol, diskFile.getParentFile());\n         volumeMap.add(bpid, diskBlockInfo);\n-        perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume()).\n-            remove(bpid, diskBlockInfo);\n+        perVolumeReplicaMap.get(vol.getStorageID())\n+            .remove(bpid, diskBlockInfo);\n         final DataBlockScanner blockScanner = datanode.getBlockScanner();\n         if (blockScanner != null) {\n           blockScanner.addBlock(new ExtendedBlock(bpid, diskBlockInfo));\n@@ -1695,7 +1698,7 @@ public synchronized void addBlockPool(String bpid, Configuration conf)\n \n     // TODO: Avoid the double scan.\n     for (FsVolumeImpl v : getVolumes()) {\n-      ReplicaMap rMap = perVolumeReplicaMap.get(v);\n+      ReplicaMap rMap = perVolumeReplicaMap.get(v.getStorageID());\n       rMap.initBlockPool(bpid);\n       volumes.getVolumeMap(bpid, v, rMap);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "9077c40a8367a9f4bdddc10e439a1e95920ef481",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "patch": "@@ -90,13 +90,13 @@ long getRemaining() throws IOException {\n     return remaining;\n   }\n     \n-  void initializeReplicaMaps(Map<FsVolumeImpl, ReplicaMap> perVolumeReplicaMap,\n+  void initializeReplicaMaps(Map<String, ReplicaMap> perVolumeReplicaMap,\n                              ReplicaMap globalReplicaMap,\n                              Object mutex) throws IOException {\n     for (FsVolumeImpl v : volumes) {\n       ReplicaMap rMap = new ReplicaMap(mutex);\n       v.getVolumeMap(rMap);\n-      perVolumeReplicaMap.put(v, rMap);\n+      perVolumeReplicaMap.put(v.getStorageID(), rMap);\n       globalReplicaMap.addAll(rMap);\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "sha": "671996718be2eea6db9b076cf8a5aed4de36705e",
                "status": "modified"
            }
        ],
        "message": "HDFS-5401. Fix NPE in Directory Scanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1535158 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/5d8a7875a3329c95d738f4b97001182d8235d79a",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBPOfferService.java",
            "TestFsDatasetImpl.java"
        ]
    },
    "hadoop-common_77cf4ed": {
        "bug_id": "hadoop-common_77cf4ed",
        "commit": "https://github.com/apache/hadoop-common/commit/77cf4edee832532c1ab043a45b8ee94c7ba20bb1",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=77cf4edee832532c1ab043a45b8ee94c7ba20bb1",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -131,3 +131,5 @@ HDFS-2804. Should not mark blocks under-replicated when exiting safemode (todd)\n HDFS-2807. Service level authorizartion for HAServiceProtocol. (jitendra)\n \n HDFS-2809. Add test to verify that delegation tokens are honored after failover. (jitendra and atm)\n+\n+HDFS-2838. NPE in FSNamesystem when in safe mode. (Gregory Chanan via eli)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "c8a760336a9c9c95f075c4e2d894747b31d6207e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=77cf4edee832532c1ab043a45b8ee94c7ba20bb1",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3623,11 +3623,10 @@ private void doConsistencyCheck() {\n       assert assertsOn = true; // set to true if asserts are on\n       if (!assertsOn) return;\n       \n-      \n-      int activeBlocks = blockManager.getActiveBlockCount();\n       if (blockTotal == -1 && blockSafe == -1) {\n         return; // manual safe mode\n       }\n+      int activeBlocks = blockManager.getActiveBlockCount();\n       if ((blockTotal != activeBlocks) &&\n           !(blockSafe >= 0 && blockSafe <= blockTotal)) {\n         throw new AssertionError(",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "b3b3dbdaf31bcb28f1c91d0cbdd4949a93c8f0fa",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java?ref=77cf4edee832532c1ab043a45b8ee94c7ba20bb1",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import junit.framework.Assert;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.protocol.FSConstants;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n@@ -37,6 +38,7 @@\n   private static final String CLUSTER_1 = \"cluster1\";\n   private static final String CLUSTER_2 = \"cluster2\";\n   private static final String CLUSTER_3 = \"cluster3\";\n+  private static final String CLUSTER_4 = \"cluster4\";\n   protected String testDataPath;\n   protected File testDataDir;\n   @Before\n@@ -104,5 +106,21 @@ public void testDualClusters() throws Throwable {\n     }\n   }\n \n-\n+  @Test(timeout=100000)\n+  public void testIsClusterUpAfterShutdown() throws Throwable {\n+    Configuration conf = new HdfsConfiguration();\n+    File testDataCluster4 = new File(testDataPath, CLUSTER_4);\n+    String c4Path = testDataCluster4.getAbsolutePath();\n+    conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, c4Path);\n+    MiniDFSCluster cluster4 = new MiniDFSCluster.Builder(conf).build();\n+    try {\n+      DistributedFileSystem dfs = (DistributedFileSystem) cluster4.getFileSystem();\n+      dfs.setSafeMode(FSConstants.SafeModeAction.SAFEMODE_ENTER);\n+      cluster4.shutdown();\n+    } finally {\n+      while(cluster4.isClusterUp()){\n+        Thread.sleep(1000);\n+      }  \n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "sha": "0eec0d187746184179ed9f814d59dbd059c390e0",
                "status": "modified"
            }
        ],
        "message": "HDFS-2838. NPE in FSNamesystem when in safe mode. Contributed by Gregory Chanan\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1236450 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/83d2cbf7007e415f3c3a76153242858e89b33ed2",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop-common_77fb965": {
        "bug_id": "hadoop-common_77fb965",
        "commit": "https://github.com/apache/hadoop-common/commit/77fb965df780c375f6279f11dc9a902c1ff1380e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=77fb965df780c375f6279f11dc9a902c1ff1380e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -144,6 +144,9 @@ Release 0.23.1 - Unreleased\n     MAPREDUCE-3369. Migrate MR1 tests to run on MR2 using the new interfaces\n     introduced in MAPREDUCE-3169. (Ahmed Radwan via tomwhite)\n \n+    MAPREDUCE-3518. mapred queue -info <queue> -showJobs throws NPE. \n+    (Jonathan Eagles via mahadev)\n+\n   OPTIMIZATIONS\n \n   BUG FIXES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "f53ffec718a02c41aca8181ac32600b7c013dd30",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java?ref=77fb965df780c375f6279f11dc9a902c1ff1380e",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -432,7 +432,6 @@ public String getFailureInfo() throws IOException {\n \n   }\n \n-  Cluster cluster;\n   /**\n    * Ugi of the client. We store this ugi when the client is created and \n    * then make sure that the same ugi is used to run the various protocols.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "fa3d799fe70d60bb3247edc940ff2f9d5f1d953e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java?ref=77fb965df780c375f6279f11dc9a902c1ff1380e",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "patch": "@@ -55,7 +55,7 @@\n @InterfaceStability.Stable\n public class CLI extends Configured implements Tool {\n   private static final Log LOG = LogFactory.getLog(CLI.class);\n-  private Cluster cluster;\n+  protected Cluster cluster;\n \n   public CLI() {\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "sha": "f7ac9c40a6a2074a6602bc8d12548d8c403682c0",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "changes": 81,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java?ref=77fb965df780c375f6279f11dc9a902c1ff1380e",
                "deletions": 8,
                "filename": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "patch": "@@ -19,21 +19,41 @@\n package org.apache.hadoop.mapred;\n \n import static org.junit.Assert.assertEquals;\n+import static org.mockito.Matchers.isA;\n+import static org.mockito.Mockito.atLeastOnce;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n+import java.io.IOException;\n+import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapreduce.Cluster;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobPriority;\n+import org.apache.hadoop.mapreduce.JobStatus;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.hadoop.mapreduce.TaskReport;\n import org.junit.Test;\n \n public class JobClientUnitTest {\n   \n+  public class TestJobClient extends JobClient {\n+\n+    TestJobClient(JobConf jobConf) throws IOException {\n+      super(jobConf);\n+    }\n+\n+    void setCluster(Cluster cluster) {\n+      this.cluster = cluster;\n+    }\n+  }\n+\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testMapTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -47,9 +67,9 @@ public void testMapTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testReduceTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -63,9 +83,9 @@ public void testReduceTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testSetupTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -79,9 +99,9 @@ public void testSetupTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testCleanupTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -91,4 +111,49 @@ public void testCleanupTaskReportsWithNullJob() throws Exception {\n     \n     verify(mockCluster).getJob(id);\n   }\n+\n+  @Test\n+  public void testShowJob() throws Exception {\n+    TestJobClient client = new TestJobClient(new JobConf());\n+    JobID jobID = new JobID(\"test\", 0);\n+\n+    JobStatus mockJobStatus = mock(JobStatus.class);\n+    when(mockJobStatus.getJobID()).thenReturn(jobID);\n+    when(mockJobStatus.getState()).thenReturn(JobStatus.State.RUNNING);\n+    when(mockJobStatus.getStartTime()).thenReturn(0L);\n+    when(mockJobStatus.getUsername()).thenReturn(\"mockuser\");\n+    when(mockJobStatus.getQueue()).thenReturn(\"mockqueue\");\n+    when(mockJobStatus.getPriority()).thenReturn(JobPriority.NORMAL);\n+    when(mockJobStatus.getNumUsedSlots()).thenReturn(1);\n+    when(mockJobStatus.getNumReservedSlots()).thenReturn(1);\n+    when(mockJobStatus.getUsedMem()).thenReturn(1024);\n+    when(mockJobStatus.getReservedMem()).thenReturn(512);\n+    when(mockJobStatus.getNeededMem()).thenReturn(2048);\n+    when(mockJobStatus.getSchedulingInfo()).thenReturn(\"NA\");\n+\n+    Job mockJob = mock(Job.class);\n+    when(mockJob.getTaskReports(isA(TaskType.class))).thenReturn(new TaskReport[0]);\n+\n+    Cluster mockCluster = mock(Cluster.class);\n+    when(mockCluster.getJob(jobID)).thenReturn(mockJob);\n+\n+    client.setCluster(mockCluster);\n+    \n+    \n+    client.displayJobList(new JobStatus[] {mockJobStatus});\n+    verify(mockJobStatus, atLeastOnce()).getJobID();\n+    verify(mockJob, atLeastOnce()).getTaskReports(isA(TaskType.class));\n+    verify(mockCluster, atLeastOnce()).getJob(jobID);\n+    verify(mockJobStatus).getState();\n+    verify(mockJobStatus).getStartTime();\n+    verify(mockJobStatus).getUsername();\n+    verify(mockJobStatus).getQueue();\n+    verify(mockJobStatus).getPriority();\n+    verify(mockJobStatus).getNumUsedSlots();\n+    verify(mockJobStatus).getNumReservedSlots();\n+    verify(mockJobStatus).getUsedMem();\n+    verify(mockJobStatus).getReservedMem();\n+    verify(mockJobStatus).getNeededMem();\n+    verify(mockJobStatus).getSchedulingInfo();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "sha": "3f54e09a33d707a0e388239e7e2d8d9a0d274c07",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3518. mapred queue -info <queue> -showJobs throws NPE. (Jonathan Eagles via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213464 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ad3b9f14154660b24fa7f04d8f045c5e8fa4cc84",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobClient.java",
            "TestCLI.java"
        ]
    },
    "hadoop-common_796909b": {
        "bug_id": "hadoop-common_796909b",
        "commit": "https://github.com/apache/hadoop-common/commit/796909bd8462a11ee78c75625d7056fafefdb290",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=796909bd8462a11ee78c75625d7056fafefdb290",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -203,6 +203,10 @@ Release 0.23.2 - UNRELEASED\n     MAPREDUCE-3816. capacity scheduler web ui bar graphs for used capacity wrong\n     (tgraves via bobby)\n \n+    MAPREDUCE-3930. Fixed an NPE while accessing the AM page/webservice for a \n+    task attempt without an assigned container. (Robert Joseph Evans via\n+    sseth)\n+\n Release 0.23.1 - 2012-02-17\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "d293b52b3f7f2466e11cf730a8d4aa9ef58ad197",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java?ref=796909bd8462a11ee78c75625d7056fafefdb290",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "patch": "@@ -142,7 +142,7 @@ private static ApplicationId toApplicationId(\n   }\n \n   public static String toString(ContainerId cId) {\n-    return cId.toString();\n+    return cId == null ? null : cId.toString();\n   }\n \n   public static NodeId toNodeId(String nodeIdStr) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "sha": "21fe2d9874cf0223ea91b4fb1af96c13f306464c",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop-common/blob/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java?ref=796909bd8462a11ee78c75625d7056fafefdb290",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.net.URISyntaxException;\n \n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.URL;\n import org.junit.Test;\n \n@@ -35,4 +36,17 @@ public void testConvertUrlWithNoPort() throws URISyntaxException {\n     assertEquals(expectedPath, actualPath);\n   }\n \n+  @Test\n+  public void testContainerId() throws URISyntaxException {\n+    ContainerId id = BuilderUtils.newContainerId(0, 0, 0, 0);\n+    String cid = ConverterUtils.toString(id);\n+    assertEquals(\"container_0_0000_00_000000\", cid);\n+    ContainerId gen = ConverterUtils.toContainerId(cid);\n+    assertEquals(gen, id);\n+  }\n+\n+  @Test\n+  public void testContainerIdNull() throws URISyntaxException {\n+    assertNull(ConverterUtils.toString((ContainerId)null));\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "sha": "7924124bf40e7a3d6332a3ee5485a6a595bacfcd",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3930. Fixed an NPE while accessing the AM page/webservice for a task attempt without an assigned container. (Contributed by Robert Joseph Evans)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1294901 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/9ea38c42160269cb24d5823268f60f5f817ad9a7",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestConverterUtils.java"
        ]
    },
    "hadoop-common_7d2b717": {
        "bug_id": "hadoop-common_7d2b717",
        "commit": "https://github.com/apache/hadoop-common/commit/7d2b7171ff0dab08e6e918425afd2b985bb25206",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=7d2b7171ff0dab08e6e918425afd2b985bb25206",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -401,6 +401,9 @@ Release 2.1.1-beta - UNRELEASED\n     HDFS-5132. Deadlock in NameNode between SafeModeMonitor#run and \n     DatanodeManager#handleHeartbeat. (kihwal)\n \n+    HDFS-5077. NPE in FSNamesystem.commitBlockSynchronization().\n+    (Plamen Jeliazkov via shv)\n+\n Release 2.1.0-beta - 2013-08-22\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "84100e18c4dd693927880cc330ce7cad9758642e",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=7d2b7171ff0dab08e6e918425afd2b985bb25206",
                "deletions": 10,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -174,7 +174,6 @@\n import org.apache.hadoop.hdfs.server.namenode.INode.BlocksMapUpdateInfo;\n import org.apache.hadoop.hdfs.server.namenode.JournalSet.JournalAndStream;\n import org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNode.OperationCategory;\n import org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase;\n import org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;\n@@ -3772,24 +3771,32 @@ void commitBlockSynchronization(ExtendedBlock lastblock,\n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n-        DatanodeDescriptor[] descriptors = null;\n+        List<DatanodeDescriptor> targetList =\n+            new ArrayList<DatanodeDescriptor>(newtargets.length);\n         if (newtargets.length > 0) {\n-          descriptors = new DatanodeDescriptor[newtargets.length];\n-          for(int i = 0; i < newtargets.length; i++) {\n-            descriptors[i] = blockManager.getDatanodeManager().getDatanode(\n-                newtargets[i]);\n+          for (DatanodeID newtarget : newtargets) {\n+            // try to get targetNode\n+            DatanodeDescriptor targetNode =\n+                blockManager.getDatanodeManager().getDatanode(newtarget);\n+            if (targetNode != null)\n+              targetList.add(targetNode);\n+            else if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"DatanodeDescriptor (=\" + newtarget + \") not found\");\n+            }\n           }\n         }\n-        if ((closeFile) && (descriptors != null)) {\n+        if ((closeFile) && !targetList.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n-          for (int i = 0; i < descriptors.length; i++) {\n-            descriptors[i].addBlock(storedBlock);\n+          for (DatanodeDescriptor targetNode : targetList) {\n+            targetNode.addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n-        pendingFile.setLastBlock(storedBlock, descriptors);\n+        DatanodeDescriptor[] targetArray =\n+            new DatanodeDescriptor[targetList.size()];\n+        pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n       }\n \n       if (closeFile) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "a397ce94fd1e4727903d2b56e5d84a487b8c0502",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java?ref=7d2b7171ff0dab08e6e918425afd2b985bb25206",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "patch": "@@ -169,4 +169,23 @@ public void testCommitBlockSynchronizationWithClose() throws IOException {\n     namesystemSpy.commitBlockSynchronization(\n         lastBlock, genStamp, length, true, false, newTargets, null);\n   }\n+\n+  @Test\n+  public void testCommitBlockSynchronizationWithCloseAndNonExistantTarget()\n+      throws IOException {\n+    INodeFileUnderConstruction file = mock(INodeFileUnderConstruction.class);\n+    Block block = new Block(blockId, length, genStamp);\n+    FSNamesystem namesystemSpy = makeNameSystemSpy(block, file);\n+    DatanodeID[] newTargets = new DatanodeID[]{\n+        new DatanodeID(\"0.0.0.0\", \"nonexistantHost\", \"1\", 0, 0, 0)};\n+\n+    ExtendedBlock lastBlock = new ExtendedBlock();\n+    namesystemSpy.commitBlockSynchronization(\n+        lastBlock, genStamp, length, true,\n+        false, newTargets, null);\n+\n+    // Repeat the call to make sure it returns true\n+    namesystemSpy.commitBlockSynchronization(\n+        lastBlock, genStamp, length, true, false, newTargets, null);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "sha": "f40b799d1a824c27dfc948f51d85052a8fb04392",
                "status": "modified"
            }
        ],
        "message": "HDFS-5077. NPE in FSNamesystem.commitBlockSynchronization(). Contributed by Plamen Jeliazkov.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1518851 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/53d8868abe58a9469736ee7d7011fddd1f41bb62",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop-common_853e1e1": {
        "bug_id": "hadoop-common_853e1e1",
        "commit": "https://github.com/apache/hadoop-common/commit/853e1e1c60f3b949ad9877e55e59b38a91da6e48",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=853e1e1c60f3b949ad9877e55e59b38a91da6e48",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -201,6 +201,9 @@ Release 2.5.0 - UNRELEASED\n     YARN-2117. Fixed the issue that secret file reader is potentially not\n     closed in TimelineAuthenticationFilterInitializer. (Chen He via zjshen)\n \n+    YARN-2121. Fixed NPE handling in Timeline Server's TimelineAuthenticator.\n+    (Zhijie Shen via vinodkv)\n+\n Release 2.4.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/CHANGES.txt",
                "sha": "f32978745e7f6532310c937ac3d8be265aeee4b5",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop-common/blob/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java?ref=853e1e1c60f3b949ad9877e55e59b38a91da6e48",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java",
                "patch": "@@ -35,13 +35,15 @@\n import org.apache.hadoop.security.authentication.client.KerberosAuthenticator;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.yarn.api.records.timeline.TimelineDelegationTokenResponse;\n+import org.apache.hadoop.yarn.security.client.TimelineAuthenticationConsts;\n import org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier;\n import org.apache.hadoop.yarn.security.client.TimelineDelegationTokenOperation;\n-import org.apache.hadoop.yarn.security.client.TimelineAuthenticationConsts;\n import org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider;\n import org.codehaus.jackson.JsonNode;\n import org.codehaus.jackson.map.ObjectMapper;\n \n+import com.google.common.annotations.VisibleForTesting;\n+\n /**\n  * A <code>KerberosAuthenticator</code> subclass that fallback to\n  * {@link TimelineAuthenticationConsts}.\n@@ -77,9 +79,15 @@ public static void injectDelegationToken(Map<String, String> params,\n     }\n   }\n \n-  private boolean hasDelegationToken(URL url) {\n-    return url.getQuery().contains(\n-        TimelineAuthenticationConsts.DELEGATION_PARAM + \"=\");\n+  @Private\n+  @VisibleForTesting\n+  boolean hasDelegationToken(URL url) {\n+    if (url.getQuery() == null) {\n+      return false;\n+    } else {\n+      return url.getQuery().contains(\n+          TimelineAuthenticationConsts.DELEGATION_PARAM + \"=\");\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop-common/raw/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java",
                "sha": "25333c7551b014e424045bebaf839ba3a7baf5dd",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/hadoop-common/blob/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java?ref=853e1e1c60f3b949ad9877e55e59b38a91da6e48",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java",
                "patch": "@@ -0,0 +1,40 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.client.api.impl;\n+\n+import java.net.URL;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestTimelineAuthenticator {\n+\n+  @Test\n+  public void testHasDelegationTokens() throws Exception {\n+    TimelineAuthenticator authenticator = new TimelineAuthenticator();\n+    Assert.assertFalse(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource\")));\n+    Assert.assertFalse(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource?other=xxxx\")));\n+    Assert.assertTrue(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource?delegation=yyyy\")));\n+    Assert.assertTrue(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource?other=xxxx&delegation=yyyy\")));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java",
                "sha": "19aaa88533f24514a224645387244cc421a3db77",
                "status": "added"
            }
        ],
        "message": "YARN-2121. Fixed NPE handling in Timeline Server's TimelineAuthenticator. Contributed by Zhijie Shen.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601000 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/81b9fd46eb8ac6e22d9523ff056b509d805e9017",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestTimelineAuthenticator.java"
        ]
    },
    "hadoop-common_8955b15": {
        "bug_id": "hadoop-common_8955b15",
        "commit": "https://github.com/apache/hadoop-common/commit/8955b152c13c1d256f745379ca54ec41e96c1dfb",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt?ref=8955b152c13c1d256f745379ca54ec41e96c1dfb",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "patch": "@@ -59,3 +59,6 @@ fs-encryption (Unreleased)\n   OPTIMIZATIONS\n \n   BUG FIXES\n+\n+    HDFS-6733. Creating encryption zone results in NPE when\n+    KeyProvider is null. (clamb)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "sha": "c447d49b52ceab9ba1ad9515a622bb17810bf5ff",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=8955b152c13c1d256f745379ca54ec41e96c1dfb",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -8448,6 +8448,11 @@ void createEncryptionZone(final String src, String keyNameArg)\n     String keyName = keyNameArg;\n     boolean success = false;\n     try {\n+      if (provider == null) {\n+        throw new IOException(\n+            \"Can't create an encryption zone for \" + src +\n+            \" since no key provider is available.\");\n+      }\n       if (keyName == null || keyName.isEmpty()) {\n         keyName = UUID.randomUUID().toString();\n         createNewKey(keyName, src);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "9b7cb1995839e75c07298a7dc62e449b1192b541",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java?ref=8955b152c13c1d256f745379ca54ec41e96c1dfb",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "patch": "@@ -68,6 +68,7 @@\n   private MiniDFSCluster cluster;\n   private HdfsAdmin dfsAdmin;\n   private DistributedFileSystem fs;\n+  private File testRootDir;\n \n   protected FileSystemTestWrapper fsWrapper;\n   protected FileContextTestWrapper fcWrapper;\n@@ -78,14 +79,14 @@ public void setup() throws IOException {\n     fsHelper = new FileSystemTestHelper();\n     // Set up java key store\n     String testRoot = fsHelper.getTestRootDir();\n-    File testRootDir = new File(testRoot).getAbsoluteFile();\n+    testRootDir = new File(testRoot).getAbsoluteFile();\n     conf.set(KeyProviderFactory.KEY_PROVIDER_PATH,\n         JavaKeyStoreProvider.SCHEME_NAME + \"://file\" + testRootDir + \"/test.jks\"\n     );\n     cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();\n     Logger.getLogger(EncryptionZoneManager.class).setLevel(Level.TRACE);\n     fs = cluster.getFileSystem();\n-    fsWrapper = new FileSystemTestWrapper(cluster.getFileSystem());\n+    fsWrapper = new FileSystemTestWrapper(fs);\n     fcWrapper = new FileContextTestWrapper(\n         FileContext.getFileContext(cluster.getURI(), conf));\n     dfsAdmin = new HdfsAdmin(cluster.getURI(), conf);\n@@ -429,4 +430,25 @@ public void testCipherSuiteNegotiation() throws Exception {\n     }\n   }\n \n+  @Test(timeout = 120000)\n+  public void testCreateEZWithNoProvider() throws Exception {\n+\n+    final Configuration clusterConf = cluster.getConfiguration(0);\n+    clusterConf.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"\");\n+    cluster.restartNameNode(true);\n+    /* Test failure of create EZ on a directory that doesn't exist. */\n+    final Path zone1 = new Path(\"/zone1\");\n+    /* Normal creation of an EZ */\n+    fsWrapper.mkdir(zone1, FsPermission.getDirDefault(), true);\n+    try {\n+      dfsAdmin.createEncryptionZone(zone1, null);\n+      fail(\"expected exception\");\n+    } catch (IOException e) {\n+      assertExceptionContains(\"since no key provider is available\", e);\n+    }\n+    clusterConf.set(KeyProviderFactory.KEY_PROVIDER_PATH,\n+        JavaKeyStoreProvider.SCHEME_NAME + \"://file\" + testRootDir + \"/test.jks\"\n+    );\n+    cluster.restartNameNode(true);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "sha": "421396bdf3873e51886b7e34eaad6a0037162d89",
                "status": "modified"
            }
        ],
        "message": "HDFS-6733. Creating encryption zone results in NPE when KeyProvider is null. (clamb)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612843 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/0d4a9e51c83323efe987a0a279df9bb3521a53e2",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop-common_8cc0089": {
        "bug_id": "hadoop-common_8cc0089",
        "commit": "https://github.com/apache/hadoop-common/commit/8cc0089994e2d8674396eebef72bda0544eae718",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=8cc0089994e2d8674396eebef72bda0544eae718",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -3114,6 +3114,9 @@ Release 0.23.9 - UNRELEASED\n \n   BUG FIXES\n \n+    HDFS-4867. metaSave NPEs when there are invalid blocks in repl queue.\n+    (Plamen Jeliazkov and Ravi Prakash via shv)\n+\n Release 0.23.8 - 2013-06-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "9bc76f3d5078b72e219dfc0f9dcc468276a052a6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=8cc0089994e2d8674396eebef72bda0544eae718",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -458,7 +458,8 @@ private void dumpBlockMeta(Block block, PrintWriter out) {\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n-      String fileName = ((BlockInfo)block).getBlockCollection().getName();\n+      BlockCollection bc = ((BlockInfo) block).getBlockCollection();\n+      String fileName = (bc == null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: == live:, d: == decommissioned c: == corrupt e: == excess",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "966c59656742edeb7431b031687a716c42c8f8b7",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java?ref=8cc0089994e2d8674396eebef72bda0544eae718",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "patch": "@@ -24,11 +24,8 @@\n import java.io.FileInputStream;\n import java.io.IOException;\n import java.io.InputStreamReader;\n-import java.util.Random;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.CommonConfigurationKeys;\n-import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DFSConfigKeys;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "sha": "c0775a62504d43d5526e674c082c2017edb8f8b0",
                "status": "modified"
            }
        ],
        "message": "HDFS-4867. metaSave NPEs when there are invalid blocks in repl queue. Contributed by Plamen Jeliazkov and Ravi Prakash.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1490433 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/318276e2fff29e0af2f8d72c183d3db57fc225d7",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop-common_8e7a6f8": {
        "bug_id": "hadoop-common_8e7a6f8",
        "commit": "https://github.com/apache/hadoop-common/commit/8e7a6f89ec501030e4dcc53f5230310083e8312d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=8e7a6f89ec501030e4dcc53f5230310083e8312d",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -335,6 +335,9 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4128. AM Recovery expects all attempts of a completed task to\n     also be completed. (Bikas Saha via bobby)\n \n+    MAPREDUCE-4144. Fix a NPE in the ResourceManager when handling node\n+    updates. (Jason Lowe via sseth)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "d0eaa60a7342be1d2e8012c739814d903a2851de",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=8e7a6f89ec501030e4dcc53f5230310083e8312d",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -1118,13 +1118,12 @@ private Resource assignOffSwitchContainers(Resource clusterResource, SchedulerNo\n   boolean canAssign(SchedulerApp application, Priority priority, \n       SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n-    // Reserved... \n-    if (reservedContainer != null) {\n-      return true;\n-    }\n-    \n     // Clearly we need containers for this application...\n     if (type == NodeType.OFF_SWITCH) {\n+      if (reservedContainer != null) {\n+        return true;\n+      }\n+\n       // 'Delay' off-switch\n       ResourceRequest offSwitchRequest = \n           application.getResourceRequest(priority, RMNode.ANY);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "2256799f9b55e4212bc184560789dcf12bcfb178",
                "status": "modified"
            },
            {
                "additions": 97,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "changes": 97,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java?ref=8e7a6f89ec501030e4dcc53f5230310083e8312d",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "patch": "@@ -926,6 +926,103 @@ public void testReservation() throws Exception {\n     assertEquals(4*GB, a.getMetrics().getAllocatedMB());\n   }\n   \n+  @Test\n+  public void testStolenReservedContainer() throws Exception {\n+    // Manipulate queue 'a'\n+    LeafQueue a = stubLeafQueue((LeafQueue)queues.get(A));\n+    //unset maxCapacity\n+    a.setMaxCapacity(1.0f);\n+\n+    // Users\n+    final String user_0 = \"user_0\";\n+    final String user_1 = \"user_1\";\n+\n+    // Submit applications\n+    final ApplicationAttemptId appAttemptId_0 =\n+        TestUtils.getMockApplicationAttemptId(0, 0);\n+    SchedulerApp app_0 =\n+        new SchedulerApp(appAttemptId_0, user_0, a,\n+            mock(ActiveUsersManager.class), rmContext, null);\n+    a.submitApplication(app_0, user_0, A);\n+\n+    final ApplicationAttemptId appAttemptId_1 =\n+        TestUtils.getMockApplicationAttemptId(1, 0);\n+    SchedulerApp app_1 =\n+        new SchedulerApp(appAttemptId_1, user_1, a,\n+            mock(ActiveUsersManager.class), rmContext, null);\n+    a.submitApplication(app_1, user_1, A);\n+\n+    // Setup some nodes\n+    String host_0 = \"host_0\";\n+    SchedulerNode node_0 = TestUtils.getMockNode(host_0, DEFAULT_RACK, 0, 4*GB);\n+    String host_1 = \"host_1\";\n+    SchedulerNode node_1 = TestUtils.getMockNode(host_1, DEFAULT_RACK, 0, 4*GB);\n+\n+    final int numNodes = 3;\n+    Resource clusterResource = Resources.createResource(numNodes * (4*GB));\n+    when(csContext.getNumClusterNodes()).thenReturn(numNodes);\n+\n+    // Setup resource-requests\n+    Priority priority = TestUtils.createMockPriority(1);\n+    app_0.updateResourceRequests(Collections.singletonList(\n+            TestUtils.createResourceRequest(RMNodeImpl.ANY, 2*GB, 1, priority,\n+                recordFactory)));\n+\n+    // Setup app_1 to request a 4GB container on host_0 and\n+    // another 4GB container anywhere.\n+    ArrayList<ResourceRequest> appRequests_1 =\n+        new ArrayList<ResourceRequest>(4);\n+    appRequests_1.add(TestUtils.createResourceRequest(host_0, 4*GB, 1,\n+        priority, recordFactory));\n+    appRequests_1.add(TestUtils.createResourceRequest(DEFAULT_RACK, 4*GB, 1,\n+        priority, recordFactory));\n+    appRequests_1.add(TestUtils.createResourceRequest(RMNodeImpl.ANY, 4*GB, 2,\n+        priority, recordFactory));\n+    app_1.updateResourceRequests(appRequests_1);\n+\n+    // Start testing...\n+\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(2*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, a.getMetrics().getReservedMB());\n+    assertEquals(2*GB, a.getMetrics().getAllocatedMB());\n+    assertEquals(0*GB, a.getMetrics().getAvailableMB());\n+\n+    // Now, reservation should kick in for app_1\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(6*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(2*GB, node_0.getUsedResource().getMemory());\n+    assertEquals(4*GB, a.getMetrics().getReservedMB());\n+    assertEquals(2*GB, a.getMetrics().getAllocatedMB());\n+\n+    // node_1 heartbeats in and gets the DEFAULT_RACK request for app_1\n+    a.assignContainers(clusterResource, node_1);\n+    assertEquals(10*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(4*GB, node_1.getUsedResource().getMemory());\n+    assertEquals(4*GB, a.getMetrics().getReservedMB());\n+    assertEquals(6*GB, a.getMetrics().getAllocatedMB());\n+\n+    // Now free 1 container from app_0 and try to assign to node_0\n+    a.completedContainer(clusterResource, app_0, node_0,\n+        app_0.getLiveContainers().iterator().next(), null, RMContainerEventType.KILL);\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(8*GB, a.getUsedResources().getMemory());\n+    assertEquals(0*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(8*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(4*GB, node_0.getUsedResource().getMemory());\n+    assertEquals(0*GB, a.getMetrics().getReservedMB());\n+    assertEquals(8*GB, a.getMetrics().getAllocatedMB());\n+  }\n+\n   @Test\n   public void testReservationExchange() throws Exception {\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "sha": "8be9b20193e1b19eb3e99808b94dc5b15b8325b6",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4144. Fix a NPE in the ResourceManager when handling node updates. (Contributed by Jason Lowe)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1325991 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/459ddddc435dfd1ae01a64321a691d65fa6ced6f",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestLeafQueue.java"
        ]
    },
    "hadoop-common_8ea3e22": {
        "bug_id": "hadoop-common_8ea3e22",
        "commit": "https://github.com/apache/hadoop-common/commit/8ea3e22086f1fac73ba23285348a2f019df0ec92",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=8ea3e22086f1fac73ba23285348a2f019df0ec92",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -715,6 +715,9 @@ Release 0.23.1 - Unreleased\n \n     MAPREDUCE-3813. Added a cache for resolved racks. (vinodkv via acmurthy)   \n \n+    MAPREDUCE-3808. Fixed an NPE in FileOutputCommitter for jobs with maps\n+    but no reduces. (Robert Joseph Evans via vinodkv)\n+\n Release 0.23.0 - 2011-11-01 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "050af54fcebc18df5a1efc456ce94a44380010c8",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java?ref=8ea3e22086f1fac73ba23285348a2f019df0ec92",
                "deletions": 7,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "patch": "@@ -85,18 +85,21 @@ private static Path getOutputPath(TaskAttemptContext context) {\n    */\n   @Private\n   Path getJobAttemptPath(JobContext context) {\n-    return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n-        .getJobAttemptPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : \n+      org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n+        .getJobAttemptPath(context, out);\n   }\n \n   @Private\n   Path getTaskAttemptPath(TaskAttemptContext context) throws IOException {\n-    return getTaskAttemptPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : getTaskAttemptPath(context, out);\n   }\n \n   private Path getTaskAttemptPath(TaskAttemptContext context, Path out) throws IOException {\n     Path workPath = FileOutputFormat.getWorkOutputPath(context.getJobConf());\n-    if(workPath == null) {\n+    if(workPath == null && out != null) {\n       return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n       .getTaskAttemptPath(context, out);\n     }\n@@ -110,14 +113,17 @@ private Path getTaskAttemptPath(TaskAttemptContext context, Path out) throws IOE\n    * @return the path where the output of a committed task is stored until\n    * the entire job is committed.\n    */\n+  @Private\n   Path getCommittedTaskPath(TaskAttemptContext context) {\n-    return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n-        .getCommittedTaskPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : \n+      org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n+        .getCommittedTaskPath(context, out);\n   }\n \n   public Path getWorkPath(TaskAttemptContext context, Path outputPath) \n   throws IOException {\n-    return getTaskAttemptPath(context, outputPath);\n+    return outputPath == null ? null : getTaskAttemptPath(context, outputPath);\n   }\n   \n   @Override\n@@ -156,6 +162,7 @@ public void abortJob(JobContext context, int runState)\n     getWrapped(context).abortJob(context, state);\n   }\n   \n+  @Override\n   public void setupTask(TaskAttemptContext context) throws IOException {\n     getWrapped(context).setupTask(context);\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "sha": "a6190d2060d4c3ee0fc1934f86235ac51366e7ca",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java?ref=8ea3e22086f1fac73ba23285348a2f019df0ec92",
                "deletions": 27,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "patch": "@@ -495,36 +495,40 @@ public boolean isRecoverySupported() {\n   @Override\n   public void recoverTask(TaskAttemptContext context)\n       throws IOException {\n-    context.progress();\n-    TaskAttemptID attemptId = context.getTaskAttemptID();\n-    int previousAttempt = getAppAttemptId(context) - 1;\n-    if (previousAttempt < 0) {\n-      throw new IOException (\"Cannot recover task output for first attempt...\");\n-    }\n-    \n-    Path committedTaskPath = getCommittedTaskPath(context);\n-    Path previousCommittedTaskPath = getCommittedTaskPath(\n-        previousAttempt, context);\n-    FileSystem fs = committedTaskPath.getFileSystem(context.getConfiguration());\n-    \n-    LOG.debug(\"Trying to recover task from \" + previousCommittedTaskPath \n-        + \" into \" + committedTaskPath);\n-    if (fs.exists(previousCommittedTaskPath)) {\n-      if(fs.exists(committedTaskPath)) {\n-        if(!fs.delete(committedTaskPath, true)) {\n-          throw new IOException(\"Could not delete \"+committedTaskPath);\n-        }\n+    if(hasOutputPath()) {\n+      context.progress();\n+      TaskAttemptID attemptId = context.getTaskAttemptID();\n+      int previousAttempt = getAppAttemptId(context) - 1;\n+      if (previousAttempt < 0) {\n+        throw new IOException (\"Cannot recover task output for first attempt...\");\n       }\n-      //Rename can fail if the parent directory does not yet exist.\n-      Path committedParent = committedTaskPath.getParent();\n-      fs.mkdirs(committedParent);\n-      if(!fs.rename(previousCommittedTaskPath, committedTaskPath)) {\n-        throw new IOException(\"Could not rename \" + previousCommittedTaskPath +\n-            \" to \" + committedTaskPath);\n+\n+      Path committedTaskPath = getCommittedTaskPath(context);\n+      Path previousCommittedTaskPath = getCommittedTaskPath(\n+          previousAttempt, context);\n+      FileSystem fs = committedTaskPath.getFileSystem(context.getConfiguration());\n+\n+      LOG.debug(\"Trying to recover task from \" + previousCommittedTaskPath \n+          + \" into \" + committedTaskPath);\n+      if (fs.exists(previousCommittedTaskPath)) {\n+        if(fs.exists(committedTaskPath)) {\n+          if(!fs.delete(committedTaskPath, true)) {\n+            throw new IOException(\"Could not delete \"+committedTaskPath);\n+          }\n+        }\n+        //Rename can fail if the parent directory does not yet exist.\n+        Path committedParent = committedTaskPath.getParent();\n+        fs.mkdirs(committedParent);\n+        if(!fs.rename(previousCommittedTaskPath, committedTaskPath)) {\n+          throw new IOException(\"Could not rename \" + previousCommittedTaskPath +\n+              \" to \" + committedTaskPath);\n+        }\n+        LOG.info(\"Saved output of \" + attemptId + \" to \" + committedTaskPath);\n+      } else {\n+        LOG.warn(attemptId+\" had no output to recover.\");\n       }\n-      LOG.info(\"Saved output of \" + attemptId + \" to \" + committedTaskPath);\n     } else {\n-      LOG.warn(attemptId+\" had no output to recover.\");\n+      LOG.warn(\"Output Path is null in recoverTask()\");\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "sha": "7bad09f303977f66b49e7aad180c0e1d4268703e",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java?ref=8ea3e22086f1fac73ba23285348a2f019df0ec92",
                "deletions": 3,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "patch": "@@ -104,7 +104,9 @@ public void testRecovery() throws Exception {\n     writeOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     Path jobTempDir1 = committer.getCommittedTaskPath(tContext);\n     File jtd1 = new File(jobTempDir1.toUri().getPath());\n     assertTrue(jtd1.exists());\n@@ -188,7 +190,9 @@ public void testCommitter() throws Exception {\n     writeOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     committer.commitJob(jContext);\n \n     // validate output\n@@ -214,14 +218,38 @@ public void testMapFileOutputCommitter() throws Exception {\n     writeMapFileOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     committer.commitJob(jContext);\n \n     // validate output\n     validateMapFileOutputContent(FileSystem.get(conf), outDir);\n     FileUtil.fullyDelete(new File(outDir.toString()));\n   }\n   \n+  public void testMapOnlyNoOutput() throws Exception {\n+    JobConf conf = new JobConf();\n+    //This is not set on purpose. FileOutputFormat.setOutputPath(conf, outDir);\n+    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\n+    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n+    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n+    FileOutputCommitter committer = new FileOutputCommitter();    \n+    \n+    // setup\n+    committer.setupJob(jContext);\n+    committer.setupTask(tContext);\n+    \n+    if(committer.needsTaskCommit(tContext)) {\n+      // do commit\n+      committer.commitTask(tContext);\n+    }\n+    committer.commitJob(jContext);\n+\n+    // validate output\n+    FileUtil.fullyDelete(new File(outDir.toString()));\n+  }\n+  \n   public void testAbort() throws IOException, InterruptedException {\n     JobConf conf = new JobConf();\n     FileOutputFormat.setOutputPath(conf, outDir);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "sha": "0859571d1f2bde79064403fd1d5023d368f1826d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3808. Fixed an NPE in FileOutputCommitter for jobs with maps but no reduces. Contributed by Robert Joseph Evans.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241217 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/70c5e8dc338e0dbf00d5fe2f9aa99d0d9ac5b8f9",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFileOutputCommitter.java",
            "TestFileOutputCommitter.java"
        ]
    },
    "hadoop-common_90dbbfc": {
        "bug_id": "hadoop-common_90dbbfc",
        "commit": "https://github.com/apache/hadoop-common/commit/90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -481,6 +481,8 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4237. TestNodeStatusUpdater can fail if localhost has a domain\n     associated with it (bobby)\n \n+    MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "ff4664675d44bb97f2aff87288a3d2c7127d3307",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop-common/blob/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java?ref=90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "patch": "@@ -21,22 +21,28 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockNodes;\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n import org.codehaus.jackson.JsonNode;\n import org.codehaus.jackson.map.ObjectMapper;\n import org.junit.AfterClass;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n+import static org.mockito.Mockito.*;\n \n public class TestRMNMInfo {\n   private static final Log LOG = LogFactory.getLog(TestRMNMInfo.class);\n@@ -116,14 +122,47 @@ public void testRMNMInfo() throws Exception {\n               n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n       Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n       Assert.assertNotNull(n.get(\"HealthReport\"));\n-      Assert.assertNotNull(n.get(\"NumContainersMB\"));\n+      Assert.assertNotNull(n.get(\"NumContainers\"));\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected number of used containers\",\n-              0, n.get(\"NumContainersMB\").getValueAsInt());\n+              0, n.get(\"NumContainers\").getValueAsInt());\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected amount of used memory\",\n               0, n.get(\"UsedMemoryMB\").getValueAsInt());\n       Assert.assertNotNull(n.get(\"AvailableMemoryMB\"));\n     }\n   }\n+  \n+  @Test\n+  public void testRMNMInfoMissmatch() throws Exception {\n+    RMContext rmc = mock(RMContext.class);\n+    ResourceScheduler rms = mock(ResourceScheduler.class);\n+    ConcurrentMap<NodeId, RMNode> map = new ConcurrentHashMap<NodeId, RMNode>();\n+    RMNode node = MockNodes.newNodeInfo(1, MockNodes.newResource(4 * 1024));\n+    map.put(node.getNodeID(), node);\n+    when(rmc.getRMNodes()).thenReturn(map);\n+    \n+    RMNMInfo rmInfo = new RMNMInfo(rmc,rms);\n+    String liveNMs = rmInfo.getLiveNodeManagers();\n+    ObjectMapper mapper = new ObjectMapper();\n+    JsonNode jn = mapper.readTree(liveNMs);\n+    Assert.assertEquals(\"Unexpected number of live nodes:\",\n+                                               1, jn.size());\n+    Iterator<JsonNode> it = jn.iterator();\n+    while (it.hasNext()) {\n+      JsonNode n = it.next();\n+      Assert.assertNotNull(n.get(\"HostName\"));\n+      Assert.assertNotNull(n.get(\"Rack\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be RUNNING\",\n+              n.get(\"State\").getValueAsText().contains(\"RUNNING\"));\n+      Assert.assertNotNull(n.get(\"NodeHTTPAddress\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be Healthy\",\n+              n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n+      Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n+      Assert.assertNotNull(n.get(\"HealthReport\"));\n+      Assert.assertNull(n.get(\"NumContainers\"));\n+      Assert.assertNull(n.get(\"UsedMemoryMB\"));\n+      Assert.assertNull(n.get(\"AvailableMemoryMB\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "sha": "4ee485644d91ad5b1fcffde9846301c36d237353",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java?ref=90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
                "deletions": 4,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "patch": "@@ -93,10 +93,12 @@ public String getLiveNodeManagers() {\n                         ni.getNodeHealthStatus().getLastHealthReportTime());\n         info.put(\"HealthReport\",\n                         ni.getNodeHealthStatus().getHealthReport());\n-        info.put(\"NumContainersMB\", report.getNumContainers());\n-        info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n-        info.put(\"AvailableMemoryMB\",\n-                                report.getAvailableResource().getMemory());\n+        if(report != null) {\n+          info.put(\"NumContainers\", report.getNumContainers());\n+          info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n+          info.put(\"AvailableMemoryMB\",\n+              report.getAvailableResource().getMemory());\n+        }\n \n         nodesInfo.add(info);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "sha": "0db42e40ec0e08d72413048956baf0393062157d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1337363 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/6135cf06692f8691f49a57704827b3bb729ae8bd",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRMNMInfo.java"
        ]
    },
    "hadoop-common_98d731e": {
        "bug_id": "hadoop-common_98d731e",
        "commit": "https://github.com/apache/hadoop-common/commit/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "patch": "@@ -49,3 +49,5 @@ IMPROVEMENTS:\n     HDFS-5401. Fix NPE in Directory Scanner. (Arpit Agarwal)\n \n     HDFS-5417. Fix storage IDs in PBHelper and UpgradeUtilities.  (szetszwo)\n+\n+    HDFS-5214. Fix NPEs in BlockManager and DirectoryScanner. (Arpit Agarwal)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "sha": "d878b66b3032b4dcaa4bfc1dbe688bbd82fcf1d9",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -1833,7 +1833,10 @@ private void reportDiff(DatanodeDescriptor dn, DatanodeStorage storage,\n       ReplicaState iState = itBR.getCurrentReplicaState();\n       BlockInfo storedBlock = processReportedBlock(dn, storage.getStorageID(),\n           iblk, iState, toAdd, toInvalidate, toCorrupt, toUC);\n-      toRemove.remove(storedBlock);\n+\n+      if (storedBlock != null) {\n+        toRemove.remove(storedBlock);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "6d5c604ba7dbc631855003ffb675575f3d842d5d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "patch": "@@ -187,7 +187,7 @@ public LinkedElement getNext() {\n         + hours + \" hours for block pool \" + bpid);\n \n     // get the list of blocks and arrange them in random order\n-    List<Block> arr = dataset.getFinalizedBlocks(blockPoolId);\n+    List<FinalizedReplica> arr = dataset.getFinalizedBlocks(blockPoolId);\n     Collections.shuffle(arr);\n     \n     long scanTime = -1;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "sha": "13a83bce5fdc7a701bba62f192f37f588df07ead",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 10,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "patch": "@@ -230,10 +230,6 @@ private static String getSuffix(File f, String prefix) {\n       throw new RuntimeException(prefix + \" is not a prefix of \" + fullPath);\n     }\n \n-    ScanInfo(long blockId) {\n-      this(blockId, null, null, null);\n-    }\n-\n     ScanInfo(long blockId, File blockFile, File metaFile, FsVolumeSpi vol) {\n       this.blockId = blockId;\n       String condensedVolPath = vol == null ? null :\n@@ -439,8 +435,8 @@ void scan() {\n         diffs.put(bpid, diffRecord);\n         \n         statsRecord.totalBlocks = blockpoolReport.length;\n-        List<Block> bl = dataset.getFinalizedBlocks(bpid);\n-        Block[] memReport = bl.toArray(new Block[bl.size()]);\n+        List<FinalizedReplica> bl = dataset.getFinalizedBlocks(bpid);\n+        FinalizedReplica[] memReport = bl.toArray(new FinalizedReplica[bl.size()]);\n         Arrays.sort(memReport); // Sort based on blockId\n   \n         int d = 0; // index for blockpoolReport\n@@ -458,7 +454,8 @@ void scan() {\n           }\n           if (info.getBlockId() > memBlock.getBlockId()) {\n             // Block is missing on the disk\n-            addDifference(diffRecord, statsRecord, memBlock.getBlockId());\n+            addDifference(diffRecord, statsRecord,\n+                          memBlock.getBlockId(), info.getVolume());\n             m++;\n             continue;\n           }\n@@ -478,7 +475,9 @@ void scan() {\n           m++;\n         }\n         while (m < memReport.length) {\n-          addDifference(diffRecord, statsRecord, memReport[m++].getBlockId());\n+          FinalizedReplica current = memReport[m++];\n+          addDifference(diffRecord, statsRecord,\n+                        current.getBlockId(), current.getVolume());\n         }\n         while (d < blockpoolReport.length) {\n           statsRecord.missingMemoryBlocks++;\n@@ -502,10 +501,11 @@ private void addDifference(LinkedList<ScanInfo> diffRecord,\n \n   /** Block is not found on the disk */\n   private void addDifference(LinkedList<ScanInfo> diffRecord,\n-                             Stats statsRecord, long blockId) {\n+                             Stats statsRecord, long blockId,\n+                             FsVolumeSpi vol) {\n     statsRecord.missingBlockFile++;\n     statsRecord.missingMetaFile++;\n-    diffRecord.add(new ScanInfo(blockId));\n+    diffRecord.add(new ScanInfo(blockId, null, null, vol));\n   }\n \n   /** Is the given volume still valid in the dataset? */",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "sha": "17ec35d6fb670e13c585e456bbdfafc917a8d991",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "patch": "@@ -61,6 +61,10 @@ public FinalizedReplica(FinalizedReplica from) {\n     this.unlinked = from.isUnlinked();\n   }\n \n+  public FinalizedReplica(ReplicaInfo replicaInfo) {\n+    super(replicaInfo);\n+  }\n+\n   @Override  // ReplicaInfo\n   public ReplicaState getState() {\n     return ReplicaState.FINALIZED;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "sha": "1a852c346689ca36638e027e966dcc8ae1b080c2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.hadoop.hdfs.protocol.HdfsBlocksMetadata;\n import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.datanode.DataStorage;\n+import org.apache.hadoop.hdfs.server.datanode.FinalizedReplica;\n import org.apache.hadoop.hdfs.server.datanode.Replica;\n import org.apache.hadoop.hdfs.server.datanode.ReplicaInPipelineInterface;\n import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory;\n@@ -98,7 +99,7 @@ public RollingLogs createRollingLogs(String bpid, String prefix\n   public Map<String, Object> getVolumeInfoMap();\n \n   /** @return a list of finalized blocks for the given block pool. */\n-  public List<Block> getFinalizedBlocks(String bpid);\n+  public List<FinalizedReplica> getFinalizedBlocks(String bpid);\n \n   /**\n    * Check whether the in-memory block record matches the block on the disk,",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "sha": "90edd5104ffbf33263006a7c4a17a07433bb74f4",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -1079,11 +1079,12 @@ public BlockListAsLongs getBlockReport(String bpid) {\n    * Get the list of finalized blocks from in-memory blockmap for a block pool.\n    */\n   @Override\n-  public synchronized List<Block> getFinalizedBlocks(String bpid) {\n-    ArrayList<Block> finalized = new ArrayList<Block>(volumeMap.size(bpid));\n+  public synchronized List<FinalizedReplica> getFinalizedBlocks(String bpid) {\n+    ArrayList<FinalizedReplica> finalized =\n+        new ArrayList<FinalizedReplica>(volumeMap.size(bpid));\n     for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n       if(b.getState() == ReplicaState.FINALIZED) {\n-        finalized.add(new Block(b));\n+        finalized.add(new FinalizedReplica(b));\n       }\n     }\n     return finalized;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "8677131d4abbe7ffcae4aa85738d8991ce486dcc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "patch": "@@ -1006,7 +1006,7 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n   }\n \n   @Override\n-  public List<Block> getFinalizedBlocks(String bpid) {\n+  public List<FinalizedReplica> getFinalizedBlocks(String bpid) {\n     throw new UnsupportedOperationException();\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "sha": "6f3bed9fda04b728c1665a972ff34b09696110fd",
                "status": "modified"
            },
            {
                "additions": 115,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "changes": 163,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 48,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "patch": "@@ -17,14 +17,17 @@\n  */\n package org.apache.hadoop.hdfs.server.datanode;\n \n+import static org.hamcrest.core.Is.is;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n import static org.junit.Assert.assertTrue;\n \n import java.io.File;\n import java.io.FilenameFilter;\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n+import java.util.Map;\n import java.util.Random;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.TimeoutException;\n@@ -89,7 +92,7 @@\n   private MiniDFSCluster cluster;\n   private DistributedFileSystem fs;\n \n-  Random rand = new Random(RAND_LIMIT);\n+  private static Random rand = new Random(RAND_LIMIT);\n \n   private static Configuration conf;\n \n@@ -113,6 +116,57 @@ public void shutDownCluster() throws IOException {\n     cluster.shutdown();\n   }\n \n+  private static StorageBlockReport[] getBlockReports(DataNode dn, String bpid) {\n+    Map<String, BlockListAsLongs> perVolumeBlockLists =\n+        dn.getFSDataset().getBlockReports(bpid);\n+\n+    // Send block report\n+    StorageBlockReport[] reports =\n+        new StorageBlockReport[perVolumeBlockLists.size()];\n+\n+    int i = 0;\n+    for(Map.Entry<String, BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {\n+      String storageID = kvPair.getKey();\n+      long[] blockList = kvPair.getValue().getBlockListAsLongs();\n+\n+      // Dummy DatanodeStorage object just for sending the block report.\n+      DatanodeStorage dnStorage = new DatanodeStorage(storageID);\n+      reports[i++] = new StorageBlockReport(dnStorage, blockList);\n+    }\n+\n+    return reports;\n+  }\n+\n+  // Get block reports but modify the GS of one of the blocks.\n+  private static StorageBlockReport[] getBlockReportsCorruptSingleBlockGS(\n+      DataNode dn, String bpid) {\n+    Map<String, BlockListAsLongs> perVolumeBlockLists =\n+        dn.getFSDataset().getBlockReports(bpid);\n+\n+    // Send block report\n+    StorageBlockReport[] reports =\n+        new StorageBlockReport[perVolumeBlockLists.size()];\n+\n+    boolean corruptedBlock = false;\n+\n+    int i = 0;\n+    for(Map.Entry<String, BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {\n+      String storageID = kvPair.getKey();\n+      long[] blockList = kvPair.getValue().getBlockListAsLongs();\n+\n+      if (!corruptedBlock) {\n+        blockList[4] = rand.nextInt();      // Bad GS.\n+        corruptedBlock = true;\n+      }\n+\n+      // Dummy DatanodeStorage object just for sending the block report.\n+      DatanodeStorage dnStorage = new DatanodeStorage(storageID);\n+      reports[i++] = new StorageBlockReport(dnStorage, blockList);\n+    }\n+\n+    return reports;\n+  }\n+\n   /**\n    * Test write a file, verifies and closes it. Then the length of the blocks\n    * are messed up and BlockReport is forced.\n@@ -153,10 +207,8 @@ public void blockReport_01() throws IOException {\n     DataNode dn = cluster.getDataNodes().get(DN_N0);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n \n     List<LocatedBlock> blocksAfterReport =\n       DFSTestUtil.getAllBlocks(fs.open(filePath));\n@@ -211,7 +263,6 @@ public void blockReport_02() throws IOException {\n     for (Integer aRemovedIndex : removedIndex) {\n       blocks2Remove.add(lBlocks.get(aRemovedIndex).getBlock());\n     }\n-    ArrayList<Block> blocks = locatedToBlocks(lBlocks, removedIndex);\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Number of blocks allocated \" + lBlocks.size());\n@@ -225,8 +276,11 @@ public void blockReport_02() throws IOException {\n       for (File f : findAllFiles(dataDir,\n         new MyFileFilter(b.getBlockName(), true))) {\n         DataNodeTestUtils.getFSDataset(dn0).unfinalizeBlock(b);\n-        if (!f.delete())\n+        if (!f.delete()) {\n           LOG.warn(\"Couldn't delete \" + b.getBlockName());\n+        } else {\n+          LOG.debug(\"Deleted file \" + f.toString());\n+        }\n       }\n     }\n \n@@ -235,10 +289,8 @@ public void blockReport_02() throws IOException {\n     // all blocks belong to the same file, hence same BP\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn0.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn0, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n \n     BlockManagerTestUtil.getComputedDatanodeWork(cluster.getNamesystem()\n         .getBlockManager());\n@@ -253,9 +305,8 @@ public void blockReport_02() throws IOException {\n \n \n   /**\n-   * Test writes a file and closes it. Then test finds a block\n-   * and changes its GS to be < of original one.\n-   * New empty block is added to the list of blocks.\n+   * Test writes a file and closes it.\n+   * Block reported is generated with a bad GS for a single block.\n    * Block report is forced and the check for # of corrupted blocks is performed.\n    *\n    * @throws IOException in case of an error\n@@ -264,41 +315,65 @@ public void blockReport_02() throws IOException {\n   public void blockReport_03() throws IOException {\n     final String METHOD_NAME = GenericTestUtils.getMethodName();\n     Path filePath = new Path(\"/\" + METHOD_NAME + \".dat\");\n-\n-    ArrayList<Block> blocks =\n-      prepareForRide(filePath, METHOD_NAME, FILE_SIZE);\n-\n-    // The block with modified GS won't be found. Has to be deleted\n-    blocks.get(0).setGenerationStamp(rand.nextLong());\n-    // This new block is unknown to NN and will be mark for deletion.\n-    blocks.add(new Block());\n+    DFSTestUtil.createFile(fs, filePath,\n+                           FILE_SIZE, REPL_FACTOR, rand.nextLong());\n     \n     // all blocks belong to the same file, hence same BP\n     DataNode dn = cluster.getDataNodes().get(DN_N0);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+    StorageBlockReport[] reports = getBlockReportsCorruptSingleBlockGS(dn, poolId);\n     DatanodeCommand dnCmd =\n-      cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+      cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Got the command: \" + dnCmd);\n     }\n     printStats();\n \n-    assertEquals(\"Wrong number of CorruptedReplica+PendingDeletion \" +\n-      \"blocks is found\", 2,\n-        cluster.getNamesystem().getCorruptReplicaBlocks() +\n-        cluster.getNamesystem().getPendingDeletionBlocks());\n+    assertThat(\"Wrong number of corrupt blocks\",\n+               cluster.getNamesystem().getCorruptReplicaBlocks(), is(1L));\n+    assertThat(\"Wrong number of PendingDeletion blocks\",\n+               cluster.getNamesystem().getPendingDeletionBlocks(), is(0L));\n   }\n \n   /**\n-   * This test isn't a representative case for BlockReport\n-   * The empty method is going to be left here to keep the naming\n-   * of the test plan in synch with the actual implementation\n+   * Test writes a file and closes it.\n+   * Block reported is generated with an extra block.\n+   * Block report is forced and the check for # of pendingdeletion\n+   * blocks is performed.\n+   *\n+   * @throws IOException in case of an error\n    */\n-  public void blockReport_04() {\n+  @Test\n+  public void blockReport_04() throws IOException {\n+    final String METHOD_NAME = GenericTestUtils.getMethodName();\n+    Path filePath = new Path(\"/\" + METHOD_NAME + \".dat\");\n+    DFSTestUtil.createFile(fs, filePath,\n+                           FILE_SIZE, REPL_FACTOR, rand.nextLong());\n+\n+\n+    DataNode dn = cluster.getDataNodes().get(DN_N0);\n+    // all blocks belong to the same file, hence same BP\n+    String poolId = cluster.getNamesystem().getBlockPoolId();\n+\n+    // Create a bogus new block which will not be present on the namenode.\n+    ExtendedBlock b = new ExtendedBlock(\n+        poolId, rand.nextLong(), 1024L, rand.nextLong());\n+    dn.getFSDataset().createRbw(b);\n+\n+    DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    DatanodeCommand dnCmd =\n+        cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n+    if(LOG.isDebugEnabled()) {\n+      LOG.debug(\"Got the command: \" + dnCmd);\n+    }\n+    printStats();\n+\n+    assertThat(\"Wrong number of corrupt blocks\",\n+               cluster.getNamesystem().getCorruptReplicaBlocks(), is(0L));\n+    assertThat(\"Wrong number of PendingDeletion blocks\",\n+               cluster.getNamesystem().getPendingDeletionBlocks(), is(1L));\n   }\n \n   // Client requests new block from NN. The test corrupts this very block\n@@ -331,10 +406,8 @@ public void blockReport_06() throws Exception {\n     DataNode dn = cluster.getDataNodes().get(DN_N1);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n     printStats();\n     assertEquals(\"Wrong number of PendingReplication Blocks\",\n       0, cluster.getNamesystem().getUnderReplicatedBlocks());\n@@ -382,9 +455,7 @@ public void blockReport_07() throws Exception {\n     DataNode dn = cluster.getDataNodes().get(DN_N1);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+    StorageBlockReport[] report = getBlockReports(dn, poolId);\n     cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n     printStats();\n     assertEquals(\"Wrong number of Corrupted blocks\",\n@@ -407,7 +478,7 @@ public void blockReport_07() throws Exception {\n     }\n     \n     report[0] = new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n+        report[0].getStorage(),\n         new BlockListAsLongs(blocks, null).getBlockListAsLongs());\n     cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n     printStats();\n@@ -458,9 +529,7 @@ public void blockReport_08() throws IOException {\n       DataNode dn = cluster.getDataNodes().get(DN_N1);\n       String poolId = cluster.getNamesystem().getBlockPoolId();\n       DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-      StorageBlockReport[] report = { new StorageBlockReport(\n-          new DatanodeStorage(dnR.getDatanodeUuid()),\n-          new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+      StorageBlockReport[] report = getBlockReports(dn, poolId);\n       cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n       printStats();\n       assertEquals(\"Wrong number of PendingReplication blocks\",\n@@ -506,9 +575,7 @@ public void blockReport_09() throws IOException {\n       DataNode dn = cluster.getDataNodes().get(DN_N1);\n       String poolId = cluster.getNamesystem().getBlockPoolId();\n       DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-      StorageBlockReport[] report = { new StorageBlockReport(\n-          new DatanodeStorage(dnR.getDatanodeUuid()),\n-          new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+      StorageBlockReport[] report = getBlockReports(dn, poolId);\n       cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n       printStats();\n       assertEquals(\"Wrong number of PendingReplication blocks\",",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "sha": "21d0339888a6729d0c75a8febe0d7c9413df252c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "patch": "@@ -447,7 +447,7 @@ void testScanInfoObject(long blockId, File blockFile, File metaFile)\n   \n   void testScanInfoObject(long blockId) throws Exception {\n     DirectoryScanner.ScanInfo scanInfo =\n-        new DirectoryScanner.ScanInfo(blockId);\n+        new DirectoryScanner.ScanInfo(blockId, null, null, null);\n     assertEquals(blockId, scanInfo.getBlockId());\n     assertNull(scanInfo.getBlockFile());\n     assertNull(scanInfo.getMetaFile());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "sha": "f5b535d394363b27012b10a4b62bf1d7de7481d1",
                "status": "modified"
            }
        ],
        "message": "HDFS-5214. Fix NPEs in BlockManager and DirectoryScanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1536179 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/6ab218735f33260a7d68ab449461748acc3dbf4d",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBlockManager.java",
            "TestDirectoryScanner.java",
            "TestFsDatasetImpl.java",
            "TestSimulatedFSDataset.java"
        ]
    },
    "hadoop-common_9b88fa7": {
        "bug_id": "hadoop-common_9b88fa7",
        "commit": "https://github.com/apache/hadoop-common/commit/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=9b88fa7a60778cb08f5c0b41f49de5bb166e3c78",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -180,3 +180,5 @@ HDFS-2733. Document HA configuration and CLI. (atm)\n HDFS-2794. Active NN may purge edit log files before standby NN has a chance to read them (todd)\n \n HDFS-2901. Improvements for SBN web UI - not show under-replicated/missing blocks. (Brandon Li via jitendra)\n+\n+HDFS-2905. HA: Standby NN NPE when shared edits dir is deleted. (Bikas Saha via jitendra)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "36c162482b0eb0bf27c12881b58dd11c50b67e73",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java?ref=9b88fa7a60778cb08f5c0b41f49de5bb166e3c78",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "patch": "@@ -135,8 +135,7 @@ public void purgeLogsOlderThan(long minTxIdToKeep)\n    */\n   List<RemoteEditLog> getRemoteEditLogs(long firstTxId) throws IOException {\n     File currentDir = sd.getCurrentDir();\n-    List<EditLogFile> allLogFiles = matchEditLogs(\n-        FileUtil.listFiles(currentDir));\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n     List<RemoteEditLog> ret = Lists.newArrayListWithCapacity(\n         allLogFiles.size());\n \n@@ -155,6 +154,20 @@ public void purgeLogsOlderThan(long minTxIdToKeep)\n     return ret;\n   }\n \n+  /**\n+   * returns matching edit logs via the log directory. Simple helper function\n+   * that lists the files in the logDir and calls matchEditLogs(File[])\n+   * \n+   * @param logDir\n+   *          directory to match edit logs in\n+   * @return matched edit logs\n+   * @throws IOException\n+   *           IOException thrown for invalid logDir\n+   */\n+  static List<EditLogFile> matchEditLogs(File logDir) throws IOException {\n+    return matchEditLogs(FileUtil.listFiles(logDir));\n+  }\n+  \n   static List<EditLogFile> matchEditLogs(File[] filesInStorage) {\n     List<EditLogFile> ret = Lists.newArrayList();\n     for (File f : filesInStorage) {\n@@ -278,7 +291,7 @@ public long getNumberOfTransactions(long fromTxId, boolean inProgressOk)\n   synchronized public void recoverUnfinalizedSegments() throws IOException {\n     File currentDir = sd.getCurrentDir();\n     LOG.info(\"Recovering unfinalized segments in \" + currentDir);\n-    List<EditLogFile> allLogFiles = matchEditLogs(currentDir.listFiles());\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n \n     for (EditLogFile elf : allLogFiles) {\n       if (elf.getFile().equals(currentInProgress)) {\n@@ -318,7 +331,7 @@ synchronized public void recoverUnfinalizedSegments() throws IOException {\n \n   private List<EditLogFile> getLogFiles(long fromTxId) throws IOException {\n     File currentDir = sd.getCurrentDir();\n-    List<EditLogFile> allLogFiles = matchEditLogs(currentDir.listFiles());\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n     List<EditLogFile> logFiles = Lists.newArrayList();\n     \n     for (EditLogFile elf : allLogFiles) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "sha": "1eca2797b44c09be1830b59be2c639d7f061435b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java?ref=9b88fa7a60778cb08f5c0b41f49de5bb166e3c78",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "patch": "@@ -440,7 +440,7 @@ public static EditLogFile findLatestEditsLog(StorageDirectory sd)\n   throws IOException {\n     File currentDir = sd.getCurrentDir();\n     List<EditLogFile> foundEditLogs \n-      = Lists.newArrayList(FileJournalManager.matchEditLogs(currentDir.listFiles()));\n+      = Lists.newArrayList(FileJournalManager.matchEditLogs(currentDir));\n     return Collections.max(foundEditLogs, EditLogFile.COMPARE_BY_START_TXID);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "sha": "665e088cb800bd95af4a5a4771d5f5b280e188b6",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java?ref=9b88fa7a60778cb08f5c0b41f49de5bb166e3c78",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "patch": "@@ -315,6 +315,15 @@ public void testGetRemoteEditLog() throws IOException {\n         \"\", getLogsAsString(fjm, 9999));\n   }\n \n+  /**\n+   * tests that passing an invalid dir to matchEditLogs throws IOException \n+   */\n+  @Test(expected = IOException.class)\n+  public void testMatchEditLogInvalidDirThrowsIOException() throws IOException {\n+    File badDir = new File(\"does not exist\");\n+    FileJournalManager.matchEditLogs(badDir);\n+  }\n+  \n   /**\n    * Make sure that we starting reading the correct op when we request a stream\n    * with a txid in the middle of an edit log file.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "sha": "def293657768e434d587492e65dea58631153e45",
                "status": "modified"
            }
        ],
        "message": "HDFS-2905. HA: Standby NN NPE when shared edits dir is deleted. Contributed by Bikas Saha.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1241757 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d8b3427ae26e8105a0fc87b56a65908d7122c284",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFileJournalManager.java"
        ]
    },
    "hadoop-common_9c08b79": {
        "bug_id": "hadoop-common_9c08b79",
        "commit": "https://github.com/apache/hadoop-common/commit/9c08b79db0aa747720b5794ad67cba9187d29371",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9c08b79db0aa747720b5794ad67cba9187d29371/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -960,3 +960,7 @@ Release 0.21.0 - Unreleased\n \n     MAPREDUCE-1161. Remove ineffective synchronization in NotificationTestCase.\n     (Owen O'Malley via cdouglas)\n+\n+    MAPREDUCE-1075. Fix JobTracker to not throw an NPE for a non-existent\n+    queue. (V.V.Chaitanya Krishna via yhemanth)\n+",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9c08b79db0aa747720b5794ad67cba9187d29371/CHANGES.txt",
                "sha": "385924c8cfabac25911a0372a8c58acbc643c1df",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -1001,8 +1001,12 @@ public Path getSystemDir() {\n   \n   public JobStatus[] getJobsFromQueue(String queueName) throws IOException {\n     try {\n+      QueueInfo queue = cluster.getQueue(queueName);\n+      if (queue == null) {\n+        return null;\n+      }\n       org.apache.hadoop.mapreduce.JobStatus[] stats = \n-        cluster.getQueue(queueName).getJobStatuses();\n+        queue.getJobStatuses();\n       JobStatus[] ret = new JobStatus[stats.length];\n       for (int i = 0 ; i < stats.length; i++ ) {\n         ret[i] = JobStatus.downgrade(stats[i]);\n@@ -1022,7 +1026,11 @@ public Path getSystemDir() {\n    */\n   public JobQueueInfo getQueueInfo(String queueName) throws IOException {\n     try {\n-      return new JobQueueInfo(cluster.getQueue(queueName));\n+      QueueInfo queueInfo = cluster.getQueue(queueName);\n+      if (queueInfo != null) {\n+        return new JobQueueInfo(queueInfo);\n+      }\n+      return null;\n     } catch (InterruptedException ie) {\n       throw new IOException(ie);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "3ec640566492e9af0e17a0d348b52d43e1617fe4",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobQueueClient.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobQueueClient.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/mapred/JobQueueClient.java",
                "patch": "@@ -175,6 +175,11 @@ private void displayQueueList() throws IOException {\n   private void displayQueueInfo(String queue, boolean showJobs)\n       throws IOException {\n     JobQueueInfo jobQueueInfo = jc.getQueueInfo(queue);\n+    \n+    if (jobQueueInfo == null) {\n+      System.out.println(\"Queue \\\"\" + queue + \"\\\" does not exist.\");\n+      return;\n+    }\n     printJobQueueInfo(jobQueueInfo, new PrintWriter(System.out));\n     if (showJobs && (jobQueueInfo.getChildren() == null ||\n         jobQueueInfo.getChildren().size() == 0)) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobQueueClient.java",
                "sha": "129b175966e1fda1d229e1117f2ed5c64b8411ab",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobTracker.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/mapred/JobTracker.java",
                "patch": "@@ -3966,7 +3966,9 @@ public JobQueueInfo getQueueInfo(String queue) throws IOException {\n   @Override\n   public QueueInfo getQueue(String queue) throws IOException {\n     JobQueueInfo jqueue = queueManager.getJobQueueInfo(queue);\n-    jqueue.setJobStatuses(getJobsFromQueue(jqueue.getQueueName()));\n+    if (jqueue != null) {\n+      jqueue.setJobStatuses(getJobsFromQueue(jqueue.getQueueName()));\n+    }\n     return jqueue;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "sha": "6686c97fb3dede682ea457b0a2b577795a5746d2",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "deletions": 4,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java",
                "patch": "@@ -17,20 +17,33 @@\n  */\n package org.apache.hadoop.mapred;\n \n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.CONFIG;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.checkForConfigFile;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.createDocument;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.createSimpleDocumentWithAcls;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.miniMRCluster;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.setUpCluster;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.writeToFile;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n import java.io.File;\n+import java.io.IOException;\n import java.io.StringWriter;\n import java.util.ArrayList;\n import java.util.List;\n \n-import org.junit.After;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Cluster;\n+import org.apache.hadoop.mapreduce.QueueInfo;\n import org.junit.Test;\n+import org.w3c.dom.Document;\n \n public class TestJobQueueClient {\n   @Test\n   public void testQueueOrdering() throws Exception {\n-    System.out.println(\"in test queue ordering\");\n     // create some sample queues in a hierarchy..\n     JobQueueInfo[] roots = new JobQueueInfo[2];\n     roots[0] = new JobQueueInfo(\"q1\", \"q1 scheduling info\");\n@@ -53,7 +66,6 @@ public void testQueueOrdering() throws Exception {\n   \n   @Test\n   public void testQueueInfoPrinting() throws Exception {\n-    System.out.println(\"in test queue info printing\");\n     // create a test queue with children.\n     // create some sample queues in a hierarchy..\n     JobQueueInfo root = new JobQueueInfo(\"q1\", \"q1 scheduling info\");\n@@ -76,4 +88,24 @@ public void testQueueInfoPrinting() throws Exception {\n     \n     assertEquals(sb.toString(), writer.toString());\n   }\n-}\n\\ No newline at end of file\n+  \n+  @Test\n+  public void testGetQueue() throws Exception {\n+    checkForConfigFile();\n+    Document doc = createDocument();\n+    createSimpleDocumentWithAcls(doc, \"true\");\n+    writeToFile(doc, CONFIG);\n+    Configuration conf = new Configuration();\n+    conf.addResource(CONFIG);\n+    setUpCluster(conf);\n+    JobClient jc = new JobClient(miniMRCluster.createJobConf());\n+    // test for existing queue\n+    QueueInfo queueInfo = jc.getQueueInfo(\"q1\");\n+    assertEquals(\"q1\",queueInfo.getQueueName());\n+    // try getting a non-existing queue\n+    queueInfo = jc.getQueueInfo(\"queue\");\n+    assertNull(queueInfo);\n+\n+    new File(CONFIG).delete();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java",
                "sha": "94735fe317e52351eb3e6335c25534af225b676b",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1075. Fix JobTracker to not throw an NPE for a non-existent queue. Contributed by V.V.Chaitanya Krishna.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@888257 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ddc67bf61e875e82b47ffb998172f5062791586d",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobClient.java",
            "TestJobQueueClient.java"
        ]
    },
    "hadoop-common_a0692b2": {
        "bug_id": "hadoop-common_a0692b2",
        "commit": "https://github.com/apache/hadoop-common/commit/a0692b2452f97c31ccd9088f6e9b2568047799f7",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a0692b2452f97c31ccd9088f6e9b2568047799f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=a0692b2452f97c31ccd9088f6e9b2568047799f7",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3681,7 +3681,8 @@ public boolean isInStartupSafeMode() {\n \n   @Override\n   public boolean isPopulatingReplQueues() {\n-    if (!haContext.getState().shouldPopulateReplQueues()) {\n+    if (haContext != null && // null during startup!\n+        !haContext.getState().shouldPopulateReplQueues()) {\n       return false;\n     }\n     // safeMode is volatile, and may be set to null at any time",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a0692b2452f97c31ccd9088f6e9b2568047799f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "258cb53186ad15632cf9899da3f0304a189f942c",
                "status": "modified"
            }
        ],
        "message": "Amend HDFS-2795. Fix PersistBlocks failure due to an NPE in isPopulatingReplQueues()\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1232510 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/835b4872937f693034d83003c6f7f0ded1d0df4d",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop-common_a341cdc": {
        "bug_id": "hadoop-common_a341cdc",
        "commit": "https://github.com/apache/hadoop-common/commit/a341cdc655cd07d00dd5ca73006950ef9326fbee",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -509,8 +509,6 @@ Branch-2 ( Unreleased changes )\n     HDFS-3609. libhdfs: don't force the URI to look like hdfs://hostname:port.\n     (Colin Patrick McCabe via eli)\n \n-    HDFS-3654. TestJspHelper#testGetUgi fails with NPE. (eli)\n-\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HDFS-2185. HDFS portion of ZK-based FailoverController (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "4c01ec63631384b08b4b0ab66dbb7a2226b41f61",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "patch": "@@ -540,7 +540,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n     final String usernameFromQuery = getUsernameFromQuery(request, tryUgiParameter);\n     final String doAsUserFromQuery = request.getParameter(DoAsParam.NAME);\n \n-    if (UserGroupInformation.isSecurityEnabled()) {\n+    if(UserGroupInformation.isSecurityEnabled()) {\n       final String remoteUser = request.getRemoteUser();\n       String tokenString = request.getParameter(DELEGATION_PARAMETER_NAME);\n       if (tokenString != null) {\n@@ -558,7 +558,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n         DelegationTokenIdentifier id = new DelegationTokenIdentifier();\n         id.readFields(in);\n         final NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n-        nn.verifyToken(id, token.getPassword());\n+        nn.getNamesystem().verifyToken(id, token.getPassword());\n         ugi = id.getUser();\n         if (ugi.getRealUser() == null) {\n           //non-proxy case",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "sha": "c0da8779fd354d13c394ddf75046f8315df5b240",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -5464,11 +5464,21 @@ public BlockManager getBlockManager() {\n     return blockManager;\n   }\n   \n+  /**\n+   * Verifies that the given identifier and password are valid and match.\n+   * @param identifier Token identifier.\n+   * @param password Password in the token.\n+   * @throws InvalidToken\n+   */\n+  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n+      byte[] password) throws InvalidToken {\n+    getDelegationTokenSecretManager().verifyToken(identifier, password);\n+  }\n+  \n   @Override\n   public boolean isGenStampInFuture(long genStamp) {\n     return (genStamp > getGenerationStamp());\n   }\n-\n   @VisibleForTesting\n   public EditLogTailer getEditLogTailer() {\n     return editLogTailer;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "571bde80b7469124187525f0d6e184c117aea607",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 14,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -51,7 +51,6 @@\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n-import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.NamenodeRole;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption;\n import org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory;\n@@ -79,7 +78,6 @@\n import org.apache.hadoop.security.SecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol;\n-import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.tools.GetUserMappingsProtocol;\n import org.apache.hadoop.util.ServicePlugin;\n import org.apache.hadoop.util.StringUtils;\n@@ -1285,18 +1283,7 @@ private synchronized void doImmediateShutdown(Throwable t)\n     }\n     terminate(1, t);\n   }\n-\n-  /**\n-   * Verifies that the given identifier and password are valid and match.\n-   * @param identifier Token identifier.\n-   * @param password Password in the token.\n-   * @throws InvalidToken\n-   */\n-  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n-      byte[] password) throws InvalidToken {\n-    namesystem.getDelegationTokenSecretManager().verifyToken(identifier, password);\n-  }\n-\n+  \n   /**\n    * Class used to expose {@link NameNode} as context to {@link HAState}\n    */",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "d69328565e623774d335e906df61d5be62cab16e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "patch": "@@ -63,7 +63,7 @@\n   \n   public static final String NAMENODE_ADDRESS_ATTRIBUTE_KEY = \"name.node.address\";\n   public static final String FSIMAGE_ATTRIBUTE_KEY = \"name.system.image\";\n-  public static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n+  protected static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n   \n   public NameNodeHttpServer(\n       Configuration conf,",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "sha": "44b0437d131646de1483dcf1da4787c6e4c18c38",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "patch": "@@ -30,7 +30,6 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -70,7 +69,6 @@ public void testGetUgi() throws IOException {\n     conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:4321/\");\n     HttpServletRequest request = mock(HttpServletRequest.class);\n     ServletContext context = mock(ServletContext.class);\n-    NameNode nn = mock(NameNode.class);\n     String user = \"TheDoctor\";\n     Text userText = new Text(user);\n     DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(userText,\n@@ -81,8 +79,6 @@ public void testGetUgi() throws IOException {\n     when(request.getParameter(JspHelper.DELEGATION_PARAMETER_NAME)).thenReturn(\n         tokenString);\n     when(request.getRemoteUser()).thenReturn(user);\n-    when(context.getAttribute(\n-        NameNodeHttpServer.NAMENODE_ATTRIBUTE_KEY)).thenReturn(nn);\n \n     //Test attribute in the url to be used as service in the token.\n     when(request.getParameter(JspHelper.NAMENODE_ADDRESS)).thenReturn(",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "sha": "c7fafdb13bc8b2bdf26bf5ed53b3353552975e3c",
                "status": "modified"
            }
        ],
        "message": "Revert HDFS-3654. TestJspHelper#testGetUgi fails with NPE.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362759 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/500909d63dc43d41584bc8b24a82ae7e1d51c3b1",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJspHelper.java",
            "TestFSNamesystem.java",
            "TestNameNodeHttpServer.java"
        ]
    },
    "hadoop-common_a5ab28a": {
        "bug_id": "hadoop-common_a5ab28a",
        "commit": "https://github.com/apache/hadoop-common/commit/a5ab28a12f3485d2c70fe795d1ba5e45645dd949",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=a5ab28a12f3485d2c70fe795d1ba5e45645dd949",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -100,6 +100,9 @@ Trunk (unreleased changes)\n     HADOOP-7131. Exceptions thrown by Text methods should include the causing\n     exception. (Uma Maheswara Rao G via todd)\n \n+    HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased().\n+    (Kan Zhang via jitendra)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/CHANGES.txt",
                "sha": "ead2d3f88a9ba1f5b25e5526666fa9d823086f02",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/security/UserGroupInformation.java?ref=a5ab28a12f3485d2c70fe795d1ba5e45645dd949",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -811,8 +811,8 @@ private boolean hasSufficientTimeElapsed(long now) {\n    * Did the login happen via keytab\n    * @return true or false\n    */\n-  public synchronized static boolean isLoginKeytabBased() {\n-    return loginUser.isKeytab;\n+  public synchronized static boolean isLoginKeytabBased() throws IOException {\n+    return getLoginUser().isKeytab;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "085ce61719eefec5cd06738b846867a1335f08cd",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased(). Contributed by Kan Zhang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1079068 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d232e914e91769f80269af43f07509e918b9d06a",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop-common_a5e6943": {
        "bug_id": "hadoop-common_a5e6943",
        "commit": "https://github.com/apache/hadoop-common/commit/a5e6943c593cc2952744f015426b1af6672a135a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a5e6943c593cc2952744f015426b1af6672a135a/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=a5e6943c593cc2952744f015426b1af6672a135a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -601,6 +601,9 @@ Release 0.22.0 - Unreleased\n \n     HDFS-1781. Fix the path for jsvc in bin/hdfs.  (John George via szetszwo)\n \n+    HDFS-1782. Fix an NPE in RFSNamesystem.startFileInternal(..).\n+    (John George via szetszwo)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a5e6943c593cc2952744f015426b1af6672a135a/CHANGES.txt",
                "sha": "ae520af159c4a1e32822ca61ff846df15247d6aa",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a5e6943c593cc2952744f015426b1af6672a135a/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=a5e6943c593cc2952744f015426b1af6672a135a",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -1394,7 +1394,8 @@ private LocatedBlock startFileInternal(String src,\n                 \". Lease recovery is in progress. Try again later.\");\n \n         } else {\n-          if(pendingFile.getLastBlock().getBlockUCState() ==\n+          BlockInfoUnderConstruction lastBlock=pendingFile.getLastBlock();\n+          if(lastBlock != null && lastBlock.getBlockUCState() ==\n             BlockUCState.UNDER_RECOVERY) {\n             throw new RecoveryInProgressException(\n               \"Recovery in progress, file [\" + src + \"], \" +",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a5e6943c593cc2952744f015426b1af6672a135a/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "448f11792f49ba644bd4dadf264c39ae263ed547",
                "status": "modified"
            }
        ],
        "message": "HDFS-1782. Fix an NPE in RFSNamesystem.startFileInternal(..).  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1087115 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/682bd024a199d86e43023eaf1a952d7b85eecfc6",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop-common_b5e5a31": {
        "bug_id": "hadoop-common_b5e5a31",
        "commit": "https://github.com/apache/hadoop-common/commit/b5e5a3112d92016c2bf9f97c3eedd214d9950e45",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=b5e5a3112d92016c2bf9f97c3eedd214d9950e45",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -273,6 +273,8 @@ Trunk (Unreleased)\n     HDFS-4785. Concat operation does not remove concatenated files from\n     InodeMap. (suresh)\n \n+    HDFS-4784. NPE in FSDirectory.resolvePath(). (Brandon Li via suresh)\n+\n   BREAKDOWN OF HADOOP-8562 and HDFS-3602 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4145. Merge hdfs cmd line scripts from branch-1-win. (David Lao,",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "4329d145cb8f54eb93bcf59ea28f76aa48002736",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=b5e5a3112d92016c2bf9f97c3eedd214d9950e45",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1090,7 +1090,7 @@ int unprotectedDelete(String src, BlocksMapUpdateInfo collectedBlocks,\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.unprotectedDelete: \"\n           +src+\" is removed\");\n     }\n-    remvoedAllFromInodesFromMap(targetNode);\n+    removeAllFromInodesFromMap(targetNode);\n     return filesRemoved;\n   }\n   \n@@ -1783,14 +1783,14 @@ private final void removeFromInodeMap(INode inode) {\n   }\n   \n   /** Remove all the inodes under given inode from the map */\n-  private void remvoedAllFromInodesFromMap(INode inode) {\n+  private void removeAllFromInodesFromMap(INode inode) {\n     removeFromInodeMap(inode);\n     if (!inode.isDirectory()) {\n       return;\n     }\n     INodeDirectory dir = (INodeDirectory) inode;\n     for (INode child : dir.getChildrenList()) {\n-      remvoedAllFromInodesFromMap(child);\n+      removeAllFromInodesFromMap(child);\n     }\n     dir.clearChildren();\n   }\n@@ -2258,14 +2258,18 @@ static String resolvePath(String src, byte[][] pathComponents, FSDirectory fsd)\n     try {\n       id = Long.valueOf(inodeId);\n     } catch (NumberFormatException e) {\n-      throw new FileNotFoundException(\n-          \"File for given inode path does not exist: \" + src);\n+      throw new FileNotFoundException(\"Invalid inode path: \" + src);\n     }\n     if (id == INodeId.ROOT_INODE_ID && pathComponents.length == 4) {\n       return Path.SEPARATOR;\n     }\n+    INode inode = fsd.getInode(id);\n+    if (inode == null) {\n+      throw new FileNotFoundException(\n+          \"File for given inode path does not exist: \" + src);\n+    }\n     StringBuilder path = id == INodeId.ROOT_INODE_ID ? new StringBuilder()\n-        : new StringBuilder(fsd.getInode(id).getFullPathName());\n+        : new StringBuilder(inode.getFullPathName());\n     for (int i = 4; i < pathComponents.length; i++) {\n       path.append(Path.SEPARATOR).append(DFSUtil.bytes2String(pathComponents[i]));\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "a3150a8b20eff2ab7bb2beb9b0699e8f923348fd",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java?ref=b5e5a3112d92016c2bf9f97c3eedd214d9950e45",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "patch": "@@ -909,6 +909,17 @@ public void testInodePath() throws FileNotFoundException {\n     components = INode.getPathComponents(testPath);\n     resolvedPath = FSDirectory.resolvePath(testPath, components, fsd);\n     assertEquals(testPath, resolvedPath);\n+    \n+    // Test path with nonexistent(deleted or wrong id) inode\n+    Mockito.doReturn(null).when(fsd).getInode(Mockito.anyLong());\n+    testPath = \"/.reserved/.inodes/1234\";\n+    components = INode.getPathComponents(testPath);\n+    try {\n+      String realPath = FSDirectory.resolvePath(testPath, components, fsd);\n+      fail(\"Path should not be resolved:\" + realPath);\n+    } catch (IOException e) {\n+      assertTrue(e instanceof FileNotFoundException);\n+    }\n   }\n   \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "sha": "3b6491f6e5b4a08d5f5f0f430cb73caff9b9b0c7",
                "status": "modified"
            }
        ],
        "message": "HDFS-4784. NPE in FSDirectory.resolvePath(). Contributed by Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1478276 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/7dc8fc8442e3cf8a57c931bc732adc2e0577c705",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSDirectory.java"
        ]
    },
    "hadoop-common_be31fd0": {
        "bug_id": "hadoop-common_be31fd0",
        "commit": "https://github.com/apache/hadoop-common/commit/be31fd05dae345e62772ecc235f14f3f59c37e04",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -471,6 +471,8 @@ Release 0.22.0 - Unreleased\n     HDFS-1560. dfs.data.dir permissions should default to 700. \n     (Todd Lipcon via eli)\n \n+    HDFS-1550. NPE when listing a file with no location. (hairong)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/CHANGES.txt",
                "sha": "71ebb27b84bef74df28bb21ef9fa2bd398da28cc",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -204,6 +204,9 @@ public static String byteArray2String(byte[][] pathComponents) {\n     }\n     int nrBlocks = blocks.locatedBlockCount();\n     BlockLocation[] blkLocations = new BlockLocation[nrBlocks];\n+    if (nrBlocks == 0) {\n+      return blkLocations;\n+    }\n     int idx = 0;\n     for (LocatedBlock blk : blocks.getLocatedBlocks()) {\n       assert idx < nrBlocks : \"Incorrect index\";",
                "raw_url": "https://github.com/apache/hadoop-common/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "6975c53a25cd37604d11b0327e0db7a473aa136c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "patch": "@@ -19,6 +19,8 @@\n package org.apache.hadoop.hdfs;\n \n import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n \n import java.util.Arrays;\n@@ -65,5 +67,9 @@ public void testLocatedBlocks2Locations() {\n \n     assertTrue(\"expected 1 corrupt files but got \" + corruptCount, \n                corruptCount == 1);\n+    \n+    // test an empty location\n+    bs = DFSUtil.locatedBlocks2Locations(new LocatedBlocks());\n+    assertEquals(0, bs.length);\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "sha": "03e6b39c735c4110059773a65fe0ee61de79ec4d",
                "status": "modified"
            }
        ],
        "message": "HDFS-1550. NPE when listing a file with no location. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1054807 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d1d9265d1758813b62c20157297e8cfd824628ee",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    },
    "hadoop-common_bee7247": {
        "bug_id": "hadoop-common_bee7247",
        "commit": "https://github.com/apache/hadoop-common/commit/bee72473f36c24d0103236153044bd7836bc19e3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=bee72473f36c24d0103236153044bd7836bc19e3",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -439,6 +439,9 @@ Release 2.3.0 - UNRELEASED\n \n     HADOOP-10100. MiniKDC shouldn't use apacheds-all artifact. (rkanter via tucu)\n \n+    HADOOP-10107. Server.getNumOpenConnections may throw NPE. (Kihwal Lee via\n+    jing9)\n+\n Release 2.2.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "2b9aeb88a5a759ab34e1eccab28d261d53a65826",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java?ref=bee72473f36c24d0103236153044bd7836bc19e3",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "patch": "@@ -2109,6 +2109,7 @@ protected Server(String bindAddress, int port,\n     // Start the listener here and let it bind to the port\n     listener = new Listener();\n     this.port = listener.getAddress().getPort();    \n+    connectionManager = new ConnectionManager();\n     this.rpcMetrics = RpcMetrics.create(this);\n     this.rpcDetailedMetrics = RpcDetailedMetrics.create(this.port);\n     this.tcpNoDelay = conf.getBoolean(\n@@ -2117,7 +2118,6 @@ protected Server(String bindAddress, int port,\n \n     // Create the responder here\n     responder = new Responder();\n-    connectionManager = new ConnectionManager();\n     \n     if (secretManager != null) {\n       SaslRpcServer.init(conf);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "sha": "7fb395cdb056029d85a7940a7e7fbedd941a5a4c",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10107. Server.getNumOpenConnections may throw NPE. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543335 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/66f8ea400a5d97d9602405b5e9ff597d75e30b3f",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestServer.java"
        ]
    },
    "hadoop-common_c231fc2": {
        "bug_id": "hadoop-common_c231fc2",
        "commit": "https://github.com/apache/hadoop-common/commit/c231fc2046a3c0b6f7dda83f71810809b3b7fac4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=c231fc2046a3c0b6f7dda83f71810809b3b7fac4",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -434,6 +434,9 @@ Release 2.3.0 - UNRELEASED\n     HADOOP-10093. hadoop-env.cmd sets HADOOP_CLIENT_OPTS with a max heap size\n     that is too small. (Shanyu Zhao via cnauroth)\n \n+    HADOOP-10094. NPE in GenericOptionsParser#preProcessForWindows().\n+    (Enis Soztutar via cnauroth)\n+\n Release 2.2.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "4fd41a876c51b5258033fc4e71d5f2656c402207",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java?ref=c231fc2046a3c0b6f7dda83f71810809b3b7fac4",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "patch": "@@ -431,6 +431,9 @@ private String validateFiles(String files, Configuration conf)\n     if (!Shell.WINDOWS) {\n       return args;\n     }\n+    if (args == null) {\n+      return null;\n+    }\n     List<String> newArgs = new ArrayList<String>(args.length);\n     for (int i=0; i < args.length; i++) {\n       String prop = null;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "sha": "678185553939200900c73b1ac6952a05c7c07816",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java?ref=c231fc2046a3c0b6f7dda83f71810809b3b7fac4",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "patch": "@@ -282,4 +282,12 @@ private void assertDOptionParsing(String[] args,\n       Arrays.toString(remainingArgs) + Arrays.toString(expectedRemainingArgs),\n       expectedRemainingArgs, remainingArgs);\n   }\n+\n+  /** Test passing null as args. Some classes still call\n+   * Tool interface from java passing null.\n+   */\n+  public void testNullArgs() throws IOException {\n+    GenericOptionsParser parser = new GenericOptionsParser(conf, null);\n+    parser.getRemainingArgs();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "sha": "48a419b3a52f7b72889e8820fb2561630d73531d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10094. NPE in GenericOptionsParser#preProcessForWindows(). Contributed by Enis Soztutar.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1541991 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/b65d41d0b4fc6a9966eb5a7281bfa59f2c7a4307",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestGenericOptionsParser.java"
        ]
    },
    "hadoop-common_c6dcdf6": {
        "bug_id": "hadoop-common_c6dcdf6",
        "commit": "https://github.com/apache/hadoop-common/commit/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -488,6 +488,8 @@ Branch-2 ( Unreleased changes )\n     HDFS-3609. libhdfs: don't force the URI to look like hdfs://hostname:port.\n     (Colin Patrick McCabe via eli)\n \n+    HDFS-3654. TestJspHelper#testGetUgi fails with NPE. (eli)\n+\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HDFS-2185. HDFS portion of ZK-based FailoverController (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "30d61c92e5b840b8c6a48e79d0ab2a3a58d1075e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "patch": "@@ -538,7 +538,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n     final String usernameFromQuery = getUsernameFromQuery(request, tryUgiParameter);\n     final String doAsUserFromQuery = request.getParameter(DoAsParam.NAME);\n \n-    if(UserGroupInformation.isSecurityEnabled()) {\n+    if (UserGroupInformation.isSecurityEnabled()) {\n       final String remoteUser = request.getRemoteUser();\n       String tokenString = request.getParameter(DELEGATION_PARAMETER_NAME);\n       if (tokenString != null) {\n@@ -556,7 +556,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n         DelegationTokenIdentifier id = new DelegationTokenIdentifier();\n         id.readFields(in);\n         final NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n-        nn.getNamesystem().verifyToken(id, token.getPassword());\n+        nn.verifyToken(id, token.getPassword());\n         ugi = id.getUser();\n         if (ugi.getRealUser() == null) {\n           //non-proxy case",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "sha": "39aa8db16d1c8139107767c3529deef7a1900cea",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 11,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -5460,20 +5460,10 @@ public BlockManager getBlockManager() {\n     return blockManager;\n   }\n   \n-  /**\n-   * Verifies that the given identifier and password are valid and match.\n-   * @param identifier Token identifier.\n-   * @param password Password in the token.\n-   * @throws InvalidToken\n-   */\n-  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n-      byte[] password) throws InvalidToken {\n-    getDelegationTokenSecretManager().verifyToken(identifier, password);\n-  }\n-  \n   public boolean isGenStampInFuture(long genStamp) {\n     return (genStamp > getGenerationStamp());\n   }\n+\n   @VisibleForTesting\n   public EditLogTailer getEditLogTailer() {\n     return editLogTailer;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "88716a5ecdb890136bec739c7e5fdfc5501c42c3",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.NamenodeRole;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption;\n import org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory;\n@@ -78,6 +79,7 @@\n import org.apache.hadoop.security.SecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol;\n+import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.tools.GetUserMappingsProtocol;\n import org.apache.hadoop.util.ServicePlugin;\n import org.apache.hadoop.util.StringUtils;\n@@ -1283,7 +1285,18 @@ private synchronized void doImmediateShutdown(Throwable t)\n     }\n     terminate(1, t.getMessage());\n   }\n-  \n+\n+  /**\n+   * Verifies that the given identifier and password are valid and match.\n+   * @param identifier Token identifier.\n+   * @param password Password in the token.\n+   * @throws InvalidToken\n+   */\n+  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n+      byte[] password) throws InvalidToken {\n+    namesystem.getDelegationTokenSecretManager().verifyToken(identifier, password);\n+  }\n+\n   /**\n    * Class used to expose {@link NameNode} as context to {@link HAState}\n    */",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "2d5a90a8adb10e085161687ad7a661a74e29ea38",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "patch": "@@ -63,7 +63,7 @@\n   \n   public static final String NAMENODE_ADDRESS_ATTRIBUTE_KEY = \"name.node.address\";\n   public static final String FSIMAGE_ATTRIBUTE_KEY = \"name.system.image\";\n-  protected static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n+  public static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n   \n   public NameNodeHttpServer(\n       Configuration conf,",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "sha": "f00bb9c40a12d05de8d69885452b4d02a1604419",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n+import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -69,6 +70,7 @@ public void testGetUgi() throws IOException {\n     conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:4321/\");\n     HttpServletRequest request = mock(HttpServletRequest.class);\n     ServletContext context = mock(ServletContext.class);\n+    NameNode nn = mock(NameNode.class);\n     String user = \"TheDoctor\";\n     Text userText = new Text(user);\n     DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(userText,\n@@ -79,6 +81,8 @@ public void testGetUgi() throws IOException {\n     when(request.getParameter(JspHelper.DELEGATION_PARAMETER_NAME)).thenReturn(\n         tokenString);\n     when(request.getRemoteUser()).thenReturn(user);\n+    when(context.getAttribute(\n+        NameNodeHttpServer.NAMENODE_ATTRIBUTE_KEY)).thenReturn(nn);\n \n     //Test attribute in the url to be used as service in the token.\n     when(request.getParameter(JspHelper.NAMENODE_ADDRESS)).thenReturn(",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "sha": "8dad3b33e6fd65d7eefcf83507b93dd5257c86a8",
                "status": "modified"
            }
        ],
        "message": "HDFS-3654. TestJspHelper#testGetUgi fails with NPE. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1361463 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ebd86f6cecbc6a9200aaef3e47537c2ab8ff09aa",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJspHelper.java",
            "TestFSNamesystem.java",
            "TestNameNodeHttpServer.java"
        ]
    },
    "hadoop-common_ce96b98": {
        "bug_id": "hadoop-common_ce96b98",
        "commit": "https://github.com/apache/hadoop-common/commit/ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -74,3 +74,5 @@ Release 0.23.3 - Unreleased\n     YARN-63. RMNodeImpl is missing valid transitions from the UNHEALTHY state\n     (Jason Lowe via bobby)\n \n+    YARN-60. Fixed a bug in ResourceManager which causes all NMs to get NPEs and\n+    thus causes all containers to be rejected. (vinodkv)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/CHANGES.txt",
                "sha": "8284215a98abec90778697f810fabb528d00e125",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "patch": "@@ -18,11 +18,14 @@\n \n package org.apache.hadoop.yarn.server.api.protocolrecords;\n \n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n \n-\n public interface NodeHeartbeatRequest {\n-  public abstract NodeStatus getNodeStatus();\n-  \n-  public abstract void setNodeStatus(NodeStatus status);\n+\n+  NodeStatus getNodeStatus();\n+  void setNodeStatus(NodeStatus status);\n+\n+  MasterKey getLastKnownMasterKey();\n+  void setLastKnownMasterKey(MasterKey secretKey);\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "sha": "9e69680d87f6ed2198fd764ac2b9437dc80194fb",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "patch": "@@ -18,24 +18,25 @@\n \n package org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb;\n \n-\n import org.apache.hadoop.yarn.api.records.ProtoBase;\n+import org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProtoOrBuilder;\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n+import org.apache.hadoop.yarn.server.api.records.impl.pb.MasterKeyPBImpl;\n import org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl;\n \n-\n-    \n-public class NodeHeartbeatRequestPBImpl extends ProtoBase<NodeHeartbeatRequestProto> implements NodeHeartbeatRequest {\n+public class NodeHeartbeatRequestPBImpl extends\n+    ProtoBase<NodeHeartbeatRequestProto> implements NodeHeartbeatRequest {\n   NodeHeartbeatRequestProto proto = NodeHeartbeatRequestProto.getDefaultInstance();\n   NodeHeartbeatRequestProto.Builder builder = null;\n   boolean viaProto = false;\n   \n   private NodeStatus nodeStatus = null;\n-  \n+  private MasterKey lastKnownMasterKey = null;\n   \n   public NodeHeartbeatRequestPBImpl() {\n     builder = NodeHeartbeatRequestProto.newBuilder();\n@@ -57,6 +58,10 @@ private void mergeLocalToBuilder() {\n     if (this.nodeStatus != null) {\n       builder.setNodeStatus(convertToProtoFormat(this.nodeStatus));\n     }\n+    if (this.lastKnownMasterKey != null) {\n+      builder\n+        .setLastKnownMasterKey(convertToProtoFormat(this.lastKnownMasterKey));\n+    }\n   }\n \n   private void mergeLocalToProto() {\n@@ -96,6 +101,27 @@ public void setNodeStatus(NodeStatus nodeStatus) {\n     this.nodeStatus = nodeStatus;\n   }\n \n+  @Override\n+  public MasterKey getLastKnownMasterKey() {\n+    NodeHeartbeatRequestProtoOrBuilder p = viaProto ? proto : builder;\n+    if (this.lastKnownMasterKey != null) {\n+      return this.lastKnownMasterKey;\n+    }\n+    if (!p.hasLastKnownMasterKey()) {\n+      return null;\n+    }\n+    this.lastKnownMasterKey = convertFromProtoFormat(p.getLastKnownMasterKey());\n+    return this.lastKnownMasterKey;\n+  }\n+\n+  @Override\n+  public void setLastKnownMasterKey(MasterKey masterKey) {\n+    maybeInitBuilder();\n+    if (masterKey == null) \n+      builder.clearLastKnownMasterKey();\n+    this.lastKnownMasterKey = masterKey;\n+  }\n+\n   private NodeStatusPBImpl convertFromProtoFormat(NodeStatusProto p) {\n     return new NodeStatusPBImpl(p);\n   }\n@@ -104,6 +130,11 @@ private NodeStatusProto convertToProtoFormat(NodeStatus t) {\n     return ((NodeStatusPBImpl)t).getProto();\n   }\n \n+  private MasterKeyPBImpl convertFromProtoFormat(MasterKeyProto p) {\n+    return new MasterKeyPBImpl(p);\n+  }\n \n-\n+  private MasterKeyProto convertToProtoFormat(MasterKey t) {\n+    return ((MasterKeyPBImpl)t).getProto();\n+  }\n }  ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "sha": "8fcf7f2c147a901117435dfd09262655f2bc75fc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "patch": "@@ -35,6 +35,7 @@ message RegisterNodeManagerResponseProto {\n \n message NodeHeartbeatRequestProto {\n   optional NodeStatusProto node_status = 1;\n+  optional MasterKeyProto last_known_master_key = 2;\n }\n \n message NodeHeartbeatResponseProto {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "sha": "e4d82c75d61e56c087f51bd90a52da149de00398",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.avro.AvroRuntimeException;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.classification.InterfaceAudience.Private;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.YarnException;\n@@ -111,10 +112,7 @@ public synchronized void init(Configuration conf) {\n     this.totalResource = recordFactory.newRecordInstance(Resource.class);\n     this.totalResource.setMemory(memoryMb);\n     metrics.addResource(totalResource);\n-    this.tokenKeepAliveEnabled =\n-        conf.getBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED,\n-            YarnConfiguration.DEFAULT_LOG_AGGREGATION_ENABLED)\n-            && isSecurityEnabled();\n+    this.tokenKeepAliveEnabled = isTokenKeepAliveEnabled(conf);\n     this.tokenRemovalDelayMs =\n         conf.getInt(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS,\n             YarnConfiguration.DEFAULT_RM_NM_EXPIRY_INTERVAL_MS);\n@@ -163,10 +161,17 @@ synchronized boolean hasToRebootNode() {\n     return this.hasToRebootNode;\n   }\n \n-  protected boolean isSecurityEnabled() {\n+  private boolean isSecurityEnabled() {\n     return UserGroupInformation.isSecurityEnabled();\n   }\n \n+  @Private\n+  protected boolean isTokenKeepAliveEnabled(Configuration conf) {\n+    return conf.getBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED,\n+        YarnConfiguration.DEFAULT_LOG_AGGREGATION_ENABLED)\n+        && isSecurityEnabled();\n+  }\n+\n   protected ResourceTracker getRMClient() {\n     Configuration conf = getConfig();\n     YarnRPC rpc = YarnRPC.create(conf);\n@@ -321,7 +326,11 @@ public void run() {\n             \n             NodeHeartbeatRequest request = recordFactory\n                 .newRecordInstance(NodeHeartbeatRequest.class);\n-            request.setNodeStatus(nodeStatus);            \n+            request.setNodeStatus(nodeStatus);\n+            if (isSecurityEnabled()) {\n+              request.setLastKnownMasterKey(NodeStatusUpdaterImpl.this.context\n+                .getContainerTokenSecretManager().getCurrentKey());\n+            }\n             HeartbeatResponse response =\n               resourceTracker.nodeHeartbeat(request).getHeartbeatResponse();\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "sha": "819e22d2146ae18083ee7ee7609b7678351c8407",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "patch": "@@ -92,7 +92,6 @@ public synchronized void setMasterKey(MasterKey masterKeyRecord) {\n         containerId.getApplicationAttemptId().getApplicationId();\n \n     MasterKeyData masterKeyToUse = null;\n-\n     if (this.previousMasterKey != null\n         && keyId == this.previousMasterKey.getMasterKey().getKeyId()) {\n       // A container-launch has come in with a token generated off the last",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "sha": "bc70f26a07e93e1bbc53e26d7f8fbd938348a99f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "patch": "@@ -261,7 +261,7 @@ protected ResourceTracker getRMClient() {\n     }\n     \n     @Override\n-    protected boolean isSecurityEnabled() {\n+    protected boolean isTokenKeepAliveEnabled(Configuration conf) {\n       return true;\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "sha": "41d171f97c034a2be50743f9d9887121ac3644af",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -159,7 +159,7 @@ public synchronized void init(Configuration conf) {\n     DelegationTokenRenewer tokenRenewer = createDelegationTokenRenewer();\n     addService(tokenRenewer);\n \n-    this.containerTokenSecretManager = new RMContainerTokenSecretManager(conf);\n+    this.containerTokenSecretManager = createContainerTokenSecretManager(conf);\n     \n     this.rmContext =\n         new RMContextImpl(this.store, this.rmDispatcher,\n@@ -231,6 +231,11 @@ public synchronized void init(Configuration conf) {\n     super.init(conf);\n   }\n \n+  protected RMContainerTokenSecretManager createContainerTokenSecretManager(\n+      Configuration conf) {\n+    return new RMContainerTokenSecretManager(conf);\n+  }\n+\n   protected EventHandler<SchedulerEvent> createSchedulerEventDispatcher() {\n     return new SchedulerEventDispatcher(this.scheduler);\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "e9e5340b80cbd23cc817c6a9708bf2dc3e82f614",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 9,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -169,14 +169,14 @@ public RegisterNodeManagerResponse registerNodeManager(\n       return response;\n     }\n \n-    MasterKey nextMasterKeyForNode = null;\n     if (isSecurityEnabled()) {\n-      nextMasterKeyForNode = this.containerTokenSecretManager.getCurrentKey();\n+      MasterKey nextMasterKeyForNode =\n+          this.containerTokenSecretManager.getCurrentKey();\n       regResponse.setMasterKey(nextMasterKeyForNode);\n     }\n \n     RMNode rmNode = new RMNodeImpl(nodeId, rmContext, host, cmPort, httpPort,\n-        resolve(host), capability, nextMasterKeyForNode);\n+        resolve(host), capability);\n \n     RMNode oldNode = this.rmContext.getRMNodes().putIfAbsent(nodeId, rmNode);\n     if (oldNode == null) {\n@@ -266,17 +266,18 @@ public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request)\n     latestResponse.addAllApplicationsToCleanup(rmNode.getAppsToCleanup());\n     latestResponse.setNodeAction(NodeAction.NORMAL);\n \n-    MasterKey nextMasterKeyForNode = null;\n-\n     // Check if node's masterKey needs to be updated and if the currentKey has\n     // roller over, send it across\n     if (isSecurityEnabled()) {\n+\n       boolean shouldSendMasterKey = false;\n-      MasterKey nodeKnownMasterKey = rmNode.getCurrentMasterKey();\n-      nextMasterKeyForNode = this.containerTokenSecretManager.getNextKey();\n+\n+      MasterKey nextMasterKeyForNode =\n+          this.containerTokenSecretManager.getNextKey();\n       if (nextMasterKeyForNode != null) {\n         // nextMasterKeyForNode can be null if there is no outstanding key that\n         // is in the activation period.\n+        MasterKey nodeKnownMasterKey = request.getLastKnownMasterKey();\n         if (nodeKnownMasterKey.getKeyId() != nextMasterKeyForNode.getKeyId()) {\n           shouldSendMasterKey = true;\n         }\n@@ -290,8 +291,7 @@ public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request)\n     this.rmContext.getDispatcher().getEventHandler().handle(\n         new RMNodeStatusEvent(nodeId, remoteNodeStatus.getNodeHealthStatus(),\n             remoteNodeStatus.getContainersStatuses(), \n-            remoteNodeStatus.getKeepAliveApplications(), latestResponse,\n-            nextMasterKeyForNode));\n+            remoteNodeStatus.getKeepAliveApplications(), latestResponse));\n \n     nodeHeartBeatResponse.setHeartbeatResponse(latestResponse);\n     return nodeHeartBeatResponse;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "ed4a021b0db631102cbc075c9d961020362aea4e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeState;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n \n /**\n  * Node managers information on available resources \n@@ -107,6 +106,4 @@\n   public List<ApplicationId> getAppsToCleanup();\n \n   public HeartbeatResponse getLastHeartBeatResponse();\n-  \n-  public MasterKey getCurrentMasterKey();\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "sha": "aafa3dbdefeaf585d509d3b0e3cc4dd2d56df23b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 19,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "patch": "@@ -46,7 +46,6 @@\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType;\n@@ -105,8 +104,6 @@\n   private HeartbeatResponse latestHeartBeatResponse = recordFactory\n       .newRecordInstance(HeartbeatResponse.class);\n   \n-  private MasterKey currentMasterKey;\n-\n   private static final StateMachineFactory<RMNodeImpl,\n                                            NodeState,\n                                            RMNodeEventType,\n@@ -167,8 +164,7 @@\n                              RMNodeEvent> stateMachine;\n \n   public RMNodeImpl(NodeId nodeId, RMContext context, String hostName,\n-      int cmPort, int httpPort, Node node, Resource capability,\n-      MasterKey masterKey) {\n+      int cmPort, int httpPort, Node node, Resource capability) {\n     this.nodeId = nodeId;\n     this.context = context;\n     this.hostName = hostName;\n@@ -178,7 +174,6 @@ public RMNodeImpl(NodeId nodeId, RMContext context, String hostName,\n     this.nodeAddress = hostName + \":\" + cmPort;\n     this.httpAddress = hostName + \":\" + httpPort;\n     this.node = node;\n-    this.currentMasterKey = masterKey;\n     this.nodeHealthStatus.setIsNodeHealthy(true);\n     this.nodeHealthStatus.setHealthReport(\"Healthy\");\n     this.nodeHealthStatus.setLastHealthReportTime(System.currentTimeMillis());\n@@ -312,17 +307,6 @@ public HeartbeatResponse getLastHeartBeatResponse() {\n       this.readLock.unlock();\n     }\n   }\n-  \n-  @Override\n-  public MasterKey getCurrentMasterKey() {\n-    this.readLock.lock();\n-    try {\n-      return this.currentMasterKey;\n-    } finally {\n-      this.readLock.unlock();\n-    }\n-  }\n-  \n \n   public void handle(RMNodeEvent event) {\n     LOG.debug(\"Processing \" + event.getNodeId() + \" of type \" + event.getType());\n@@ -500,7 +484,6 @@ public NodeState transition(RMNodeImpl rmNode, RMNodeEvent event) {\n \n       // Switch the last heartbeatresponse.\n       rmNode.latestHeartBeatResponse = statusEvent.getLatestResponse();\n-      rmNode.currentMasterKey = statusEvent.getCurrentMasterKey();\n \n       NodeHealthStatus remoteNodeHealthStatus = \n           statusEvent.getNodeHealthStatus();\n@@ -582,7 +565,6 @@ public NodeState transition(RMNodeImpl rmNode, RMNodeEvent event) {\n \n       // Switch the last heartbeatresponse.\n       rmNode.latestHeartBeatResponse = statusEvent.getLatestResponse();\n-      rmNode.currentMasterKey = statusEvent.getCurrentMasterKey();\n       NodeHealthStatus remoteNodeHealthStatus = statusEvent.getNodeHealthStatus();\n       rmNode.setNodeHealthStatus(remoteNodeHealthStatus);\n       if (remoteNodeHealthStatus.getIsNodeHealthy()) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "sha": "83833b9bdb3c4ac1e2ea1e9fac616bbfccbc09b0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "patch": "@@ -25,25 +25,22 @@\n import org.apache.hadoop.yarn.api.records.NodeHealthStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n \n public class RMNodeStatusEvent extends RMNodeEvent {\n \n   private final NodeHealthStatus nodeHealthStatus;\n   private final List<ContainerStatus> containersCollection;\n   private final HeartbeatResponse latestResponse;\n   private final List<ApplicationId> keepAliveAppIds;\n-  private final MasterKey currentMasterKey;\n \n   public RMNodeStatusEvent(NodeId nodeId, NodeHealthStatus nodeHealthStatus,\n       List<ContainerStatus> collection, List<ApplicationId> keepAliveAppIds,\n-      HeartbeatResponse latestResponse, MasterKey currentMasterKey) {\n+      HeartbeatResponse latestResponse) {\n     super(nodeId, RMNodeEventType.STATUS_UPDATE);\n     this.nodeHealthStatus = nodeHealthStatus;\n     this.containersCollection = collection;\n     this.keepAliveAppIds = keepAliveAppIds;\n     this.latestResponse = latestResponse;\n-    this.currentMasterKey = currentMasterKey;\n   }\n \n   public NodeHealthStatus getNodeHealthStatus() {\n@@ -61,8 +58,4 @@ public HeartbeatResponse getLatestResponse() {\n   public List<ApplicationId> getKeepAliveAppIds() {\n     return this.keepAliveAppIds;\n   }\n-  \n-  public MasterKey getCurrentMasterKey() {\n-    return this.currentMasterKey;\n-  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "sha": "1285c2bed99d979c34b86cd59238b75bceb41aa8",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "patch": "@@ -89,14 +89,17 @@ public void stop() {\n    * Creates a new master-key and sets it as the primary.\n    */\n   @Private\n-  protected void rollMasterKey() {\n+  public void rollMasterKey() {\n     super.writeLock.lock();\n     try {\n       LOG.info(\"Rolling master-key for container-tokens\");\n       if (this.currentMasterKey == null) { // Setting up for the first time.\n         this.currentMasterKey = createNewMasterKey();\n       } else {\n         this.nextMasterKey = createNewMasterKey();\n+        LOG.info(\"Going to activate master-key with key-id \"\n+            + this.nextMasterKey.getMasterKey().getKeyId() + \" in \"\n+            + this.activationDelay + \"ms\");\n         this.timer.schedule(new NextKeyActivator(), this.activationDelay);\n       }\n     } finally {\n@@ -122,7 +125,7 @@ public MasterKey getNextKey() {\n    * Activate the new master-key\n    */\n   @Private\n-  protected void activateNextMasterKey() {\n+  public void activateNextMasterKey() {\n     super.writeLock.lock();\n     try {\n       LOG.info(\"Activating next master key with id: \"",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "sha": "cc4ccd7e1ebd863edaae6f6f488c0341e520e4df",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "patch": "@@ -35,7 +35,9 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n+import org.apache.hadoop.yarn.server.api.records.RegistrationResponse;\n import org.apache.hadoop.yarn.util.BuilderUtils;\n import org.apache.hadoop.yarn.util.Records;\n \n@@ -46,8 +48,9 @@\n   private final int memory;\n   private final ResourceTrackerService resourceTracker;\n   private final int httpPort = 2;\n+  private MasterKey currentMasterKey;\n \n-  MockNM(String nodeIdStr, int memory, ResourceTrackerService resourceTracker) {\n+  public MockNM(String nodeIdStr, int memory, ResourceTrackerService resourceTracker) {\n     this.memory = memory;\n     this.resourceTracker = resourceTracker;\n     String[] splits = nodeIdStr.split(\":\");\n@@ -72,21 +75,23 @@ public void containerStatus(Container container) throws Exception {\n     nodeHeartbeat(conts, true);\n   }\n \n-  public NodeId registerNode() throws Exception {\n+  public RegistrationResponse registerNode() throws Exception {\n     RegisterNodeManagerRequest req = Records.newRecord(\n         RegisterNodeManagerRequest.class);\n     req.setNodeId(nodeId);\n     req.setHttpPort(httpPort);\n     Resource resource = Records.newRecord(Resource.class);\n     resource.setMemory(memory);\n     req.setResource(resource);\n-    resourceTracker.registerNodeManager(req);\n-    return nodeId;\n+    RegistrationResponse registrationResponse =\n+        resourceTracker.registerNodeManager(req).getRegistrationResponse();\n+    this.currentMasterKey = registrationResponse.getMasterKey();\n+    return registrationResponse;\n   }\n \n-  public HeartbeatResponse nodeHeartbeat(boolean b) throws Exception {\n+  public HeartbeatResponse nodeHeartbeat(boolean isHealthy) throws Exception {\n     return nodeHeartbeat(new HashMap<ApplicationId, List<ContainerStatus>>(),\n-        b, ++responseId);\n+        isHealthy, ++responseId);\n   }\n \n   public HeartbeatResponse nodeHeartbeat(ApplicationAttemptId attemptId,\n@@ -123,7 +128,15 @@ public HeartbeatResponse nodeHeartbeat(Map<ApplicationId,\n     healthStatus.setLastHealthReportTime(1);\n     status.setNodeHealthStatus(healthStatus);\n     req.setNodeStatus(status);\n-    return resourceTracker.nodeHeartbeat(req).getHeartbeatResponse();\n+    req.setLastKnownMasterKey(this.currentMasterKey);\n+    HeartbeatResponse heartbeatResponse =\n+        resourceTracker.nodeHeartbeat(req).getHeartbeatResponse();\n+    MasterKey masterKeyFromRM = heartbeatResponse.getMasterKey();\n+    this.currentMasterKey =\n+        (masterKeyFromRM != null\n+            && masterKeyFromRM.getKeyId() != this.currentMasterKey.getKeyId()\n+            ? masterKeyFromRM : this.currentMasterKey);\n+    return heartbeatResponse;\n   }\n \n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "sha": "ba999bfb2e094b837d3cb83cc890548543858186",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "patch": "@@ -25,12 +25,11 @@\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.NodeHealthStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n-import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.NodeState;\n+import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n \n import com.google.common.collect.Lists;\n@@ -188,11 +187,6 @@ public NodeState getState() {\n     public HeartbeatResponse getLastHeartBeatResponse() {\n       return null;\n     }\n-\n-    @Override\n-    public MasterKey getCurrentMasterKey() {\n-      return null;\n-    }\n   };\n \n   private static RMNode buildRMNode(int rack, final Resource perNode, NodeState state, String httpAddr) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "sha": "0c56a27ada03143e36ffac9c39f469e27807369e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "patch": "@@ -105,7 +105,7 @@ public Void answer(InvocationOnMock invocation) throws Throwable {\n         new TestSchedulerEventDispatcher());\n     \n     NodeId nodeId = BuilderUtils.newNodeId(\"localhost\", 0);\n-    node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);\n+    node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null);\n \n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "sha": "2b2decccb6bc4d6d2bb0e3f3b6217ef8d23786fd",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "patch": "@@ -54,6 +54,7 @@\n import org.w3c.dom.Element;\n import org.w3c.dom.NodeList;\n import org.xml.sax.InputSource;\n+\n import com.google.inject.Guice;\n import com.google.inject.Injector;\n import com.google.inject.servlet.GuiceServletContextListener;\n@@ -145,7 +146,7 @@ public void testNodesDefaultWithUnHealthyNode() throws JSONException,\n     nodeHealth.setHealthReport(\"test health report\");\n     nodeHealth.setIsNodeHealthy(false);\n     node.handle(new RMNodeStatusEvent(nm3.getNodeId(), nodeHealth,\n-        new ArrayList<ContainerStatus>(), null, null, null));\n+        new ArrayList<ContainerStatus>(), null, null));\n     rm.NMwaitForState(nm3.getNodeId(), NodeState.UNHEALTHY);\n \n     ClientResponse response =\n@@ -360,7 +361,7 @@ public void testNodesQueryHealthyAndState() throws JSONException, Exception {\n     nodeHealth.setHealthReport(\"test health report\");\n     nodeHealth.setIsNodeHealthy(false);\n     node.handle(new RMNodeStatusEvent(nm1.getNodeId(), nodeHealth,\n-        new ArrayList<ContainerStatus>(), null, null, null));\n+        new ArrayList<ContainerStatus>(), null, null));\n     rm.NMwaitForState(nm1.getNodeId(), NodeState.UNHEALTHY);\n \n     ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"cluster\")",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "sha": "084dcffe4cbb20c550e21ca1452c6a0a904decd9",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "patch": "@@ -44,6 +44,12 @@\n       <groupId>org.apache.hadoop</groupId>\n       <artifactId>hadoop-yarn-server-resourcemanager</artifactId>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-yarn-server-resourcemanager</artifactId>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n   </dependencies>\n \n   <build>",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "sha": "600c647f9f775e49fb4a4e8e1078435449002a38",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "patch": "@@ -46,7 +46,6 @@\n import org.apache.hadoop.io.DataInputBuffer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.ipc.RPC;\n-import org.apache.hadoop.ipc.RemoteException;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.SecurityUtil;\n@@ -222,7 +221,7 @@ public void testMaliceUser() throws IOException, InterruptedException {\n     Resource modifiedResource = BuilderUtils.newResource(2048);\n     ContainerTokenIdentifier modifiedIdentifier = new ContainerTokenIdentifier(\n         dummyIdentifier.getContainerID(), dummyIdentifier.getNmHostAddress(),\n-        modifiedResource, Long.MAX_VALUE, 0);\n+        modifiedResource, Long.MAX_VALUE, dummyIdentifier.getMasterKeyId());\n     Token<ContainerTokenIdentifier> modifiedToken = new Token<ContainerTokenIdentifier>(\n         modifiedIdentifier.getBytes(), containerToken.getPassword().array(),\n         new Text(containerToken.getKind()), new Text(containerToken\n@@ -250,19 +249,14 @@ public Void run() {\n               + \"it will indicate RPC success\");\n         } catch (Exception e) {\n           Assert.assertEquals(\n-              java.lang.reflect.UndeclaredThrowableException.class\n-                  .getCanonicalName(), e.getClass().getCanonicalName());\n-          Assert.assertEquals(RemoteException.class.getCanonicalName(), e\n-            .getCause().getClass().getCanonicalName());\n-          Assert.assertEquals(\n-            \"org.apache.hadoop.security.token.SecretManager$InvalidToken\",\n-            ((RemoteException) e.getCause()).getClassName());\n+            java.lang.reflect.UndeclaredThrowableException.class\n+              .getCanonicalName(), e.getClass().getCanonicalName());\n           Assert.assertTrue(e\n             .getCause()\n             .getMessage()\n-            .matches(\n-              \"Given Container container_\\\\d*_\\\\d*_\\\\d\\\\d_\\\\d*\"\n-                  + \" seems to have an illegally generated token.\"));\n+            .contains(\n+              \"DIGEST-MD5: digest response format violation. \"\n+                  + \"Mismatched response.\"));\n         }\n         return null;\n       }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "sha": "1c7933ae275e20cc592feac9b26d687c3f2c8724",
                "status": "modified"
            },
            {
                "additions": 120,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "changes": 120,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "patch": "@@ -0,0 +1,120 @@\n+/**\n+* Licensed to the Apache Software Foundation (ASF) under one\n+* or more contributor license agreements.  See the NOTICE file\n+* distributed with this work for additional information\n+* regarding copyright ownership.  The ASF licenses this file\n+* to you under the Apache License, Version 2.0 (the\n+* \"License\"); you may not use this file except in compliance\n+* with the License.  You may obtain a copy of the License at\n+*\n+*     http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.hadoop.yarn.server;\n+\n+import java.io.IOException;\n+\n+import junit.framework.Assert;\n+\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.event.Dispatcher;\n+import org.apache.hadoop.yarn.event.DrainDispatcher;\n+import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n+import org.apache.hadoop.yarn.server.api.records.RegistrationResponse;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockNM;\n+import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n+import org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager;\n+import org.junit.Test;\n+\n+public class TestRMNMSecretKeys {\n+\n+  @Test\n+  public void testNMUpdation() throws Exception {\n+    YarnConfiguration conf = new YarnConfiguration();\n+    conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,\n+      \"kerberos\");\n+    UserGroupInformation.setConfiguration(conf);\n+    // Default rolling and activation intervals are large enough, no need to\n+    // intervene\n+\n+    final DrainDispatcher dispatcher = new DrainDispatcher();\n+    ResourceManager rm = new ResourceManager(null) {\n+      @Override\n+      protected void doSecureLogin() throws IOException {\n+        // Do nothing.\n+      }\n+\n+      @Override\n+      protected Dispatcher createDispatcher() {\n+        return dispatcher;\n+      }\n+    };\n+    rm.init(conf);\n+    rm.start();\n+\n+    MockNM nm = new MockNM(\"host:1234\", 3072, rm.getResourceTrackerService());\n+    RegistrationResponse registrationResponse = nm.registerNode();\n+    MasterKey masterKey = registrationResponse.getMasterKey();\n+    Assert.assertNotNull(\"Registration should cause a key-update!\", masterKey);\n+    dispatcher.await();\n+\n+    HeartbeatResponse response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"First heartbeat after registration shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert\n+      .assertNull(\n+        \"Even second heartbeat after registration shouldn't get any key updates!\",\n+        response.getMasterKey());\n+    dispatcher.await();\n+\n+    // Let's force a roll-over\n+    RMContainerTokenSecretManager secretManager =\n+        rm.getRMContainerTokenSecretManager();\n+    secretManager.rollMasterKey();\n+\n+    // Heartbeats after roll-over and before activation should be fine.\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNotNull(\n+      \"Heartbeats after roll-over and before activation should not err out.\",\n+      response.getMasterKey());\n+    Assert.assertEquals(\n+      \"Roll-over should have incremented the key-id only by one!\",\n+      masterKey.getKeyId() + 1, response.getMasterKey().getKeyId());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"Second heartbeat after roll-over shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    // Let's force activation\n+    secretManager.activateNextMasterKey();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\"Activation shouldn't cause any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"Even second heartbeat after activation shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    rm.stop();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "sha": "9b6024ce3c0621706b5ffce30d59fb58947ed517",
                "status": "added"
            }
        ],
        "message": "YARN-60. Fixed a bug in ResourceManager which causes all NMs to get NPEs and thus causes all containers to be rejected. Contributed by Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1379550 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/99c9057d2ca9b4ac2a0325f6ac618792dd170071",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestNMContainerTokenSecretManager.java",
            "TestResourceManager.java",
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop-common_d055a72": {
        "bug_id": "hadoop-common_d055a72",
        "commit": "https://github.com/apache/hadoop-common/commit/d055a7269a0a2231360fa48b9647fb8d75d4e1fa",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=d055a7269a0a2231360fa48b9647fb8d75d4e1fa",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -12,6 +12,9 @@ Trunk (unreleased changes)\n \n   NEW FEATURES\n \n+    HADOOP-7322. Adding a util method in FileUtil for directory listing,\n+    avoid NPEs on File.listFiles() (Bharath Mundlapudi via mattf)\n+\n     HADOOP-7023. Add listCorruptFileBlocks to Filesysem. (Patrick Kling\n     via hairong)\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/CHANGES.txt",
                "sha": "b6d57b4104b866a5cfb9af40fea15f9e617741c6",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/java/org/apache/hadoop/fs/FileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/fs/FileUtil.java?ref=d055a7269a0a2231360fa48b9647fb8d75d4e1fa",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/fs/FileUtil.java",
                "patch": "@@ -324,7 +324,7 @@ public static boolean copy(File src,\n       if (!dstFS.mkdirs(dst)) {\n         return false;\n       }\n-      File contents[] = src.listFiles();\n+      File contents[] = listFiles(src);\n       for (int i = 0; i < contents.length; i++) {\n         copy(contents[i], dstFS, new Path(dst, contents[i].getName()),\n              deleteSource, conf);\n@@ -486,8 +486,10 @@ public static long getDU(File dir) {\n     } else {\n       size = dir.length();\n       File[] allFiles = dir.listFiles();\n-      for (int i = 0; i < allFiles.length; i++) {\n-        size = size + getDU(allFiles[i]);\n+      if(allFiles != null) {\n+         for (int i = 0; i < allFiles.length; i++) {\n+            size = size + getDU(allFiles[i]);\n+         }\n       }\n       return size;\n     }\n@@ -707,4 +709,23 @@ public static void replaceFile(File src, File target) throws IOException {\n       }\n     }\n   }\n+  \n+  /**\n+   * A wrapper for {@link File#listFiles()}. This java.io API returns null \n+   * when a dir is not a directory or for any I/O error. Instead of having\n+   * null check everywhere File#listFiles() is used, we will add utility API\n+   * to get around this problem. For the majority of cases where we prefer \n+   * an IOException to be thrown.\n+   * @param dir directory for which listing should be performed\n+   * @return list of files or empty list\n+   * @exception IOException for invalid directory or for a bad disk.\n+   */\n+  public static File[] listFiles(File dir) throws IOException {\n+    File[] files = dir.listFiles();\n+    if(files == null) {\n+      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n+                + dir.toString());\n+    }\n+    return files;\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/java/org/apache/hadoop/fs/FileUtil.java",
                "sha": "537959bd7317e49342f72650c5507729f3b5ea0a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java?ref=d055a7269a0a2231360fa48b9647fb8d75d4e1fa",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "patch": "@@ -276,7 +276,7 @@ public boolean delete(Path p, boolean recursive) throws IOException {\n     if (f.isFile()) {\n       return f.delete();\n     } else if ((!recursive) && f.isDirectory() && \n-        (f.listFiles().length != 0)) {\n+        (FileUtil.listFiles(f).length != 0)) {\n       throw new IOException(\"Directory \" + f.toString() + \" is not empty\");\n     }\n     return FileUtil.fullyDelete(f);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "sha": "63579980cd8f3ca5d4d3b1b2b6132bf54426cc61",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/core/org/apache/hadoop/fs/TestFileUtil.java?ref=d055a7269a0a2231360fa48b9647fb8d75d4e1fa",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "patch": "@@ -117,6 +117,33 @@ private void createFile(File directory, String name, String contents)\n     }\n   }\n \n+  @Test\n+  public void testListFiles() throws IOException {\n+    setupDirs();\n+    //Test existing files case \n+    File[] files = FileUtil.listFiles(partitioned);\n+    Assert.assertEquals(2, files.length);\n+\n+    //Test existing directory with no files case \n+    File newDir = new File(tmp.getPath(),\"test\");\n+    newDir.mkdir();\n+    Assert.assertTrue(\"Failed to create test dir\", newDir.exists());\n+    files = FileUtil.listFiles(newDir);\n+    Assert.assertEquals(0, files.length);\n+    newDir.delete();\n+    Assert.assertFalse(\"Failed to delete test dir\", newDir.exists());\n+    \n+    //Test non-existing directory case, this throws \n+    //IOException\n+    try {\n+      files = FileUtil.listFiles(newDir);\n+      Assert.fail(\"IOException expected on listFiles() for non-existent dir \"\n+      \t\t+ newDir.toString());\n+    } catch(IOException ioe) {\n+    \t//Expected an IOException\n+    }\n+  }\n+  \n   @After\n   public void tearDown() throws IOException {\n     FileUtil.fullyDelete(del);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "sha": "1c193db3e74118e3d496ff68eec991522f008999",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7322. Adding a util method in FileUtil for directory listing, avoid NPEs on File.listFiles(). Contributed by Bharath Mundlapudi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1127697 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/1b97bccd81c7f855c5126f1807d7c3bf059ec83c",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFileUtil.java"
        ]
    },
    "hadoop-common_d1e971c": {
        "bug_id": "hadoop-common_d1e971c",
        "commit": "https://github.com/apache/hadoop-common/commit/d1e971c25f7c8e4e8e7aebbb5555a57829114236",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=d1e971c25f7c8e4e8e7aebbb5555a57829114236",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -149,3 +149,5 @@ HDFS-2845. SBN should not allow browsing of the file system via web UI. (Bikas S\n HDFS-2742. HA: observed dataloss in replication stress test. (todd via eli)\n \n HDFS-2870. Fix log level for block debug info in processMisReplicatedBlocks (todd)\n+\n+HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect (Bikas Saha via todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "7a4ef27f19513e6bb421cf8ad4d79f49e29b0397",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=d1e971c25f7c8e4e8e7aebbb5555a57829114236",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -61,6 +61,8 @@\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n \n import com.google.common.base.Joiner;\n import com.google.common.collect.Lists;\n@@ -69,6 +71,8 @@\n \n @InterfaceAudience.Private\n public class DFSUtil {\n+  private static final Log LOG = LogFactory.getLog(DFSUtil.class.getName());\n+  \n   private DFSUtil() { /* Hidden constructor */ }\n   private static final ThreadLocal<Random> RANDOM = new ThreadLocal<Random>() {\n     @Override\n@@ -935,9 +939,10 @@ private static String getNameServiceId(Configuration conf, String addressKey) {\n         try {\n           s = NetUtils.createSocketAddr(addr);\n         } catch (Exception e) {\n+          LOG.warn(\"Exception in creating socket address\", e);\n           continue;\n         }\n-        if (matcher.match(s)) {\n+        if (!s.isUnresolved() && matcher.match(s)) {\n           nameserviceId = nsId;\n           namenodeId = nnId;\n           found++;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "c9ccf9f38c7cafd26872165b4890710acc96be21",
                "status": "modified"
            }
        ],
        "message": "HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect. Contributed by Bikas Saha.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1239356 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/35f474465df629f5b7f6199a2c029139922588db",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    },
    "hadoop-common_d1f7fd8": {
        "bug_id": "hadoop-common_d1f7fd8",
        "commit": "https://github.com/apache/hadoop-common/commit/d1f7fd801bf00ce62df6ef065cc906d2816352e2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=d1f7fd801bf00ce62df6ef065cc906d2816352e2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -443,6 +443,9 @@ Release 2.4.0 - UNRELEASED\n     apps-killed metrics correctly for killed applications. (Varun Vasudev via\n     vinodkv)\n \n+    YARN-1821. NPE on registerNodeManager if the request has containers for \n+    UnmanagedAMs. (kasha)\n+\n Release 2.3.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/CHANGES.txt",
                "sha": "a222fed796ef219a5784f065c33fd29eaf1ec68e",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=d1f7fd801bf00ce62df6ef065cc906d2816352e2",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -210,14 +210,16 @@ public RegisterNodeManagerResponse registerNodeManager(\n             rmContext.getRMApps().get(appAttemptId.getApplicationId());\n         if (rmApp != null) {\n           RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n-          if (rmAppAttempt.getMasterContainer().getId()\n-              .equals(containerStatus.getContainerId())\n-              && containerStatus.getState() == ContainerState.COMPLETE) {\n-            // sending master container finished event.\n-            RMAppAttemptContainerFinishedEvent evt =\n-                new RMAppAttemptContainerFinishedEvent(appAttemptId,\n-                    containerStatus);\n-            rmContext.getDispatcher().getEventHandler().handle(evt);\n+          if (rmAppAttempt != null) {\n+            if (rmAppAttempt.getMasterContainer().getId()\n+                .equals(containerStatus.getContainerId())\n+                && containerStatus.getState() == ContainerState.COMPLETE) {\n+              // sending master container finished event.\n+              RMAppAttemptContainerFinishedEvent evt =\n+                  new RMAppAttemptContainerFinishedEvent(appAttemptId,\n+                      containerStatus);\n+              rmContext.getDispatcher().getEventHandler().handle(evt);\n+            }\n           }\n         } else {\n           LOG.error(\"Received finished container :\"",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "8a2c53958cba746d4fb5e7f0bf30e199235cb4bb",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java?ref=d1f7fd801bf00ce62df6ef065cc906d2816352e2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "patch": "@@ -21,6 +21,8 @@\n import java.io.File;\n import java.io.FileOutputStream;\n import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n \n@@ -29,7 +31,11 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.ContainerState;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeState;\n@@ -42,6 +48,7 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse;\n import org.apache.hadoop.yarn.server.api.records.NodeAction;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n@@ -50,6 +57,8 @@\n import org.junit.After;\n import org.junit.Test;\n \n+import static org.junit.Assert.assertEquals;\n+\n public class TestResourceTrackerService {\n \n   private final static File TEMP_DIR = new File(System.getProperty(\n@@ -457,6 +466,28 @@ private void checkUnealthyNMCount(MockRM rm, MockNM nm1, boolean health,\n         ClusterMetrics.getMetrics().getUnhealthyNMs());\n   }\n \n+  @Test\n+  public void testNodeRegistrationWithContainers() throws Exception {\n+    MockRM rm = new MockRM();\n+    rm.init(new YarnConfiguration());\n+    rm.start();\n+    RMApp app = rm.submitApp(1024);\n+\n+    MockNM nm = rm.registerNode(\"host1:1234\", 8192);\n+    nm.nodeHeartbeat(true);\n+\n+    // Register node with some container statuses\n+    ContainerStatus status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(ApplicationAttemptId.newInstance(\n+            app.getApplicationId(), 2), 1),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+\n+    // The following shouldn't throw NPE\n+    nm.registerNode(Collections.singletonList(status));\n+    assertEquals(\"Incorrect number of nodes\", 1,\n+        rm.getRMContext().getRMNodes().size());\n+  }\n+\n   @Test\n   public void testReconnectNode() throws Exception {\n     final DrainDispatcher dispatcher = new DrainDispatcher();",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "sha": "eed9ecf9fa7e28f66bcb962fc41c2e842d9c7443",
                "status": "modified"
            }
        ],
        "message": "YARN-1821. NPE on registerNodeManager if the request has containers for UnmanagedAMs (kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1576525 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/0dd4a321c7399694a5f959eb95fe3337204ddf31",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop-common_e21d6b0": {
        "bug_id": "hadoop-common_e21d6b0",
        "commit": "https://github.com/apache/hadoop-common/commit/e21d6b0c394452f3f73d66255e5734d3dbd4dd57",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=e21d6b0c394452f3f73d66255e5734d3dbd4dd57",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -573,6 +573,9 @@ Release 0.23.0 - Unreleased\n     HADOOP-7598. Fix smart-apply-patch.sh to handle patching from a sub\n     directory correctly. (Robert Evans via acmurthy) \n \n+    HADOOP-7328. When a serializer class is missing, return null, not throw\n+    an NPE. (Harsh J Chouraria via todd)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "23ac34d30664d2f504e3883127cb50b1959ae0b9",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java?ref=e21d6b0c394452f3f73d66255e5734d3dbd4dd57",
                "deletions": 10,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "patch": "@@ -27,10 +27,10 @@\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.io.serializer.avro.AvroReflectSerialization;\n import org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization;\n import org.apache.hadoop.util.ReflectionUtils;\n-import org.apache.hadoop.util.StringUtils;\n \n /**\n  * <p>\n@@ -50,14 +50,15 @@\n    * <p>\n    * Serializations are found by reading the <code>io.serializations</code>\n    * property from <code>conf</code>, which is a comma-delimited list of\n-   * classnames. \n+   * classnames.\n    * </p>\n    */\n   public SerializationFactory(Configuration conf) {\n     super(conf);\n-    for (String serializerName : conf.getStrings(\"io.serializations\", \n-      new String[]{WritableSerialization.class.getName(), \n-        AvroSpecificSerialization.class.getName(), \n+    for (String serializerName : conf.getStrings(\n+      CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,\n+      new String[]{WritableSerialization.class.getName(),\n+        AvroSpecificSerialization.class.getName(),\n         AvroReflectSerialization.class.getName()})) {\n       add(conf, serializerName);\n     }\n@@ -67,27 +68,35 @@ public SerializationFactory(Configuration conf) {\n   private void add(Configuration conf, String serializationName) {\n     try {\n       Class<? extends Serialization> serializionClass =\n-\t(Class<? extends Serialization>) conf.getClassByName(serializationName);\n+        (Class<? extends Serialization>) conf.getClassByName(serializationName);\n       serializations.add((Serialization)\n-\t  ReflectionUtils.newInstance(serializionClass, getConf()));\n+      ReflectionUtils.newInstance(serializionClass, getConf()));\n     } catch (ClassNotFoundException e) {\n       LOG.warn(\"Serialization class not found: \", e);\n     }\n   }\n \n   public <T> Serializer<T> getSerializer(Class<T> c) {\n-    return getSerialization(c).getSerializer(c);\n+    Serialization<T> serializer = getSerialization(c);\n+    if (serializer != null) {\n+      return serializer.getSerializer(c);\n+    }\n+    return null;\n   }\n \n   public <T> Deserializer<T> getDeserializer(Class<T> c) {\n-    return getSerialization(c).getDeserializer(c);\n+    Serialization<T> serializer = getSerialization(c);\n+    if (serializer != null) {\n+      return serializer.getDeserializer(c);\n+    }\n+    return null;\n   }\n \n   @SuppressWarnings(\"unchecked\")\n   public <T> Serialization<T> getSerialization(Class<T> c) {\n     for (Serialization serialization : serializations) {\n       if (serialization.accept(c)) {\n-\treturn (Serialization<T>) serialization;\n+        return (Serialization<T>) serialization;\n       }\n     }\n     return null;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "sha": "52a0a253bbaa6bb7e7d580f71292e72fd9c28678",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java?ref=e21d6b0c394452f3f73d66255e5734d3dbd4dd57",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.io.serializer;\n+\n+import org.junit.Test;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertNotNull;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+\n+public class TestSerializationFactory {\n+\n+  @Test\n+  public void testSerializerAvailability() {\n+    Configuration conf = new Configuration();\n+    SerializationFactory factory = new SerializationFactory(conf);\n+    // Test that a valid serializer class is returned when its present\n+    assertNotNull(\"A valid class must be returned for default Writable Serde\",\n+        factory.getSerializer(Writable.class));\n+    assertNotNull(\"A valid class must be returned for default Writable serDe\",\n+        factory.getDeserializer(Writable.class));\n+    // Test that a null is returned when none can be found.\n+    assertNull(\"A null should be returned if there are no serializers found.\",\n+        factory.getSerializer(TestSerializationFactory.class));\n+    assertNull(\"A null should be returned if there are no deserializers found\",\n+        factory.getDeserializer(TestSerializationFactory.class));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "sha": "18c2637ec5adb644f7a56fb41aef5fe82967e9e2",
                "status": "added"
            }
        ],
        "message": "HADOOP-7328. When a serializer class is missing, return null, not throw an NPE. Contributed by Harsh J Chouraria.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1167363 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/b69d3b0c121e6440690935a6f23e6914b0ad77b3",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestSerializationFactory.java"
        ]
    },
    "hadoop-common_e63a794": {
        "bug_id": "hadoop-common_e63a794",
        "commit": "https://github.com/apache/hadoop-common/commit/e63a7949fdf084dd8a93573a9d9b96ae5d08b407",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt?ref=e63a7949fdf084dd8a93573a9d9b96ae5d08b407",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "patch": "@@ -18,3 +18,5 @@ HDFS-3773. TestNNWithQJM fails after HDFS-3741. (atm)\n HDFS-3793. Implement genericized format() in QJM (todd)\n \n HDFS-3795. QJM: validate journal dir at startup (todd)\n+\n+HDFS-3798. Avoid throwing NPE when finalizeSegment() is called on invalid segment (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "sha": "74e443d74d69e12b59b3ac49b5f769f3a7bd7c3b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java?ref=e63a7949fdf084dd8a93573a9d9b96ae5d08b407",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "patch": "@@ -273,6 +273,11 @@ public synchronized void finalizeLogSegment(RequestInfo reqInfo, long startTxId,\n     }\n     \n     FileJournalManager.EditLogFile elf = fjm.getLogFile(startTxId);\n+    if (elf == null) {\n+      throw new IllegalStateException(\"No log file to finalize at \" +\n+          \"transaction ID \" + startTxId);\n+    }\n+\n     if (elf.isInProgress()) {\n       // TODO: this is slow to validate when in non-recovery cases\n       // we already know the length here!\n@@ -281,7 +286,7 @@ public synchronized void finalizeLogSegment(RequestInfo reqInfo, long startTxId,\n       elf.validateLog();\n       \n       Preconditions.checkState(elf.getLastTxId() == endTxId,\n-          \"Trying to finalize log %s-%s, but current state of log\" +\n+          \"Trying to finalize log %s-%s, but current state of log \" +\n           \"is %s\", startTxId, endTxId, elf);\n       fjm.finalizeLogSegment(startTxId, endTxId);\n     } else {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "sha": "cf2c11d885db87a04fb4ddbc59f8db25b34bc1fb",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java?ref=e63a7949fdf084dd8a93573a9d9b96ae5d08b407",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "patch": "@@ -155,6 +155,60 @@ public void testJournalLocking() throws Exception {\n     journal2.newEpoch(FAKE_NSINFO, 2);\n   }\n   \n+  /**\n+   * Test finalizing a segment after some batch of edits were missed.\n+   * This should fail, since we validate the log before finalization.\n+   */\n+  @Test\n+  public void testFinalizeWhenEditsAreMissed() throws Exception {\n+    journal.newEpoch(FAKE_NSINFO, 1);\n+    journal.startLogSegment(makeRI(1), 1);\n+    journal.journal(makeRI(2), 1, 3,\n+        QJMTestUtil.createTxnData(1, 3));\n+    \n+    // Try to finalize up to txn 6, even though we only wrote up to txn 3.\n+    try {\n+      journal.finalizeLogSegment(makeRI(3), 1, 6);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"but current state of log is\", ise);\n+    }\n+    \n+    // Check that, even if we re-construct the journal by scanning the\n+    // disk, we don't allow finalizing incorrectly.\n+    journal.close();\n+    journal = new Journal(TEST_LOG_DIR, mockErrorReporter);\n+    \n+    try {\n+      journal.finalizeLogSegment(makeRI(4), 1, 6);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"but current state of log is\", ise);\n+    }\n+  }\n+  \n+  /**\n+   * Ensure that finalizing a segment which doesn't exist throws the\n+   * appropriate exception.\n+   */\n+  @Test\n+  public void testFinalizeMissingSegment() throws Exception {\n+    journal.newEpoch(FAKE_NSINFO, 1);\n+    try {\n+      journal.finalizeLogSegment(makeRI(1), 1000, 1001);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"No log file to finalize at transaction ID 1000\", ise);\n+    }\n+  }\n+  \n+  private static RequestInfo makeRI(int serial) {\n+    return new RequestInfo(JID, 1, serial);\n+  }\n+  \n   @Test\n   public void testNamespaceVerification() throws Exception {\n     journal.newEpoch(FAKE_NSINFO, 1);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "sha": "f9539d9bb61c8db495b076a4b32c7523cd71c5a4",
                "status": "modified"
            }
        ],
        "message": "HDFS-3798. Avoid throwing NPE when finalizeSegment() is called on invalid segment. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1373179 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/2c199eef8c87a9e972100b018a4b3b519ec6b8cd",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJournal.java"
        ]
    },
    "hadoop-common_e772b01": {
        "bug_id": "hadoop-common_e772b01",
        "commit": "https://github.com/apache/hadoop-common/commit/e772b01c839c438fefaf7cc53c5abc62cc22a379",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=e772b01c839c438fefaf7cc53c5abc62cc22a379",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -105,3 +105,5 @@ HDFS-2766. Test for case where standby partially reads log and then performs che\n HDFS-2738. FSEditLog.selectinputStreams is reading through in-progress streams even when non-in-progress are requested. (atm)\n \n HDFS-2789. TestHAAdmin.testFailover is failing (eli)\n+\n+HDFS-2747. Entering safe mode after starting SBN can NPE. (Uma Maheswara Rao G via todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "fc245ed3e690cc629d69542cf53170b90417b462",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=e772b01c839c438fefaf7cc53c5abc62cc22a379",
                "deletions": 15,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3774,21 +3774,28 @@ private long getCompleteBlocksTotal() {\n   void enterSafeMode(boolean resourcesLow) throws IOException {\n     writeLock();\n     try {\n-    // Ensure that any concurrent operations have been fully synced\n-    // before entering safe mode. This ensures that the FSImage\n-    // is entirely stable on disk as soon as we're in safe mode.\n-    getEditLog().logSyncAll();\n-    if (!isInSafeMode()) {\n-      safeMode = new SafeModeInfo(resourcesLow);\n-      return;\n-    }\n-    if (resourcesLow) {\n-      safeMode.setResourcesLow();\n-    }\n-    safeMode.setManual();\n-    getEditLog().logSyncAll();\n-    NameNode.stateChangeLog.info(\"STATE* Safe mode is ON. \" \n-                                + safeMode.getTurnOffTip());\n+      // Ensure that any concurrent operations have been fully synced\n+      // before entering safe mode. This ensures that the FSImage\n+      // is entirely stable on disk as soon as we're in safe mode.\n+      boolean isEditlogOpenForWrite = getEditLog().isOpenForWrite();\n+      // Before Editlog is in OpenForWrite mode, editLogStream will be null. So,\n+      // logSyncAll call can be called only when Edlitlog is in OpenForWrite mode\n+      if (isEditlogOpenForWrite) {\n+        getEditLog().logSyncAll();\n+      }\n+      if (!isInSafeMode()) {\n+        safeMode = new SafeModeInfo(resourcesLow);\n+        return;\n+      }\n+      if (resourcesLow) {\n+        safeMode.setResourcesLow();\n+      }\n+      safeMode.setManual();\n+      if (isEditlogOpenForWrite) {\n+        getEditLog().logSyncAll();\n+      }\n+      NameNode.stateChangeLog.info(\"STATE* Safe mode is ON. \"\n+          + safeMode.getTurnOffTip());\n     } finally {\n       writeUnlock();\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "f1664f7e62d02065e7fb1231a7238e45d257ac00",
                "status": "modified"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java?ref=e772b01c839c438fefaf7cc53c5abc62cc22a379",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerTestUtil;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeAdapter;\n import org.junit.After;\n@@ -95,6 +96,68 @@ private void restartStandby() throws IOException {\n     nn1.getNamesystem().getEditLogTailer().interrupt();\n   }\n   \n+  /**\n+   * Test case for enter safemode in active namenode, when it is already in startup safemode.\n+   * It is a regression test for HDFS-2747.\n+   */\n+  @Test\n+  public void testEnterSafeModeInANNShouldNotThrowNPE() throws Exception {\n+    banner(\"Restarting active\");\n+    restartActive();\n+    FSNamesystem namesystem = nn0.getNamesystem();\n+    String status = namesystem.getSafemode();\n+    assertTrue(\"Bad safemode status: '\" + status + \"'\", status\n+        .startsWith(\"Safe mode is ON.\"));\n+    NameNodeAdapter.enterSafeMode(nn0, false);\n+    assertTrue(\"Failed to enter into safemode in active\", namesystem\n+        .isInSafeMode());\n+    NameNodeAdapter.enterSafeMode(nn0, false);\n+    assertTrue(\"Failed to enter into safemode in active\", namesystem\n+        .isInSafeMode());\n+  }\n+\n+  /**\n+   * Test case for enter safemode in standby namenode, when it is already in startup safemode.\n+   * It is a regression test for HDFS-2747.\n+   */\n+  @Test\n+  public void testEnterSafeModeInSBNShouldNotThrowNPE() throws Exception {\n+    banner(\"Starting with NN0 active and NN1 standby, creating some blocks\");\n+    DFSTestUtil\n+        .createFile(fs, new Path(\"/test\"), 3 * BLOCK_SIZE, (short) 3, 1L);\n+    // Roll edit log so that, when the SBN restarts, it will load\n+    // the namespace during startup and enter safemode.\n+    nn0.getRpcServer().rollEditLog();\n+    banner(\"Creating some blocks that won't be in the edit log\");\n+    DFSTestUtil.createFile(fs, new Path(\"/test2\"), 5 * BLOCK_SIZE, (short) 3,\n+        1L);\n+    banner(\"Deleting the original blocks\");\n+    fs.delete(new Path(\"/test\"), true);\n+    banner(\"Restarting standby\");\n+    restartStandby();\n+    FSNamesystem namesystem = nn1.getNamesystem();\n+    String status = namesystem.getSafemode();\n+    assertTrue(\"Bad safemode status: '\" + status + \"'\", status\n+        .startsWith(\"Safe mode is ON.\"));\n+    NameNodeAdapter.enterSafeMode(nn1, false);\n+    assertTrue(\"Failed to enter into safemode in standby\", namesystem\n+        .isInSafeMode());\n+    NameNodeAdapter.enterSafeMode(nn1, false);\n+    assertTrue(\"Failed to enter into safemode in standby\", namesystem\n+        .isInSafeMode());\n+  }\n+\n+  private void restartActive() throws IOException {\n+    cluster.shutdownNameNode(0);\n+    // Set the safemode extension to be lengthy, so that the tests\n+    // can check the safemode message after the safemode conditions\n+    // have been achieved, without being racy.\n+    cluster.getConfiguration(0).setInt(\n+        DFSConfigKeys.DFS_NAMENODE_SAFEMODE_EXTENSION_KEY, 30000);\n+    cluster.restartNameNode(0);\n+    nn0 = cluster.getNameNode(0);\n+  }\n+  \n   /**\n    * Tests the case where, while a standby is down, more blocks are\n    * added to the namespace, but not rolled. So, when it starts up,",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "sha": "af7985e21d37e0577092f1822490922f9fb88eec",
                "status": "modified"
            }
        ],
        "message": "HDFS-2747. Entering safe mode after starting SBN can NPE. Contributed by Uma Maheswara Rao G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1232176 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/24161c11d720d85da57cfb3b0f33f2413f934250",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop-common_ea51a26": {
        "bug_id": "hadoop-common_ea51a26",
        "commit": "https://github.com/apache/hadoop-common/commit/ea51a26a9435a23151f91c36cc24692ee9a58f7d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -227,6 +227,9 @@ Release 2.5.0 - UNRELEASED\n     YARN-2128. FairScheduler: Incorrect calculation of amResource usage.\n     (Wei Yan via kasha)\n \n+    YARN-2124. Fixed NPE in ProportionalCapacityPreemptionPolicy. (Wangda Tan\n+    via jianhe)\n+\n Release 2.4.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/CHANGES.txt",
                "sha": "401fb102bf47b883c2c57fbf85ee5e61fd4281bb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -327,7 +327,7 @@ protected static void validateConfigs(Configuration conf) {\n    * RMActiveServices handles all the Active services in the RM.\n    */\n   @Private\n-  class RMActiveServices extends CompositeService {\n+  public class RMActiveServices extends CompositeService {\n \n     private DelegationTokenRenewer delegationTokenRenewer;\n     private EventHandler<SchedulerEvent> schedulerDispatcher;\n@@ -526,11 +526,9 @@ protected void createPolicyMonitors() {\n                   (PreemptableResourceScheduler) scheduler));\n           for (SchedulingEditPolicy policy : policies) {\n             LOG.info(\"LOADING SchedulingEditPolicy:\" + policy.getPolicyName());\n-            policy.init(conf, rmContext.getDispatcher().getEventHandler(),\n-                (PreemptableResourceScheduler) scheduler);\n             // periodically check whether we need to take action to guarantee\n             // constraints\n-            SchedulingMonitor mon = new SchedulingMonitor(policy);\n+            SchedulingMonitor mon = new SchedulingMonitor(rmContext, policy);\n             addService(mon);\n           }\n         } else {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "77de2090a00479282f863f110299a592f5c0d77e",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "patch": "@@ -21,6 +21,8 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.service.AbstractService;\n+import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler;\n \n import com.google.common.annotations.VisibleForTesting;\n \n@@ -34,18 +36,29 @@\n   private Thread checkerThread;\n   private volatile boolean stopped;\n   private long monitorInterval;\n+  private RMContext rmContext;\n \n-  public SchedulingMonitor(SchedulingEditPolicy scheduleEditPolicy) {\n+  public SchedulingMonitor(RMContext rmContext,\n+      SchedulingEditPolicy scheduleEditPolicy) {\n     super(\"SchedulingMonitor (\" + scheduleEditPolicy.getPolicyName() + \")\");\n     this.scheduleEditPolicy = scheduleEditPolicy;\n-    this.monitorInterval = scheduleEditPolicy.getMonitoringInterval();\n+    this.rmContext = rmContext;\n   }\n \n   public long getMonitorInterval() {\n     return monitorInterval;\n   }\n+  \n+  @VisibleForTesting\n+  public synchronized SchedulingEditPolicy getSchedulingEditPolicy() {\n+    return scheduleEditPolicy;\n+  }\n \n+  @SuppressWarnings(\"unchecked\")\n   public void serviceInit(Configuration conf) throws Exception {\n+    scheduleEditPolicy.init(conf, rmContext.getDispatcher().getEventHandler(),\n+        (PreemptableResourceScheduler) rmContext.getScheduler());\n+    this.monitorInterval = scheduleEditPolicy.getMonitoringInterval();\n     super.serviceInit(conf);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "sha": "1682f7d8612602446bc52b157945b5901a6a1a11",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "patch": "@@ -165,6 +165,11 @@ public void init(Configuration config,\n     observeOnly = config.getBoolean(OBSERVE_ONLY, false);\n     rc = scheduler.getResourceCalculator();\n   }\n+  \n+  @VisibleForTesting\n+  public ResourceCalculator getResourceCalculator() {\n+    return rc;\n+  }\n \n   @Override\n   public void editSchedule(){",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "sha": "6d1516158b5aad26990a58d562def6b0ca5df243",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "patch": "@@ -571,4 +571,8 @@ public void clearQueueMetrics(RMApp app) {\n       .getSchedulerApplications().get(app.getApplicationId()).getQueue()\n       .getMetrics().clearQueueMetrics();\n   }\n+  \n+  public RMActiveServices getRMActiveService() {\n+    return activeServices;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "sha": "67eac76e5f335d4df535ec2c888619ec677e9ae9",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "changes": 64,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "patch": "@@ -17,6 +17,25 @@\n  */\n package org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity;\n \n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MAX_IGNORED_OVER_CAPACITY;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MONITORING_INTERVAL;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.NATURAL_TERMINATION_FACTOR;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.OBSERVE_ONLY;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.TOTAL_PREEMPTION_PER_ROUND;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.WAIT_TIME_BEFORE_KILL;\n+import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.KILL_CONTAINER;\n+import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.PREEMPT_CONTAINER;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.fail;\n+import static org.mockito.Matchers.argThat;\n+import static org.mockito.Matchers.isA;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n import java.util.ArrayList;\n import java.util.Comparator;\n import java.util.Deque;\n@@ -27,12 +46,16 @@\n import java.util.TreeSet;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.service.Service;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.EventHandler;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockRM;\n+import org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor;\n import org.apache.hadoop.yarn.server.resourcemanager.resource.Priority;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEvent;\n@@ -52,17 +75,6 @@\n import org.mockito.ArgumentCaptor;\n import org.mockito.ArgumentMatcher;\n \n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MAX_IGNORED_OVER_CAPACITY;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MONITORING_INTERVAL;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.NATURAL_TERMINATION_FACTOR;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.OBSERVE_ONLY;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.TOTAL_PREEMPTION_PER_ROUND;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.WAIT_TIME_BEFORE_KILL;\n-import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.KILL_CONTAINER;\n-import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.PREEMPT_CONTAINER;\n-import static org.junit.Assert.*;\n-import static org.mockito.Mockito.*;\n-\n public class TestProportionalCapacityPreemptionPolicy {\n \n   static final long TS = 3141592653L;\n@@ -424,6 +436,36 @@ public void testContainerOrdering(){\n     assert containers.get(4).equals(rm5);\n \n   }\n+  \n+  @Test\n+  public void testPolicyInitializeAfterSchedulerInitialized() {\n+    Configuration conf = new Configuration();\n+    conf.set(YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES,\n+        ProportionalCapacityPreemptionPolicy.class.getCanonicalName());\n+    conf.setBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS, true);\n+    \n+    @SuppressWarnings(\"resource\")\n+    MockRM rm = new MockRM(conf);\n+    rm.init(conf);\n+    \n+    // ProportionalCapacityPreemptionPolicy should be initialized after\n+    // CapacityScheduler initialized. We will \n+    // 1) find SchedulingMonitor from RMActiveService's service list, \n+    // 2) check if ResourceCalculator in policy is null or not. \n+    // If it's not null, we can come to a conclusion that policy initialized\n+    // after scheduler got initialized\n+    for (Service service : rm.getRMActiveService().getServices()) {\n+      if (service instanceof SchedulingMonitor) {\n+        ProportionalCapacityPreemptionPolicy policy =\n+            (ProportionalCapacityPreemptionPolicy) ((SchedulingMonitor) service)\n+                .getSchedulingEditPolicy();\n+        assertNotNull(policy.getResourceCalculator());\n+        return;\n+      }\n+    }\n+    \n+    fail(\"Failed to find SchedulingMonitor service, please check what happened\");\n+  }\n \n   static class IsPreemptionRequestFor\n       extends ArgumentMatcher<ContainerPreemptEvent> {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "sha": "d0a80eb20bb320d8b1c1ec7009743049bc09eb3f",
                "status": "modified"
            }
        ],
        "message": "YARN-2124. Fixed NPE in ProportionalCapacityPreemptionPolicy. Contributed by Wangda Tan\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601964 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/dc1025cba292da086926dab06667621b051aa524",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestResourceManager.java",
            "TestSchedulingMonitor.java",
            "TestProportionalCapacityPreemptionPolicy.java"
        ]
    },
    "hadoop-common_eb3bdbb": {
        "bug_id": "hadoop-common_eb3bdbb",
        "commit": "https://github.com/apache/hadoop-common/commit/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=eb3bdbb0040eceb2881bdecba8a9900e5759a4ee",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -813,6 +813,8 @@ Release 2.3.0 - UNRELEASED\n     HDFS-5074. Allow starting up from an fsimage checkpoint in the middle of a\n     segment. (Todd Lipcon via atm)\n \n+    HDFS-4201. NPE in BPServiceActor#sendHeartBeat. (jxiang via cmccabe)\n+\n Release 2.2.0 - 2013-10-13\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "ee9b964724cead83d407f35b7c57096567db2885",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java?ref=eb3bdbb0040eceb2881bdecba8a9900e5759a4ee",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "patch": "@@ -274,12 +274,22 @@ DataNode getDataNode() {\n   synchronized void verifyAndSetNamespaceInfo(NamespaceInfo nsInfo) throws IOException {\n     if (this.bpNSInfo == null) {\n       this.bpNSInfo = nsInfo;\n-      \n+      boolean success = false;\n+\n       // Now that we know the namespace ID, etc, we can pass this to the DN.\n       // The DN can now initialize its local storage if we are the\n       // first BP to handshake, etc.\n-      dn.initBlockPool(this);\n-      return;\n+      try {\n+        dn.initBlockPool(this);\n+        success = true;\n+      } finally {\n+        if (!success) {\n+          // The datanode failed to initialize the BP. We need to reset\n+          // the namespace info so that other BPService actors still have\n+          // a chance to set it, and re-initialize the datanode.\n+          this.bpNSInfo = null;\n+        }\n+      }\n     } else {\n       checkNSEquality(bpNSInfo.getBlockPoolID(), nsInfo.getBlockPoolID(),\n           \"Blockpool ID\");",
                "raw_url": "https://github.com/apache/hadoop-common/raw/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "sha": "e646be9a650bd13d88caf1a6c51cbeda26d47173",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/hadoop-common/blob/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java?ref=eb3bdbb0040eceb2881bdecba8a9900e5759a4ee",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "patch": "@@ -25,7 +25,9 @@\n import java.io.File;\n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -294,6 +296,47 @@ public void testPickActiveNameNode() throws Exception {\n     }\n   }\n \n+  /**\n+   * Test datanode block pool initialization error handling.\n+   * Failure in initializing a block pool should not cause NPE.\n+   */\n+  @Test\n+  public void testBPInitErrorHandling() throws Exception {\n+    final DataNode mockDn = Mockito.mock(DataNode.class);\n+    Mockito.doReturn(true).when(mockDn).shouldRun();\n+    Configuration conf = new Configuration();\n+    File dnDataDir = new File(\n+      new File(TEST_BUILD_DATA, \"testBPInitErrorHandling\"), \"data\");\n+    conf.set(DFS_DATANODE_DATA_DIR_KEY, dnDataDir.toURI().toString());\n+    Mockito.doReturn(conf).when(mockDn).getConf();\n+    Mockito.doReturn(new DNConf(conf)).when(mockDn).getDnConf();\n+    Mockito.doReturn(DataNodeMetrics.create(conf, \"fake dn\")).\n+      when(mockDn).getMetrics();\n+    final AtomicInteger count = new AtomicInteger();\n+    Mockito.doAnswer(new Answer<Void>() {\n+      @Override\n+      public Void answer(InvocationOnMock invocation) throws Throwable {\n+        if (count.getAndIncrement() == 0) {\n+          throw new IOException(\"faked initBlockPool exception\");\n+        }\n+        // The initBlockPool is called again. Now mock init is done.\n+        Mockito.doReturn(mockFSDataset).when(mockDn).getFSDataset();\n+        return null;\n+      }\n+    }).when(mockDn).initBlockPool(Mockito.any(BPOfferService.class));\n+    BPOfferService bpos = setupBPOSForNNs(mockDn, mockNN1, mockNN2);\n+    bpos.start();\n+    try {\n+      waitForInitialization(bpos);\n+      List<BPServiceActor> actors = bpos.getBPServiceActors();\n+      assertEquals(1, actors.size());\n+      BPServiceActor actor = actors.get(0);\n+      waitForBlockReport(actor.getNameNodeProxy());\n+    } finally {\n+      bpos.stop();\n+    }\n+  }\n+\n   private void waitForOneToFail(final BPOfferService bpos)\n       throws Exception {\n     GenericTestUtils.waitFor(new Supplier<Boolean>() {\n@@ -311,6 +354,11 @@ public Boolean get() {\n    */\n   private BPOfferService setupBPOSForNNs(\n       DatanodeProtocolClientSideTranslatorPB ... nns) throws IOException {\n+    return setupBPOSForNNs(mockDn, nns);\n+  }\n+\n+  private BPOfferService setupBPOSForNNs(DataNode mockDn,\n+      DatanodeProtocolClientSideTranslatorPB ... nns) throws IOException {\n     // Set up some fake InetAddresses, then override the connectToNN\n     // function to return the corresponding proxies.\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "sha": "c5cb6c77f129e8e7a61507403342d8fab2b89505",
                "status": "modified"
            }
        ],
        "message": "HDFS-4201. NPE in BPServiceActor#sendHeartBeat (jxiang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1550269 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/3bea061a58db747dd0fbdd920dc8f478e44bf9e5",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBPOfferService.java"
        ]
    },
    "hadoop-common_ed0eae2": {
        "bug_id": "hadoop-common_ed0eae2",
        "commit": "https://github.com/apache/hadoop-common/commit/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt?ref=ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "patch": "@@ -120,3 +120,6 @@ HDFS-5535 subtasks:\n \n     HDFS-6029. Secondary NN fails to checkpoint after -rollingUpgrade prepare.\n     (jing9)\n+\n+    HDFS-6032. -rollingUpgrade query hits NPE after the NN restarts. (Haohui Mai\n+    via jing9)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "sha": "002afcd44b67682655424e7e1ddb1daae4b83cab",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java?ref=ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "patch": "@@ -881,9 +881,12 @@ private void loadFSImage(File imageFile, FSNamesystem target,\n    */\n   private void loadFSImage(File curFile, MD5Hash expectedMd5,\n       FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n+    // BlockPoolId is required when the FsImageLoader loads the rolling upgrade\n+    // information. Make sure the ID is properly set.\n+    target.setBlockPoolId(this.getBlockPoolID());\n+\n     FSImageFormat.LoaderDelegator loader = FSImageFormat.newLoader(conf, target);\n     loader.load(curFile);\n-    target.setBlockPoolId(this.getBlockPoolID());\n \n     // Check that the image digest we loaded matches up with what\n     // we expected",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "sha": "e79b6c9224665352c3ec945cab6ce37ad4b58b39",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java?ref=ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "patch": "@@ -432,6 +432,32 @@ public void testQuery() throws Exception {\n     }\n   }\n \n+  @Test (timeout = 300000)\n+  public void testQueryAfterRestart() throws IOException, InterruptedException {\n+    final Configuration conf = new Configuration();\n+    MiniDFSCluster cluster = null;\n+    try {\n+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();\n+      cluster.waitActive();\n+      DistributedFileSystem dfs = cluster.getFileSystem();\n+\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+      // start rolling upgrade\n+      dfs.rollingUpgrade(RollingUpgradeAction.PREPARE);\n+      queryForPreparation(dfs);\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+      dfs.saveNamespace();\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);\n+\n+      cluster.restartNameNodes();\n+      dfs.rollingUpgrade(RollingUpgradeAction.QUERY);\n+    } finally {\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+    }\n+  }\n+\n   @Test(timeout = 300000)\n   public void testCheckpoint() throws IOException, InterruptedException {\n     final Configuration conf = new Configuration();",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "sha": "321f8843d0f6f9091f76a43a50ea0d8ff9405171",
                "status": "modified"
            }
        ],
        "message": "HDFS-6032. -rollingUpgrade query hits NPE after the NN restarts. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1572801 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/bfecb2a3b99d46e8d9b57a23222524ed33e3d78e",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSImage.java"
        ]
    },
    "hadoop-common_ee85268": {
        "bug_id": "hadoop-common_ee85268",
        "commit": "https://github.com/apache/hadoop-common/commit/ee852685a827dcd7e077b81bf8c4ba95ec5464a8",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=ee852685a827dcd7e077b81bf8c4ba95ec5464a8",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -19,3 +19,5 @@ HDFS-2231. Configuration changes for HA namenode. (suresh)\n HDFS-2418. Change ConfiguredFailoverProxyProvider to take advantage of HDFS-2231. (atm)\n \n HDFS-2393. Mark appropriate methods of ClientProtocol with the idempotent annotation. (atm)\n+\n+HDFS-2523. Small NN fixes to include HAServiceProtocol and prevent NPE on shutdown. (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "37e6e4acac308b051fc5814e9a4d03cc957ddcf4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=ee852685a827dcd7e077b81bf8c4ba95ec5464a8",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -572,7 +572,9 @@ public void stop() {\n       stopRequested = true;\n     }\n     try {\n-      state.exitState(haContext);\n+      if (state != null) {\n+        state.exitState(haContext);\n+      }\n     } catch (ServiceFailedException e) {\n       LOG.warn(\"Encountered exception while exiting state \", e);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "4eb080105f050c928e37e4d27e2071eafa6753b7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java?ref=ee852685a827dcd7e077b81bf8c4ba95ec5464a8",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.hadoop.fs.permission.PermissionStatus;\n import static org.apache.hadoop.hdfs.DFSConfigKeys.*;\n \n+import org.apache.hadoop.ha.HAServiceProtocol;\n import org.apache.hadoop.ha.HealthCheckFailedException;\n import org.apache.hadoop.ha.ServiceFailedException;\n import org.apache.hadoop.hdfs.HDFSPolicyProvider;\n@@ -156,6 +157,7 @@ public NameNodeRpcServer(Configuration conf, NameNode nn)\n     this.server.addProtocol(RefreshAuthorizationPolicyProtocol.class, this);\n     this.server.addProtocol(RefreshUserMappingsProtocol.class, this);\n     this.server.addProtocol(GetUserMappingsProtocol.class, this);\n+    this.server.addProtocol(HAServiceProtocol.class, this);\n     \n \n     // set service-level authorization security policy\n@@ -225,6 +227,8 @@ public long getProtocolVersion(String protocol,\n       return RefreshUserMappingsProtocol.versionID;\n     } else if (protocol.equals(GetUserMappingsProtocol.class.getName())){\n       return GetUserMappingsProtocol.versionID;\n+    } else if (protocol.equals(HAServiceProtocol.class.getName())) {\n+      return HAServiceProtocol.versionID;\n     } else {\n       throw new IOException(\"Unknown protocol to name node: \" + protocol);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "sha": "6546b8fe06b07bfe73b85c60f3e143e84e847717",
                "status": "modified"
            }
        ],
        "message": "HDFS-2523. Small NN fixes to include HAServiceProtocol and prevent NPE on shutdown. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1195753 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/32851dcf1b0b95b2552fea1071fa8f78944fb67f",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestNameNodeRpcServer.java"
        ]
    },
    "hadoop-common_f00495a": {
        "bug_id": "hadoop-common_f00495a",
        "commit": "https://github.com/apache/hadoop-common/commit/f00495a4baaa6c3243f9519c4e19c12f07f8ae26",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=f00495a4baaa6c3243f9519c4e19c12f07f8ae26",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -59,6 +59,9 @@ Release 2.1.0-alpha - Unreleased\n     YARN-79. Implement close on all clients to YARN so that RPC clients don't\n     throw exceptions on shut-down. (Vinod Kumar Vavilapalli)\n \n+    YARN-42. Modify NM's non-aggregating logs' handler to stop properly so that\n+    NMs don't get NPEs on startup errors. (Devaraj K via vinodkv)\n+\n Release 0.23.4 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/CHANGES.txt",
                "sha": "8106a37f4a93cbc674e5f820b46b8ae0308f07c7",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java?ref=f00495a4baaa6c3243f9519c4e19c12f07f8ae26",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "patch": "@@ -79,16 +79,18 @@ public void init(Configuration conf) {\n \n   @Override\n   public void stop() {\n-    sched.shutdown();\n-    boolean isShutdown = false;\n-    try {\n-      isShutdown = sched.awaitTermination(10, TimeUnit.SECONDS);\n-    } catch (InterruptedException e) {\n-      sched.shutdownNow();\n-      isShutdown = true;\n-    }\n-    if (!isShutdown) {\n-      sched.shutdownNow();\n+    if (sched != null) {\n+      sched.shutdown();\n+      boolean isShutdown = false;\n+      try {\n+        isShutdown = sched.awaitTermination(10, TimeUnit.SECONDS);\n+      } catch (InterruptedException e) {\n+        sched.shutdownNow();\n+        isShutdown = true;\n+      }\n+      if (!isShutdown) {\n+        sched.shutdownNow();\n+      }\n     }\n     super.stop();\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "sha": "7ec634b0ebe59ffd65ce3de334d67a3902be4e6e",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java?ref=f00495a4baaa6c3243f9519c4e19c12f07f8ae26",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "patch": "@@ -183,6 +183,24 @@ public void testDelayedDelete() {\n     verify(mockSched).schedule(any(Runnable.class), eq(10800l),\n         eq(TimeUnit.SECONDS));\n   }\n+  \n+  @Test\n+  public void testStop() throws Exception {\n+    NonAggregatingLogHandler aggregatingLogHandler = \n+        new NonAggregatingLogHandler(null, null, null);\n+\n+    // It should not throw NullPointerException\n+    aggregatingLogHandler.stop();\n+\n+    NonAggregatingLogHandlerWithMockExecutor logHandler = \n+        new NonAggregatingLogHandlerWithMockExecutor(null, null, null);\n+    logHandler.init(new Configuration());\n+    logHandler.stop();\n+    verify(logHandler.mockSched).shutdown();\n+    verify(logHandler.mockSched)\n+        .awaitTermination(eq(10l), eq(TimeUnit.SECONDS));\n+    verify(logHandler.mockSched).shutdownNow();\n+  }\n \n   private class NonAggregatingLogHandlerWithMockExecutor extends\n       NonAggregatingLogHandler {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "sha": "36251e47e129c85380a287a665c8ca475ad71f27",
                "status": "modified"
            }
        ],
        "message": "YARN-42. Modify NM's non-aggregating logs' handler to stop properly so that NMs don't get NPEs on startup errors. Contributed by Devaraj K.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1380954 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d123c4c0d828634756c39e67f13c0efc0984df10",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestNonAggregatingLogHandler.java"
        ]
    },
    "hadoop-common_f56d99c": {
        "bug_id": "hadoop-common_f56d99c",
        "commit": "https://github.com/apache/hadoop-common/commit/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -561,6 +561,8 @@ Release 2.1.0-beta - UNRELEASED\n \n     HDFS-4382. Fix typo MAX_NOT_CHANGED_INTERATIONS. (Ted Yu via suresh)\n \n+    HDFS-4840. ReplicationMonitor gets NPE during shutdown. (kihwal)\n+\n   BREAKDOWN OF HDFS-347 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "a85341ec5480ca225f9dfb62af0e0a0ffabd4601",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -3094,10 +3094,15 @@ public void run() {\n           computeDatanodeWork();\n           processPendingReplications();\n           Thread.sleep(replicationRecheckInterval);\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"ReplicationMonitor thread received InterruptedException.\", ie);\n-          break;\n         } catch (Throwable t) {\n+          if (!namesystem.isRunning()) {\n+            LOG.info(\"Stopping ReplicationMonitor.\");\n+            if (!(t instanceof InterruptedException)) {\n+              LOG.info(\"ReplicationMonitor received an exception\"\n+                  + \" while shutting down.\", t);\n+            }\n+            break;\n+          }\n           LOG.fatal(\"ReplicationMonitor thread received Runtime exception. \", t);\n           terminate(1, t);\n         }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "46809da5cb1dca4eaeaef0696b662f86e6bdd197",
                "status": "modified"
            }
        ],
        "message": "HDFS-4840. ReplicationMonitor gets NPE during shutdown. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489634 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/60f27393764d63af7b14b66e106ee9055385873f",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop-common_f5822bd": {
        "bug_id": "hadoop-common_f5822bd",
        "commit": "https://github.com/apache/hadoop-common/commit/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -629,3 +629,6 @@ Trunk (unreleased changes)\n \n     MAPREDUCE-971. distcp does not always remove distcp.tmp.dir. (Aaron Kimball\n     via tomwhite)\n+\n+    MAPREDUCE-995. Fix a bug in JobHistory where tasks completing after the job\n+    is closed cause a NPE. (Jothi Padmanabhan via cdouglas)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/CHANGES.txt",
                "sha": "2464a02711e43999d7b52749b7bc0dfc84e75831",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java?ref=f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
                "deletions": 27,
                "filename": "src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java",
                "patch": "@@ -21,7 +21,7 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Date;\n+import java.util.Collections;\n import java.util.EnumSet;\n import java.util.HashMap;\n import java.util.List;\n@@ -55,7 +55,8 @@\n   final Log LOG = LogFactory.getLog(JobHistory.class);\n \n   private long jobHistoryBlockSize;\n-  private Map<JobID, MetaInfo> fileMap;\n+  private final Map<JobID, MetaInfo> fileMap =\n+    Collections.<JobID,MetaInfo>synchronizedMap(new HashMap<JobID,MetaInfo>());\n   private ThreadPoolExecutor executor = null;\n   static final FsPermission HISTORY_DIR_PERMISSION =\n     FsPermission.createImmutable((short) 0750); // rwxr-x---\n@@ -115,8 +116,6 @@ public void init(JobTracker jt, JobConf conf, String hostname,\n           3 * 1024 * 1024);\n     \n     jobTracker = jt;\n-    \n-    fileMap = new HashMap<JobID, MetaInfo> ();\n   }\n   \n   /** Initialize the done directory and start the history cleaner thread */\n@@ -305,40 +304,28 @@ public void setupEventWriter(JobID jobId, JobConf jobConf)\n   /** Close the event writer for this id */\n   public void closeWriter(JobID id) {\n     try {\n-      EventWriter writer = getWriter(id);\n-      writer.close();\n+      final MetaInfo mi = fileMap.get(id);\n+      if (mi != null) {\n+        mi.closeWriter();\n+      }\n     } catch (IOException e) {\n       LOG.info(\"Error closing writer for JobID: \" + id);\n     }\n   }\n \n-\n-  /**\n-   * Get the JsonEventWriter for the specified Job Id\n-   * @param jobId\n-   * @return\n-   * @throws IOException if a writer is not available\n-   */\n-  private EventWriter getWriter(final JobID jobId) throws IOException {\n-    EventWriter writer = null;\n-    MetaInfo mi = fileMap.get(jobId);\n-    if (mi == null || (writer = mi.getEventWriter()) == null) {\n-      throw new IOException(\"History File does not exist for JobID\");\n-    }\n-    return writer;\n-  }\n-\n   /**\n    * Method to log the specified event\n    * @param event The event to log\n    * @param id The Job ID of the event\n    */\n   public void logEvent(HistoryEvent event, JobID id) {\n     try {\n-      EventWriter writer = getWriter(id);\n-      writer.write(event);\n+      final MetaInfo mi = fileMap.get(id);\n+      if (mi != null) {\n+        mi.writeEvent(event);\n+      }\n     } catch (IOException e) {\n-      LOG.error(\"Error creating writer, \" + e.getMessage());\n+      LOG.error(\"Error Logging event, \" + e.getMessage());\n     }\n   }\n \n@@ -388,7 +375,7 @@ private void moveOldFiles() throws IOException {\n   \n   private void moveToDone(final JobID id) {\n     final List<Path> paths = new ArrayList<Path>();\n-    MetaInfo metaInfo = fileMap.get(id);\n+    final MetaInfo metaInfo = fileMap.get(id);\n     if (metaInfo == null) {\n       LOG.info(\"No file for job-history with \" + id + \" found in cache!\");\n       return;\n@@ -456,7 +443,19 @@ private String getUserName(JobConf jobConf) {\n \n     Path getHistoryFile() { return historyFile; }\n     Path getConfFile() { return confFile; }\n-    EventWriter getEventWriter() { return writer; }\n+\n+    synchronized void closeWriter() throws IOException {\n+      if (writer != null) {\n+        writer.close();\n+      }\n+      writer = null;\n+    }\n+\n+    synchronized void writeEvent(HistoryEvent event) throws IOException {\n+      if (writer != null) {\n+        writer.write(event);\n+      }\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java",
                "sha": "77644667b2c2acaa1e2fc6bf9f25fd566d132a65",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java",
                "changes": 116,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java?ref=f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java",
                "patch": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.mapred;\n+\n+import java.io.IOException;\n+\n+import junit.framework.TestCase;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapreduce.Counters;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.hadoop.mapreduce.jobhistory.JobFinishedEvent;\n+import org.apache.hadoop.mapreduce.jobhistory.JobHistory;\n+import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser;\n+import org.apache.hadoop.mapreduce.jobhistory.JobSubmittedEvent;\n+import org.apache.hadoop.mapreduce.jobhistory.TaskFinishedEvent;\n+\n+/**\n+ * Unit test to test if the JobHistory writer/parser is able to handle\n+ * values with special characters\n+ * This test also tests if the job history module is able to gracefully\n+ * ignore events after the event writer is closed\n+ *\n+ */\n+public class TestJobHistoryParsing  extends TestCase {\n+\n+  public void testHistoryParsing() throws IOException {\n+    // open a test history file\n+    Path historyDir = new Path(System.getProperty(\"test.build.data\", \".\"),\n+                                \"history\");\n+    JobConf conf = new JobConf();\n+    conf.set(\"hadoop.job.history.location\", historyDir.toString());\n+    FileSystem fs = FileSystem.getLocal(new JobConf());\n+\n+    // Some weird strings\n+    String username = \"user\";\n+    String weirdJob = \"Value has \\n new line \\n and \" +\n+                    \"dot followed by new line .\\n in it +\" +\n+                    \"ends with escape\\\\\";\n+    String weirdPath = \"Value has characters: \" +\n+                    \"`1234567890-=qwertyuiop[]\\\\asdfghjkl;'zxcvbnm,./\" +\n+                    \"~!@#$%^&*()_+QWERTYUIOP{}|ASDFGHJKL:\\\"'ZXCVBNM<>?\" +\n+                    \"\\t\\b\\n\\f\\\"\\n in it\";\n+\n+    conf.setUser(username);\n+\n+    MiniMRCluster mr = null;\n+    mr = new MiniMRCluster(2, \"file:///\", 3, null, null, conf);\n+\n+    JobTracker jt = mr.getJobTrackerRunner().getJobTracker();\n+    JobHistory jh = jt.getJobHistory();\n+\n+    jh.init(jt, conf, \"localhost\", 1234);\n+    JobID jobId = JobID.forName(\"job_200809171136_0001\");\n+    jh.setupEventWriter(jobId, conf);\n+    JobSubmittedEvent jse =\n+      new JobSubmittedEvent(jobId, weirdJob, username, 12345, weirdPath);\n+    jh.logEvent(jse, jobId);\n+\n+    JobFinishedEvent jfe =\n+      new JobFinishedEvent(jobId, 12346, 1, 1, 0, 0, new Counters());\n+    jh.logEvent(jfe, jobId);\n+    jh.closeWriter(jobId);\n+\n+    // Try to write one more event now, should not fail\n+    TaskID tid = TaskID.forName(\"task_200809171136_0001_m_000002\");\n+    TaskFinishedEvent tfe =\n+      new TaskFinishedEvent(tid, 0, TaskType.MAP, \"\", null);\n+    boolean caughtException = false;\n+\n+    try {\n+      jh.logEvent(tfe, jobId);\n+    } catch (Exception e) {\n+      caughtException = true;\n+    }\n+\n+    assertFalse(\"Writing an event after closing event writer is not handled\",\n+        caughtException);\n+\n+    String historyFileName = jobId.toString() + \"_\" + username;\n+    Path historyFilePath = new Path (historyDir.toString(),\n+      historyFileName);\n+\n+    System.out.println(\"History File is \" + historyFilePath.toString());\n+\n+    JobHistoryParser parser =\n+      new JobHistoryParser(fs, historyFilePath);\n+\n+    JobHistoryParser.JobInfo jobInfo = parser.parse();\n+\n+    assertTrue (jobInfo.getUsername().equals(username));\n+    assertTrue(jobInfo.getJobname().equals(weirdJob));\n+    assertTrue(jobInfo.getJobConfPath().equals(weirdPath));\n+\n+    if (mr != null) {\n+      mr.shutdown();\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java",
                "sha": "5dc445ec0633fe0f9529f9376fb55ef007a9afcc",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-995. Fix a bug in JobHistory where tasks completing after the job\nis closed cause a NPE. Contributed by Jothi Padmanabhan\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@816454 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/c82125320349962f05edc66d819c00db2681db97",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobHistory.java"
        ]
    },
    "hadoop-common_fb7ece9": {
        "bug_id": "hadoop-common_fb7ece9",
        "commit": "https://github.com/apache/hadoop-common/commit/fb7ece969d62f859de9fdc18941b53b816760161",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=fb7ece969d62f859de9fdc18941b53b816760161",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -126,6 +126,9 @@ Release 2.4.1 - UNRELEASED\n     YARN-1928. Fixed a race condition in TestAMRMRPCNodeUpdates which caused it\n     to fail occassionally. (Zhijie Shen via vinodkv)\n \n+    YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling\n+    Disconnected event from ZK. (Karthik Kambatla via jianhe)\n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/CHANGES.txt",
                "sha": "ed12a7bc8acf7df38decfbfa771b2a7dea07b881",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java?ref=fb7ece969d62f859de9fdc18941b53b816760161",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "patch": "@@ -280,10 +280,9 @@ public String run() throws KeeperException, InterruptedException {\n     }\n   }\n \n-  private void logRootNodeAcls(String prefix) throws KeeperException,\n-      InterruptedException {\n+  private void logRootNodeAcls(String prefix) throws Exception {\n     Stat getStat = new Stat();\n-    List<ACL> getAcls = zkClient.getACL(zkRootNodePath, getStat);\n+    List<ACL> getAcls = getACLWithRetries(zkRootNodePath, getStat);\n \n     StringBuilder builder = new StringBuilder();\n     builder.append(prefix);\n@@ -363,7 +362,7 @@ protected synchronized void storeVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n     byte[] data =\n         ((RMStateVersionPBImpl) CURRENT_VERSION_INFO).getProto().toByteArray();\n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       setDataWithRetries(versionNodePath, data, -1);\n     } else {\n       createWithRetries(versionNodePath, data, zkAcl, CreateMode.PERSISTENT);\n@@ -374,7 +373,7 @@ protected synchronized void storeVersion() throws Exception {\n   protected synchronized RMStateVersion loadVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n \n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       byte[] data = getDataWithRetries(versionNodePath, true);\n       RMStateVersion version =\n           new RMStateVersionPBImpl(RMStateVersionProto.parseFrom(data));\n@@ -442,7 +441,8 @@ private void loadRMSequentialNumberState(RMState rmState) throws Exception {\n   }\n \n   private void loadRMDelegationTokenState(RMState rmState) throws Exception {\n-    List<String> childNodes = zkClient.getChildren(delegationTokensRootPath, true);\n+    List<String> childNodes =\n+        getChildrenWithRetries(delegationTokensRootPath, true);\n     for (String childNodeName : childNodes) {\n       String childNodePath =\n           getNodePath(delegationTokensRootPath, childNodeName);\n@@ -567,7 +567,7 @@ public synchronized void updateApplicationStateInternal(ApplicationId appId,\n     }\n     byte[] appStateData = appStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, appStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, appStateData, zkAcl,\n@@ -610,7 +610,7 @@ public synchronized void updateApplicationAttemptStateInternal(\n     }\n     byte[] attemptStateData = attemptStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, attemptStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, attemptStateData, zkAcl,\n@@ -661,7 +661,7 @@ protected synchronized void removeRMDelegationTokenState(\n       LOG.debug(\"Removing RMDelegationToken_\"\n           + rmDTIdentifier.getSequenceNumber());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       opList.add(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -677,7 +677,7 @@ protected void updateRMDelegationTokenAndSequenceNumberInternal(\n     String nodeRemovePath =\n         getNodePath(delegationTokensRootPath, DELEGATION_TOKEN_PREFIX\n             + rmDTIdentifier.getSequenceNumber());\n-    if (zkClient.exists(nodeRemovePath, true) == null) {\n+    if (existsWithRetries(nodeRemovePath, true) == null) {\n       // in case znode doesn't exist\n       addStoreOrUpdateOps(\n           opList, rmDTIdentifier, renewDate, latestSequenceNumber, false);\n@@ -760,7 +760,7 @@ protected synchronized void removeRMDTMasterKeyState(\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Removing RMDelegationKey_\" + delegationKey.getKeyId());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       doMultiWithRetries(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -891,6 +891,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private List<ACL> getACLWithRetries(\n+      final String path, final Stat stat) throws Exception {\n+    return new ZKAction<List<ACL>>() {\n+      @Override\n+      public List<ACL> run() throws KeeperException, InterruptedException {\n+        return zkClient.getACL(path, stat);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   private List<String> getChildrenWithRetries(\n       final String path, final boolean watch) throws Exception {\n     return new ZKAction<List<String>>() {\n@@ -901,6 +911,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private Stat existsWithRetries(\n+      final String path, final boolean watch) throws Exception {\n+    return new ZKAction<Stat>() {\n+      @Override\n+      Stat run() throws KeeperException, InterruptedException {\n+        return zkClient.exists(path, watch);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   /**\n    * Helper class that periodically attempts creating a znode to ensure that\n    * this RM continues to be the Active.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "sha": "9b15bb21e7e62a1dab06a6799dd5a39621c06a4f",
                "status": "modified"
            }
        ],
        "message": "YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling Disconnected event from ZK. Contributed by Karthik Kambatla.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1587776 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/31f22748b38fda8442888c8991ba541d7590dbaa",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestZKRMStateStore.java"
        ]
    }
}