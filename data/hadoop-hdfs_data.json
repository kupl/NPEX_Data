[{"commit": "https://github.com/apache/hadoop-hdfs/commit/971a0fe85dddde762ef907e749b6db56f56c5f80", "parent": "https://github.com/apache/hadoop-hdfs/commit/3ed890b58e7d0ec077a36b9a6fa610a517807e00", "message": "HDFS-885. Datanode toString() NPEs on null dnRegistration.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@897679 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hadoop-hdfs_1", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/971a0fe85dddde762ef907e749b6db56f56c5f80/CHANGES.txt", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/971a0fe85dddde762ef907e749b6db56f56c5f80/CHANGES.txt", "sha": "14c915b5ad6d8657749926f7da8b035418856f0d", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=971a0fe85dddde762ef907e749b6db56f56c5f80", "patch": "@@ -88,6 +88,8 @@ Trunk (unreleased changes)\n     (Todd Lipcon via szetszwo)\n     \n     HDFS-775. FSDataset calls getCapacity() twice. (stevel)\n+    \n+    HDFS-885. Datanode toString() NPEs on null dnRegistration. (stevel)\n \n Release 0.21.0 - Unreleased\n ", "filename": "CHANGES.txt"}, {"additions": 4, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/971a0fe85dddde762ef907e749b6db56f56c5f80/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/971a0fe85dddde762ef907e749b6db56f56c5f80/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java", "sha": "ebdac0e2bce44737a7e07bef76c70a0e1fac91fc", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=971a0fe85dddde762ef907e749b6db56f56c5f80", "patch": "@@ -1422,8 +1422,10 @@ public static DataNode makeInstance(String[] dataDirs, Configuration conf)\n   public String toString() {\n     return \"DataNode{\" +\n       \"data=\" + data +\n-      \", localName='\" + dnRegistration.getName() + \"'\" +\n-      \", storageID='\" + dnRegistration.getStorageID() + \"'\" +\n+      (dnRegistration != null ?\n+          (\", localName='\" + dnRegistration.getName() + \"'\" +\n+              \", storageID='\" + dnRegistration.getStorageID() + \"'\")\n+          : \"\") +\n       \", xmitsInProgress=\" + xmitsInProgress.get() +\n       \"}\";\n   }", "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"}], "repo": "hadoop-hdfs"}, {"commit": "https://github.com/apache/hadoop-hdfs/commit/a5e6943c593cc2952744f015426b1af6672a135a", "parent": "https://github.com/apache/hadoop-hdfs/commit/682bd024a199d86e43023eaf1a952d7b85eecfc6", "message": "HDFS-1782. Fix an NPE in RFSNamesystem.startFileInternal(..).  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1087115 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hadoop-hdfs_2", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/a5e6943c593cc2952744f015426b1af6672a135a/CHANGES.txt", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/a5e6943c593cc2952744f015426b1af6672a135a/CHANGES.txt", "sha": "ae520af159c4a1e32822ca61ff846df15247d6aa", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=a5e6943c593cc2952744f015426b1af6672a135a", "patch": "@@ -601,6 +601,9 @@ Release 0.22.0 - Unreleased\n \n     HDFS-1781. Fix the path for jsvc in bin/hdfs.  (John George via szetszwo)\n \n+    HDFS-1782. Fix an NPE in RFSNamesystem.startFileInternal(..).\n+    (John George via szetszwo)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS", "filename": "CHANGES.txt"}, {"additions": 2, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/a5e6943c593cc2952744f015426b1af6672a135a/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/a5e6943c593cc2952744f015426b1af6672a135a/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "sha": "448f11792f49ba644bd4dadf264c39ae263ed547", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=a5e6943c593cc2952744f015426b1af6672a135a", "patch": "@@ -1394,7 +1394,8 @@ private LocatedBlock startFileInternal(String src,\n                 \". Lease recovery is in progress. Try again later.\");\n \n         } else {\n-          if(pendingFile.getLastBlock().getBlockUCState() ==\n+          BlockInfoUnderConstruction lastBlock=pendingFile.getLastBlock();\n+          if(lastBlock != null && lastBlock.getBlockUCState() ==\n             BlockUCState.UNDER_RECOVERY) {\n             throw new RecoveryInProgressException(\n               \"Recovery in progress, file [\" + src + \"], \" +", "filename": "src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"}], "repo": "hadoop-hdfs"}, {"commit": "https://github.com/apache/hadoop-hdfs/commit/be31fd05dae345e62772ecc235f14f3f59c37e04", "parent": "https://github.com/apache/hadoop-hdfs/commit/d1d9265d1758813b62c20157297e8cfd824628ee", "message": "HDFS-1550. NPE when listing a file with no location. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1054807 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hadoop-hdfs_3", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/CHANGES.txt", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/CHANGES.txt", "sha": "71ebb27b84bef74df28bb21ef9fa2bd398da28cc", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=be31fd05dae345e62772ecc235f14f3f59c37e04", "patch": "@@ -471,6 +471,8 @@ Release 0.22.0 - Unreleased\n     HDFS-1560. dfs.data.dir permissions should default to 700. \n     (Todd Lipcon via eli)\n \n+    HDFS-1550. NPE when listing a file with no location. (hairong)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS", "filename": "CHANGES.txt"}, {"additions": 3, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/src/java/org/apache/hadoop/hdfs/DFSUtil.java", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/src/java/org/apache/hadoop/hdfs/DFSUtil.java", "sha": "6975c53a25cd37604d11b0327e0db7a473aa136c", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=be31fd05dae345e62772ecc235f14f3f59c37e04", "patch": "@@ -204,6 +204,9 @@ public static String byteArray2String(byte[][] pathComponents) {\n     }\n     int nrBlocks = blocks.locatedBlockCount();\n     BlockLocation[] blkLocations = new BlockLocation[nrBlocks];\n+    if (nrBlocks == 0) {\n+      return blkLocations;\n+    }\n     int idx = 0;\n     for (LocatedBlock blk : blocks.getLocatedBlocks()) {\n       assert idx < nrBlocks : \"Incorrect index\";", "filename": "src/java/org/apache/hadoop/hdfs/DFSUtil.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java", "sha": "03e6b39c735c4110059773a65fe0ee61de79ec4d", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java?ref=be31fd05dae345e62772ecc235f14f3f59c37e04", "patch": "@@ -19,6 +19,8 @@\n package org.apache.hadoop.hdfs;\n \n import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n \n import java.util.Arrays;\n@@ -65,5 +67,9 @@ public void testLocatedBlocks2Locations() {\n \n     assertTrue(\"expected 1 corrupt files but got \" + corruptCount, \n                corruptCount == 1);\n+    \n+    // test an empty location\n+    bs = DFSUtil.locatedBlocks2Locations(new LocatedBlocks());\n+    assertEquals(0, bs.length);\n   }\n }\n\\ No newline at end of file", "filename": "src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java"}], "repo": "hadoop-hdfs"}, {"commit": "https://github.com/apache/hadoop-hdfs/commit/3407cdcbe9b272421574682332bb634900497ed2", "parent": "https://github.com/apache/hadoop-hdfs/commit/15ce625e5f269cfe06803c54d21d5dff46ab5425", "message": "HDFS-1502. TestBlockRecovery triggers NPE in assert. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1042517 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hadoop-hdfs_4", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/3407cdcbe9b272421574682332bb634900497ed2/CHANGES.txt", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/3407cdcbe9b272421574682332bb634900497ed2/CHANGES.txt", "sha": "bc620cd5cb35b960aa8cb56cc24a0c5a94259f0f", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=3407cdcbe9b272421574682332bb634900497ed2", "patch": "@@ -426,6 +426,8 @@ Release 0.22.0 - Unreleased\n \n     HDFS-1523. TestLargeBlock is failing on trunk. (cos)\n \n+    HDFS-1502. TestBlockRecovery triggers NPE in assert. (hairong via cos)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS", "filename": "CHANGES.txt"}, {"additions": 19, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/3407cdcbe9b272421574682332bb634900497ed2/src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/3407cdcbe9b272421574682332bb634900497ed2/src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java", "sha": "9061781d9c867f6df31677e72067f83b2330bcf5", "changes": 30, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java?ref=3407cdcbe9b272421574682332bb634900497ed2", "patch": "@@ -128,7 +128,8 @@ public void tearDown() throws IOException {\n   private void testSyncReplicas(ReplicaRecoveryInfo replica1, \n       ReplicaRecoveryInfo replica2,\n       InterDatanodeProtocol dn1,\n-      InterDatanodeProtocol dn2) throws IOException {\n+      InterDatanodeProtocol dn2,\n+      long expectLen) throws IOException {\n     \n     DatanodeInfo[] locs = new DatanodeInfo[]{\n         mock(DatanodeInfo.class), mock(DatanodeInfo.class)};\n@@ -141,6 +142,13 @@ private void testSyncReplicas(ReplicaRecoveryInfo replica1,\n         new DatanodeID(\"aa\", \"bb\", 11, 22), dn2, replica2);\n     syncList.add(record1);\n     syncList.add(record2);\n+    \n+    when(dn1.updateReplicaUnderRecovery((Block)anyObject(), anyLong(), \n+        anyLong())).thenReturn(new Block(block.getBlockId(), \n+            expectLen, block.getGenerationStamp()));\n+    when(dn2.updateReplicaUnderRecovery((Block)anyObject(), anyLong(), \n+        anyLong())).thenReturn(new Block(block.getBlockId(), \n+            expectLen, block.getGenerationStamp()));\n     dn.syncBlock(rBlock, syncList);\n   }\n   \n@@ -162,7 +170,7 @@ public void testFinalizedReplicas () throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);    \n \n@@ -173,7 +181,7 @@ public void testFinalizedReplicas () throws IOException {\n         REPLICA_LEN2, GEN_STAMP-2, ReplicaState.FINALIZED);\n \n     try {\n-      testSyncReplicas(replica1, replica2, dn1, dn2);\n+      testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n       Assert.fail(\"Two finalized replicas should not have different lengthes!\");\n     } catch (IOException e) {\n       Assert.assertTrue(e.getMessage().startsWith(\n@@ -201,7 +209,7 @@ public void testFinalizedRbwReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     \n@@ -214,7 +222,7 @@ public void testFinalizedRbwReplicas() throws IOException {\n     dn1 = mock(InterDatanodeProtocol.class);\n     dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);\n@@ -240,7 +248,7 @@ public void testFinalizedRwrReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);\n@@ -254,7 +262,7 @@ public void testFinalizedRwrReplicas() throws IOException {\n     dn1 = mock(InterDatanodeProtocol.class);\n     dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);\n@@ -278,8 +286,8 @@ public void testRBWReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n     long minLen = Math.min(REPLICA_LEN1, REPLICA_LEN2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, minLen);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);    \n   }\n@@ -302,7 +310,7 @@ public void testRBW_RWRReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);    \n@@ -326,9 +334,9 @@ public void testRWRReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n-    \n     long minLen = Math.min(REPLICA_LEN1, REPLICA_LEN2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, minLen);\n+    \n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);    \n   }  ", "filename": "src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java"}], "repo": "hadoop-hdfs"}, {"commit": "https://github.com/apache/hadoop-hdfs/commit/b8d08b808ae20870206e9926d629ef522f4158a7", "parent": "https://github.com/apache/hadoop-hdfs/commit/30349013843f997e30838aa4eede540691e0d805", "message": "HDFS-1202. DataBlockScanner throws NPE when updated before \ninitialized. (Todd Lipcon via dhruba)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@960813 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hadoop-hdfs_5", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b8d08b808ae20870206e9926d629ef522f4158a7/CHANGES.txt", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b8d08b808ae20870206e9926d629ef522f4158a7/CHANGES.txt", "sha": "409b4f7f313eecb4e9fc9a3e8fe1be73a7a0b806", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=b8d08b808ae20870206e9926d629ef522f4158a7", "patch": "@@ -108,6 +108,9 @@ Trunk (unreleased changes)\n     HDFS-1145. When NameNode is shutdown it does not try to exit\n     safemode anymore. (dhruba)\n \n+    HDFS-1202. DataBlockScanner throws NPE when updated before \n+    initialized. (Todd Lipcon via dhruba)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES", "filename": "CHANGES.txt"}, {"additions": 8, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b8d08b808ae20870206e9926d629ef522f4158a7/src/java/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b8d08b808ae20870206e9926d629ef522f4158a7/src/java/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java", "sha": "a2edf6389af72c6ea1dc29230d034f34245e799c", "changes": 13, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java?ref=b8d08b808ae20870206e9926d629ef522f4158a7", "patch": "@@ -158,7 +158,7 @@ public int compareTo(BlockScanInfo other) {\n     dirScanner = new DirectoryScanner(dataset, conf);\n   }\n   \n-  private synchronized boolean isInitiliazed() {\n+  private synchronized boolean isInitialized() {\n     return throttler != null;\n   }\n   \n@@ -267,7 +267,7 @@ private synchronized long getNewBlockScanTime() {\n \n   /** Adds block to list of blocks */\n   synchronized void addBlock(Block block) {\n-    if (!isInitiliazed()) {\n+    if (!isInitialized()) {\n       return;\n     }\n     \n@@ -286,7 +286,7 @@ synchronized void addBlock(Block block) {\n   \n   /** Deletes the block from internal structures */\n   synchronized void deleteBlock(Block block) {\n-    if (!isInitiliazed()) {\n+    if (!isInitialized()) {\n       return;\n     }\n     BlockScanInfo info = blockMap.get(block);\n@@ -297,7 +297,7 @@ synchronized void deleteBlock(Block block) {\n \n   /** @return the last scan time */\n   synchronized long getLastScanTime(Block block) {\n-    if (!isInitiliazed()) {\n+    if (!isInitialized()) {\n       return 0;\n     }\n     BlockScanInfo info = blockMap.get(block);\n@@ -318,6 +318,9 @@ void verifiedByClient(Block block) {\n   private synchronized void updateScanStatus(Block block, \n                                              ScanType type,\n                                              boolean scanOk) {\n+    if (!isInitialized()) {\n+      return;\n+    }\n     BlockScanInfo info = blockMap.get(block);\n     \n     if ( info != null ) {\n@@ -973,7 +976,7 @@ public void doGet(HttpServletRequest request,\n       if (blockScanner == null) {\n         buffer.append(\"Periodic block scanner is not running. \" +\n                       \"Please check the datanode log if this is unexpected.\");\n-      } else if (blockScanner.isInitiliazed()) {\n+      } else if (blockScanner.isInitialized()) {\n         blockScanner.printBlockReport(buffer, summary);\n       } else {\n         buffer.append(\"Periodic block scanner is not yet initialized. \" +", "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/DataBlockScanner.java"}], "repo": "hadoop-hdfs"}, {"commit": "https://github.com/apache/hadoop-hdfs/commit/2426d84a3a13421abb9f48c81e479e935733c3f8", "parent": "https://github.com/apache/hadoop-hdfs/commit/f2809529a9d80906c53ae6931e84255f89ff1f8b", "message": "HDFS-676. Fix NPE in FSDataset.updateReplicaUnderRecovery(). Contributed by Konstantin Shvachko.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@823732 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hadoop-hdfs_6", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/CHANGES.txt", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/CHANGES.txt", "sha": "5abba3a61bbc02969fd3476b07cd9bfd7ae376a4", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=2426d84a3a13421abb9f48c81e479e935733c3f8", "patch": "@@ -390,6 +390,8 @@ Release 0.21.0 - Unreleased\n \n     HDFS-665. TestFileAppend2 sometimes hangs. (hairong)\n \n+    HDFS-676. Fix NPE in FSDataset.updateReplicaUnderRecovery() (shv)\n+\n Release 0.20.1 - 2009-09-01\n \n   IMPROVEMENTS", "filename": "CHANGES.txt"}, {"additions": 13, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java", "sha": "12d08168eac8d3840933a776ea336865423b2838", "changes": 32, "status": "modified", "deletions": 19, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8", "patch": "@@ -1982,19 +1982,21 @@ static ReplicaRecoveryInfo initReplicaRecovery(\n     return rur.createInfo();\n   }\n \n-  /** Update a replica of a block. */\n-  synchronized void updateReplica(final Block block, final long recoveryId,\n-      final long newlength) throws IOException {\n+  @Override // FSDatasetInterface\n+  public synchronized ReplicaInfo updateReplicaUnderRecovery(\n+                                    final Block oldBlock,\n+                                    final long recoveryId,\n+                                    final long newlength) throws IOException {\n     //get replica\n-    final ReplicaInfo replica = volumeMap.get(block.getBlockId());\n-    DataNode.LOG.info(\"updateReplica: block=\" + block\n+    final ReplicaInfo replica = volumeMap.get(oldBlock.getBlockId());\n+    DataNode.LOG.info(\"updateReplica: block=\" + oldBlock\n         + \", recoveryId=\" + recoveryId\n         + \", length=\" + newlength\n         + \", replica=\" + replica);\n \n     //check replica\n     if (replica == null) {\n-      throw new ReplicaNotFoundException(block);\n+      throw new ReplicaNotFoundException(oldBlock);\n     }\n \n     //check replica state\n@@ -2007,26 +2009,18 @@ synchronized void updateReplica(final Block block, final long recoveryId,\n     checkReplicaFiles(replica);\n \n     //update replica\n-    final ReplicaInfo finalized = (ReplicaInfo)updateReplicaUnderRecovery(\n-                                    replica, recoveryId, newlength);\n+    final FinalizedReplica finalized = updateReplicaUnderRecovery(\n+        (ReplicaUnderRecovery)replica, recoveryId, newlength);\n \n     //check replica files after update\n     checkReplicaFiles(finalized);\n+    return finalized;\n   }\n \n-  @Override // FSDatasetInterface\n-  public synchronized FinalizedReplica updateReplicaUnderRecovery(\n-                                          Block oldBlock,\n+  private FinalizedReplica updateReplicaUnderRecovery(\n+                                          ReplicaUnderRecovery rur,\n                                           long recoveryId,\n                                           long newlength) throws IOException {\n-    Replica r = getReplica(oldBlock.getBlockId());\n-    if(r.getState() != ReplicaState.RUR)\n-      throw new IOException(\"Replica \" + r + \" must be under recovery.\");\n-    ReplicaUnderRecovery rur = (ReplicaUnderRecovery)r;\n-    DataNode.LOG.info(\"updateReplicaUnderRecovery: recoveryId=\" + recoveryId\n-        + \", newlength=\" + newlength\n-        + \", rur=\" + rur);\n-\n     //check recovery id\n     if (rur.getRecoveryID() != recoveryId) {\n       throw new IOException(\"rur.getRecoveryID() != recoveryId = \" + recoveryId", "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java", "sha": "462e1fffc573d6230dc402b6e34066fd7dd325f1", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8", "patch": "@@ -350,7 +350,8 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n   /**\n    * Update replica's generation stamp and length and finalize it.\n    */\n-  public FinalizedReplica updateReplicaUnderRecovery(Block oldBlock,\n+  public ReplicaInfo updateReplicaUnderRecovery(\n+                                          Block oldBlock,\n                                           long recoveryId,\n                                           long newLength) throws IOException;\n }", "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java", "sha": "1d9fe16525aab45d39a39a2ff7358ab4650c3c0e", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8", "patch": "@@ -804,10 +804,10 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n     return new ReplicaRecoveryInfo(rBlock.getBlock(), ReplicaState.FINALIZED);\n   }\n \n-  @Override\n+  @Override // FSDatasetInterface\n   public FinalizedReplica updateReplicaUnderRecovery(Block oldBlock,\n-                                          long recoveryId,\n-                                          long newlength) throws IOException {\n+                                        long recoveryId,\n+                                        long newlength) throws IOException {\n     return new FinalizedReplica(\n         oldBlock.getBlockId(), newlength, recoveryId, null, null);\n   }", "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java", "sha": "40d15a6f02c0cd211df29bfad0ebed066a4c5b4c", "changes": 5, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8", "patch": "@@ -234,9 +234,8 @@ public void testUpdateReplicaUnderRecovery() throws IOException {\n       FSDataset.checkReplicaFiles(rur);\n \n       //update\n-      final ReplicaInfo finalized = \n-        (ReplicaInfo)fsdataset.updateReplicaUnderRecovery(\n-            rur, recoveryid, newlength);\n+      final ReplicaInfo finalized = fsdataset.updateReplicaUnderRecovery(\n+                                                rur, recoveryid, newlength);\n \n       //check meta data after update\n       FSDataset.checkReplicaFiles(finalized);", "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java"}], "repo": "hadoop-hdfs"}, {"commit": "https://github.com/apache/hadoop-hdfs/commit/724e51ddd3e483607f1cf966e89c7419608db226", "parent": "https://github.com/apache/hadoop-hdfs/commit/6734001d20bc9831d1f5b2560e1f80df6d944377", "message": "HADOOP-6243. Updating Common and Mapreduce jars fixing an NPE in handling deprecated keys. Contributed by Sreekanth Ramakrishnan.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@812572 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hadoop-hdfs_7", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/724e51ddd3e483607f1cf966e89c7419608db226/CHANGES.txt", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/724e51ddd3e483607f1cf966e89c7419608db226/CHANGES.txt", "sha": "c7e7f925592c14fe3e4663b7d3c3b4c8ecd54b60", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=724e51ddd3e483607f1cf966e89c7419608db226", "patch": "@@ -205,6 +205,9 @@ Trunk (unreleased changes)\n     HDFS-586. TestBlocksWithNotEnoughRacks sometimes fails.\n     (Jitendra Nath Pandey via hairong)\n \n+    HADOOP-6243. Fixed a NullPointerException in handling deprecated keys.\n+    (Sreekanth Ramakrishnan via yhemanth)\n+\n Release 0.20.1 - Unreleased\n \n   IMPROVEMENTS", "filename": "CHANGES.txt"}, {"additions": 0, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/724e51ddd3e483607f1cf966e89c7419608db226/lib/hadoop-core-0.21.0-dev.jar", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/724e51ddd3e483607f1cf966e89c7419608db226/lib/hadoop-core-0.21.0-dev.jar", "sha": "3293cf619ddd9042952f355439dea1070f8cbeac", "changes": 0, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/lib/hadoop-core-0.21.0-dev.jar?ref=724e51ddd3e483607f1cf966e89c7419608db226", "filename": "lib/hadoop-core-0.21.0-dev.jar"}, {"additions": 0, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/724e51ddd3e483607f1cf966e89c7419608db226/lib/hadoop-core-test-0.21.0-dev.jar", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/724e51ddd3e483607f1cf966e89c7419608db226/lib/hadoop-core-test-0.21.0-dev.jar", "sha": "b70ed7e93f16a702896ea33613e4bf349ea68fee", "changes": 0, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/lib/hadoop-core-test-0.21.0-dev.jar?ref=724e51ddd3e483607f1cf966e89c7419608db226", "filename": "lib/hadoop-core-test-0.21.0-dev.jar"}, {"additions": 0, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/724e51ddd3e483607f1cf966e89c7419608db226/lib/hadoop-mapred-0.21.0-dev.jar", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/724e51ddd3e483607f1cf966e89c7419608db226/lib/hadoop-mapred-0.21.0-dev.jar", "sha": "6be3cf22df77c7589d4653fe94d95baf1a6cc6fb", "changes": 0, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/lib/hadoop-mapred-0.21.0-dev.jar?ref=724e51ddd3e483607f1cf966e89c7419608db226", "filename": "lib/hadoop-mapred-0.21.0-dev.jar"}, {"additions": 0, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/724e51ddd3e483607f1cf966e89c7419608db226/lib/hadoop-mapred-examples-0.21.0-dev.jar", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/724e51ddd3e483607f1cf966e89c7419608db226/lib/hadoop-mapred-examples-0.21.0-dev.jar", "sha": "032c4da7f6e478e2713b6346a18fbfef687d7383", "changes": 0, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/lib/hadoop-mapred-examples-0.21.0-dev.jar?ref=724e51ddd3e483607f1cf966e89c7419608db226", "filename": "lib/hadoop-mapred-examples-0.21.0-dev.jar"}, {"additions": 0, "raw_url": "https://github.com/apache/hadoop-hdfs/raw/724e51ddd3e483607f1cf966e89c7419608db226/lib/hadoop-mapred-test-0.21.0-dev.jar", "blob_url": "https://github.com/apache/hadoop-hdfs/blob/724e51ddd3e483607f1cf966e89c7419608db226/lib/hadoop-mapred-test-0.21.0-dev.jar", "sha": "810f6bf0fa267877a6230880a14717ba67b5a841", "changes": 0, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/lib/hadoop-mapred-test-0.21.0-dev.jar?ref=724e51ddd3e483607f1cf966e89c7419608db226", "filename": "lib/hadoop-mapred-test-0.21.0-dev.jar"}], "repo": "hadoop-hdfs"}]
