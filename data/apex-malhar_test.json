{
    "apex-malhar_f06cc02": {
        "repo": "apex-malhar",
        "message": "NullPointerException in hdfs store delete",
        "commit": "https://github.com/apache/apex-malhar/commit/f06cc024af7f3bd55f01f7ce6fb054638d3b0388",
        "parent": "https://github.com/apache/apex-malhar/commit/20096f8d7eb47d0b9b95d0b713f6d17ab4d3375b",
        "bug_id": "apex-malhar_f06cc02",
        "file": [
            {
                "sha": "fb6b82714fff0e6abc41e152b2e0e38a33964c55",
                "filename": "library/src/main/java/com/datatorrent/lib/bucket/HdfsBucketStore.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/f06cc024af7f3bd55f01f7ce6fb054638d3b0388/library/src/main/java/com/datatorrent/lib/bucket/HdfsBucketStore.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/f06cc024af7f3bd55f01f7ce6fb054638d3b0388/library/src/main/java/com/datatorrent/lib/bucket/HdfsBucketStore.java",
                "status": "modified",
                "changes": 16,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/com/datatorrent/lib/bucket/HdfsBucketStore.java?ref=f06cc024af7f3bd55f01f7ce6fb054638d3b0388",
                "patch": "@@ -206,13 +206,15 @@ public void storeBucketData(long window, Map<Integer, Map<Object, T>> data) thro\n   public void deleteBucket(int bucketIdx) throws IOException\n   {\n     Map<Long, Long> windowToOffsetMap = bucketPositions[bucketIdx];\n-    for (Long window : windowToOffsetMap.keySet()) {\n-      Collection<Integer> indices = windowToBuckets.get(window);\n-      indices.remove(bucketIdx);\n-      if (indices.isEmpty()) {\n-        logger.debug(\"deleting file {}\", window);\n-        Path dataFilePath = new Path(bucketRoot + PATH_SEPARATOR + window);\n-        fs.delete(dataFilePath, true);\n+    if (windowToOffsetMap != null) {\n+      for (Long window : windowToOffsetMap.keySet()) {\n+        Collection<Integer> indices = windowToBuckets.get(window);\n+        indices.remove(bucketIdx);\n+        if (indices.isEmpty()) {\n+          logger.debug(\"deleting file {}\", window);\n+          Path dataFilePath = new Path(bucketRoot + PATH_SEPARATOR + window);\n+          fs.delete(dataFilePath, true);\n+        }\n       }\n     }\n     bucketPositions[bucketIdx] = null;",
                "deletions": 7
            }
        ],
        "patched_files": [
            "HdfsBucketStore.java"
        ],
        "unit_tests": [
            "HdfsBucketStoreTest.java"
        ]
    },
    "apex-malhar_1ca48f9": {
        "repo": "apex-malhar",
        "message": "NullPointerException while collecting deduper stats",
        "commit": "https://github.com/apache/apex-malhar/commit/1ca48f932ef49d93b86f8a1d703d3a8631a72b19",
        "parent": "https://github.com/apache/apex-malhar/commit/05038baadb515b95cc73ace93d11d62df84f6d51",
        "bug_id": "apex-malhar_1ca48f9",
        "file": [
            {
                "sha": "4feb8637434e2678a5a2acb57a08f8ed6e9ce8e0",
                "filename": "library/src/main/java/com/datatorrent/lib/dedup/Deduper.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/1ca48f932ef49d93b86f8a1d703d3a8631a72b19/library/src/main/java/com/datatorrent/lib/dedup/Deduper.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/1ca48f932ef49d93b86f8a1d703d3a8631a72b19/library/src/main/java/com/datatorrent/lib/dedup/Deduper.java",
                "status": "modified",
                "changes": 16,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/com/datatorrent/lib/dedup/Deduper.java?ref=1ca48f932ef49d93b86f8a1d703d3a8631a72b19",
                "patch": "@@ -416,13 +416,15 @@ public long getNumDuplicateEvents()\n     public Response processStats(BatchedOperatorStats batchedOperatorStats)\n     {\n       List<Stats.OperatorStats> lastWindowedStats = batchedOperatorStats.getLastWindowedStats();\n-      for (Stats.OperatorStats os : lastWindowedStats) {\n-        if (os.customStats != null) {\n-          if (os.customStats instanceof Counters) {\n-            Counters cs = (Counters) os.customStats;\n-            logger.debug(\"bucketStats {} {} {} {} {} {} {} {} {} {}\", batchedOperatorStats.getOperatorId(), cs.getNumBucketsInMemory(),\n-              cs.getNumDeletedBuckets(), cs.getNumEvictedBuckets(), cs.getNumEventsInMemory(), cs.getNumEventsCommittedPerWindow(),\n-              cs.getNumIgnoredEvents(), cs.getNumDuplicateEvents(), cs.getLow(), cs.getHigh());\n+      if (lastWindowedStats != null) {\n+        for (Stats.OperatorStats os : lastWindowedStats) {\n+          if (os.customStats != null) {\n+            if (os.customStats instanceof Counters) {\n+              Counters cs = (Counters) os.customStats;\n+              logger.debug(\"bucketStats {} {} {} {} {} {} {} {} {} {}\", batchedOperatorStats.getOperatorId(), cs.getNumBucketsInMemory(),\n+                cs.getNumDeletedBuckets(), cs.getNumEvictedBuckets(), cs.getNumEventsInMemory(), cs.getNumEventsCommittedPerWindow(),\n+                cs.getNumIgnoredEvents(), cs.getNumDuplicateEvents(), cs.getLow(), cs.getHigh());\n+            }\n           }\n         }\n       }",
                "deletions": 7
            }
        ],
        "patched_files": [
            "Deduper.java"
        ],
        "unit_tests": [
            "DeduperTest.java"
        ]
    },
    "apex-malhar_5afe241": {
        "repo": "apex-malhar",
        "message": "NPE while closing streams",
        "commit": "https://github.com/apache/apex-malhar/commit/5afe2414dbbc15b8a60c9b27a0eccfdd42ec6e36",
        "parent": "https://github.com/apache/apex-malhar/commit/6b2db4de9285fe926e6ef4ae5d3a701691a9db0f",
        "bug_id": "apex-malhar_5afe241",
        "file": [
            {
                "sha": "73a852d3be269d16d303492679dc7fd433816121",
                "filename": "library/src/main/java/com/datatorrent/lib/bucket/HdfsBucketStore.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/5afe2414dbbc15b8a60c9b27a0eccfdd42ec6e36/library/src/main/java/com/datatorrent/lib/bucket/HdfsBucketStore.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/5afe2414dbbc15b8a60c9b27a0eccfdd42ec6e36/library/src/main/java/com/datatorrent/lib/bucket/HdfsBucketStore.java",
                "status": "modified",
                "changes": 17,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/com/datatorrent/lib/bucket/HdfsBucketStore.java?ref=5afe2414dbbc15b8a60c9b27a0eccfdd42ec6e36",
                "patch": "@@ -370,23 +370,20 @@ public int hashCode()\n     }\n \n     @Override\n-    public Map<Object, T> call() throws Exception\n+    public Map<Object, T> call() throws IOException\n     {\n       Kryo readSerde = new Kryo();\n       readSerde.setClassLoader(classLoader);\n \n       Map<Object, T> bucketDataPerWindow = Maps.newHashMap();\n-      Input input = null;\n-      FSDataInputStream stream = null;\n       FileSystem fs = null;\n       try {\n-        long startTime = System.currentTimeMillis();\n         //Read data only for the fileIds in which bucketIdx had events.\n         Path dataFile = new Path(bucketRoot + PATH_SEPARATOR + window);\n         fs = FileSystem.newInstance(dataFile.toUri(), configuration);\n-        stream = fs.open(dataFile);\n+        FSDataInputStream stream = fs.open(dataFile);\n         stream.seek(bucketPositions[bucketIdx].get(window));\n-        input = new Input(stream);\n+        Input input = new Input(stream);\n \n         int length = stream.readInt();\n \n@@ -411,14 +408,16 @@ else if (keyPasses) {\n             bucketDataPerWindow.put(key, null);\n           }\n         }\n+        input.close();\n+        stream.close();\n       }\n       catch (IOException e) {\n         throw new RuntimeException(e);\n       }\n       finally {\n-        input.close();\n-        stream.close();\n-        fs.close();\n+        if (fs != null) {\n+          fs.close();\n+        }\n       }\n       return bucketDataPerWindow;\n     }",
                "deletions": 9
            }
        ],
        "patched_files": [
            "HdfsBucketStore.java"
        ],
        "unit_tests": [
            "HdfsBucketStoreTest.java"
        ]
    },
    "apex-malhar_ed25960": {
        "repo": "apex-malhar",
        "message": "Fix NPE",
        "commit": "https://github.com/apache/apex-malhar/commit/ed25960e142872827da06f2756bcf6d2adae3a4d",
        "parent": "https://github.com/apache/apex-malhar/commit/212a8af39175099d8ee4035121e9ca9b95cf3d52",
        "bug_id": "apex-malhar_ed25960",
        "file": [
            {
                "sha": "75e206f5b0c23d36382f8374f61ec9742af6581f",
                "filename": "library/src/main/java/com/datatorrent/lib/appdata/datastructs/DimensionalTable.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/ed25960e142872827da06f2756bcf6d2adae3a4d/library/src/main/java/com/datatorrent/lib/appdata/datastructs/DimensionalTable.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/ed25960e142872827da06f2756bcf6d2adae3a4d/library/src/main/java/com/datatorrent/lib/appdata/datastructs/DimensionalTable.java",
                "status": "modified",
                "changes": 5,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/com/datatorrent/lib/appdata/datastructs/DimensionalTable.java?ref=ed25960e142872827da06f2756bcf6d2adae3a4d",
                "patch": "@@ -318,8 +318,11 @@ public DATA getDataPoint(Map<String, ?> keys)\n           columnIndex < tempKeys.size();\n           columnIndex++) {\n         Object key = tempKeys.get(columnIndex);\n+        Object keyColumn = keyColumns.get(columnIndex).get(rowIndex);\n \n-        if(!keyColumns.get(columnIndex).get(rowIndex).equals(key)) {\n+        if((key == null && keyColumn != null) ||\n+           (key != null && keyColumn == null) ||\n+           (key != null && keyColumn != null && !keyColumn.equals(key))) {\n           allEqual = false;\n           break;\n         }",
                "deletions": 1
            }
        ],
        "patched_files": [
            "DimensionalTable.java"
        ],
        "unit_tests": [
            "DimensionalTableTest.java"
        ]
    },
    "apex-malhar_de430a9": {
        "repo": "apex-malhar",
        "message": "Merge pull request #1480 from ilooner/MLHR-1763\n\nFix NPE",
        "commit": "https://github.com/apache/apex-malhar/commit/de430a95f614696a0c00746d6a721f4b3b9b34eb",
        "parent": "https://github.com/apache/apex-malhar/commit/16922f245414e4fb709a14312dee17f8aa93a7a6",
        "bug_id": "apex-malhar_de430a9",
        "file": [
            {
                "sha": "75e206f5b0c23d36382f8374f61ec9742af6581f",
                "filename": "library/src/main/java/com/datatorrent/lib/appdata/datastructs/DimensionalTable.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/de430a95f614696a0c00746d6a721f4b3b9b34eb/library/src/main/java/com/datatorrent/lib/appdata/datastructs/DimensionalTable.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/de430a95f614696a0c00746d6a721f4b3b9b34eb/library/src/main/java/com/datatorrent/lib/appdata/datastructs/DimensionalTable.java",
                "status": "modified",
                "changes": 5,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/com/datatorrent/lib/appdata/datastructs/DimensionalTable.java?ref=de430a95f614696a0c00746d6a721f4b3b9b34eb",
                "patch": "@@ -318,8 +318,11 @@ public DATA getDataPoint(Map<String, ?> keys)\n           columnIndex < tempKeys.size();\n           columnIndex++) {\n         Object key = tempKeys.get(columnIndex);\n+        Object keyColumn = keyColumns.get(columnIndex).get(rowIndex);\n \n-        if(!keyColumns.get(columnIndex).get(rowIndex).equals(key)) {\n+        if((key == null && keyColumn != null) ||\n+           (key != null && keyColumn == null) ||\n+           (key != null && keyColumn != null && !keyColumn.equals(key))) {\n           allEqual = false;\n           break;\n         }",
                "deletions": 1
            }
        ],
        "patched_files": [
            "DimensionalTable.java"
        ],
        "unit_tests": [
            "DimensionalTableTest.java"
        ]
    },
    "apex-malhar_c1fc919": {
        "repo": "apex-malhar",
        "message": "MLHR-1367 #resolve\n\n * Added aggregate counters for AbstractFSDirectoryInputOperator\n * Implemented counters with basic counters\n * added javadoc\n * fixed NullPointerException in stats listener of AbstractThroughputFSDirectoryInputOperator\n * Cleaned up warnings",
        "commit": "https://github.com/apache/apex-malhar/commit/c1fc919a23ff50377899407aaf8910242eb1fbed",
        "parent": "https://github.com/apache/apex-malhar/commit/dbd5238eb321cdcdd5e3336241d8308336fb2270",
        "bug_id": "apex-malhar_c1fc919",
        "file": [
            {
                "sha": "066e42e6e190e979fab43608910400857a1e2e06",
                "filename": "library/src/main/java/com/datatorrent/lib/io/fs/AbstractFSDirectoryInputOperator.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/c1fc919a23ff50377899407aaf8910242eb1fbed/library/src/main/java/com/datatorrent/lib/io/fs/AbstractFSDirectoryInputOperator.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/c1fc919a23ff50377899407aaf8910242eb1fbed/library/src/main/java/com/datatorrent/lib/io/fs/AbstractFSDirectoryInputOperator.java",
                "status": "modified",
                "changes": 336,
                "additions": 298,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/com/datatorrent/lib/io/fs/AbstractFSDirectoryInputOperator.java?ref=c1fc919a23ff50377899407aaf8910242eb1fbed",
                "patch": "@@ -15,13 +15,14 @@\n  */\n package com.datatorrent.lib.io.fs;\n \n+import com.datatorrent.api.Context.CountersAggregator;\n import com.datatorrent.api.Context.OperatorContext;\n import com.datatorrent.api.DefaultPartition;\n import com.datatorrent.api.InputOperator;\n import com.datatorrent.api.Partitioner;\n import com.datatorrent.api.StatsListener;\n+import com.datatorrent.lib.counters.BasicCounters;\n import com.esotericsoftware.kryo.Kryo;\n-import com.esotericsoftware.minlog.Log;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Sets;\n import java.io.FileNotFoundException;\n@@ -33,6 +34,7 @@\n import java.util.regex.Pattern;\n import javax.validation.constraints.NotNull;\n import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang.mutable.MutableLong;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n@@ -77,6 +79,14 @@\n   private int retryCount = 0;\n   private int maxRetryCount = 5;\n   transient protected int skipCount = 0;\n+  private transient OperatorContext context;\n+\n+  protected long globalNumberOfFailures = 0;\n+  protected long localNumberOfFailures = 0;\n+  protected long globalNumberOfRetries = 0;\n+  protected long localNumberOfRetries = 0;\n+  private transient int globalProcessedFileCount = 0;\n+  private transient int localProcessedFileCount = 0;\n \n   /**\n    * Class representing failed file, When read fails on a file in middle, then the file is\n@@ -118,7 +128,145 @@ public String toString()\n           ']';\n     }\n   }\n-  \n+\n+  /**\n+   * Enums for aggregated counters about file processing.\n+   * <p/>\n+   * Contains the enums representing number of files processed, number of\n+   * pending files, number of file errors, and number of retries.\n+   * <p/>\n+   * @since 1.0.4\n+   */\n+  public static enum AggregatedFileCounters\n+  {\n+    /**\n+     * The number of files processed by the logical operator up until this.\n+     * point in time\n+     */\n+    PROCESSED_FILES,\n+    /**\n+     * The number of files waiting to be processed by the logical operator.\n+     */\n+    PENDING_FILES,\n+    /**\n+     * The number of IO errors encountered by the logical operator.\n+     */\n+    NUMBER_OF_ERRORS,\n+    /**\n+     * The number of times the logical operator tried to resume reading a file\n+     * on which it encountered an error.\n+     */\n+    NUMBER_OF_RETRIES;\n+  }\n+\n+  /**\n+   * The enums used to track statistics about the\n+   * AbstractFSDirectoryInputOperator.\n+   */\n+  protected static enum FileCounters\n+  {\n+    /**\n+     * The number of files that were in the processed list up to the last\n+     * repartition of the operator.\n+     */\n+    GLOBAL_PROCESSED_FILES,\n+    /**\n+     * The number of files added to the processed list by the physical operator\n+     * since the last repartition.\n+     */\n+    LOCAL_PROCESSED_FILES,\n+    /**\n+     * The number of io errors encountered up to the last repartition of the\n+     * operator.\n+     */\n+    GLOBAL_NUMBER_OF_FAILURES,\n+    /**\n+     * The number of failures encountered by the physical operator since the\n+     * last repartition.\n+     */\n+    LOCAL_NUMBER_OF_FAILURES,\n+    /**\n+     * The number of retries encountered by the physical operator up to the last\n+     * repartition.\n+     */\n+    GLOBAL_NUMBER_OF_RETRIES,\n+    /**\n+     * The number of retries encountered by the physical operator since the last\n+     * repartition.\n+     */\n+    LOCAL_NUMBER_OF_RETRIES,\n+    /**\n+     * The number of files pending on the physical operator.\n+     */\n+    PENDING_FILES;\n+  }\n+\n+  /**\n+   * A counter aggregator for AbstractFSDirectoryInputOperator.\n+   * <p/>\n+   * In order for this CountersAggregator to be used on your operator, you must\n+   * set it within your application like this.\n+   * <p/>\n+   * <code>\n+   * dag.getOperatorMeta(\"fsinputoperator\").getAttributes().put(OperatorContext.COUNTERS_AGGREGATOR,\n+   *                                                            new AbstractFSDirectoryInputOperator.FileCountersAggregator());\n+   * </code>\n+   * <p/>\n+   * The value of the aggregated counter can be retrieved by issuing a get\n+   * request to the host running your gateway like this.\n+   * <p/>\n+   * <code>\n+   * http://&lt;your host&gt;:9090/ws/v1/applications/&lt;your app id&gt;/logicalPlan/operators/&lt;operatorname&gt;/aggregation\n+   * </code>\n+   * <p/>\n+   * @since 1.0.4\n+   */\n+  public final static class FileCountersAggregator implements CountersAggregator,\n+                                                        Serializable\n+  {\n+    public FileCountersAggregator()\n+    {\n+    }\n+\n+    @Override\n+    public Object aggregate(Collection<?> countersList)\n+    {\n+      if(countersList.isEmpty()) {\n+        return null;\n+      }\n+\n+      BasicCounters<MutableLong> tempFileCounters = (BasicCounters<MutableLong>) countersList.iterator().next();\n+      MutableLong globalProcessedFiles = tempFileCounters.getCounter(FileCounters.GLOBAL_PROCESSED_FILES);\n+      MutableLong globalNumberOfFailures = tempFileCounters.getCounter(FileCounters.GLOBAL_NUMBER_OF_FAILURES);\n+      MutableLong globalNumberOfRetries = tempFileCounters.getCounter(FileCounters.GLOBAL_NUMBER_OF_RETRIES);\n+      MutableLong totalLocalProcessedFiles = new MutableLong(0);\n+      MutableLong pendingFiles = new MutableLong(0);\n+      MutableLong totalLocalNumberOfFailures = new MutableLong(0);\n+      MutableLong totalLocalNumberOfRetries = new MutableLong(0);\n+\n+      for(Object fileCounters: countersList) {\n+        BasicCounters basicFileCounters = (BasicCounters) fileCounters;\n+        totalLocalProcessedFiles.add(basicFileCounters.getCounter(FileCounters.LOCAL_PROCESSED_FILES));\n+        pendingFiles.add(basicFileCounters.getCounter(FileCounters.PENDING_FILES));\n+        totalLocalNumberOfFailures.add(basicFileCounters.getCounter(FileCounters.LOCAL_NUMBER_OF_FAILURES));\n+        totalLocalNumberOfRetries.add(basicFileCounters.getCounter(FileCounters.LOCAL_NUMBER_OF_RETRIES));\n+      }\n+\n+      globalProcessedFiles.add(totalLocalProcessedFiles);\n+      globalProcessedFiles.subtract(pendingFiles);\n+      globalNumberOfFailures.add(totalLocalNumberOfFailures);\n+      globalNumberOfRetries.add(totalLocalNumberOfRetries);\n+\n+      BasicCounters<MutableLong> aggregatedCounters = new BasicCounters(MutableLong.class);\n+      aggregatedCounters.setCounter(AggregatedFileCounters.PROCESSED_FILES, globalProcessedFiles);\n+      aggregatedCounters.setCounter(AggregatedFileCounters.PENDING_FILES, pendingFiles);\n+      aggregatedCounters.setCounter(AggregatedFileCounters.NUMBER_OF_ERRORS, totalLocalNumberOfFailures);\n+      aggregatedCounters.setCounter(AggregatedFileCounters.NUMBER_OF_RETRIES, totalLocalNumberOfRetries);\n+\n+      return aggregatedCounters;\n+    }\n+  }\n+\n   protected long lastRepartition = 0;\n   private transient boolean emit = true;\n   protected boolean idempotentEmit = false;\n@@ -154,46 +302,84 @@ public void setScanner(DirectoryScanner scanner)\n     this.scanner = scanner;\n   }\n \n+  /**\n+   * Returns the frequency with which new files are scanned for in milliseconds.\n+   * @return The scan interval in milliseconds.\n+   */\n   public int getScanIntervalMillis()\n   {\n     return scanIntervalMillis;\n   }\n \n+  /**\n+   * Sets the frequency with which new files are scanned for in milliseconds.\n+   * @param scanIntervalMillis The scan interval in milliseconds.\n+   */\n   public void setScanIntervalMillis(int scanIntervalMillis)\n   {\n     this.scanIntervalMillis = scanIntervalMillis;\n   }\n \n+  /**\n+   * Returns the number of tuples emitted in a batch. If the operator is\n+   * idempotent then this is the number of tuples emitted in a window.\n+   * @return The number of tuples emitted in a batch.\n+   */\n   public int getEmitBatchSize()\n   {\n     return emitBatchSize;\n   }\n \n+  /**\n+   * Sets the number of tuples to emit in a batch. If the operator is\n+   * idempotent then this is the number of tuples emitted in a window.\n+   * @param emitBatchSize The number of tuples to emit in a batch.\n+   */\n   public void setEmitBatchSize(int emitBatchSize)\n   {\n     this.emitBatchSize = emitBatchSize;\n   }\n-  \n+\n+  /**\n+   * Sets whether the operator is idempotent or not.\n+   * @param idempotentEmit If this is true, then the operator\n+   */\n   public void setIdempotentEmit(boolean idempotentEmit)\n   {\n     this.idempotentEmit = idempotentEmit;\n   }\n-  \n+\n+  /**\n+   *\n+   * @return\n+   */\n   public boolean isIdempotentEmit()\n   {\n     return idempotentEmit;\n   }\n-  \n+\n+  /**\n+   * Returns the desired number of partitions.\n+   * @return the desired number of partitions.\n+   */\n   public int getPartitionCount()\n   {\n     return partitionCount;\n   }\n \n+  /**\n+   * Sets the desired number of partitions.\n+   * @param requiredPartitions The desired number of partitions.\n+   */\n   public void setPartitionCount(int requiredPartitions)\n   {\n-    this.partitionCount = requiredPartitions; \n+    this.partitionCount = requiredPartitions;\n   }\n \n+  /**\n+   * Returns the current number of partitions for the operator.\n+   * @return The current number of partitions for the operator.\n+   */\n   public int getCurrentPartitions()\n   {\n     return currentPartitions;\n@@ -202,6 +388,10 @@ public int getCurrentPartitions()\n   @Override\n   public void setup(OperatorContext context)\n   {\n+    globalProcessedFileCount = processedFiles.size();\n+    LOG.debug(\"Setup processed file count: {}\", globalProcessedFileCount);\n+    this.context = context;\n+\n     try {\n       filePath = new Path(directory);\n       configuration = new Configuration();\n@@ -224,11 +414,7 @@ public void setup(OperatorContext context)\n       LOG.info(\"Read offset={} records in setup time={}\", offset, System.currentTimeMillis() - startTime);\n     }\n     catch (IOException ex) {\n-      if(maxRetryCount <= 0) {\n-        throw new RuntimeException(ex);\n-      }\n-      LOG.error(\"FS reader error\", ex);\n-      addToFailedList();\n+      failureHandling(ex);\n     }\n   }\n \n@@ -248,12 +434,39 @@ public void beginWindow(long windowId)\n   @Override\n   public void endWindow()\n   {\n+    if(context != null) {\n+      int pendingFileCount = ((int) pendingFiles.size()) +\n+                             ((int) failedFiles.size()) +\n+                             ((int) unfinishedFiles.size());\n+\n+      if(currentFile != null) {\n+        pendingFileCount++;\n+      }\n+\n+      BasicCounters<MutableLong> fileCounters = new BasicCounters(MutableLong.class);\n+      fileCounters.setCounter(FileCounters.GLOBAL_PROCESSED_FILES,\n+                              new MutableLong(globalProcessedFileCount));\n+      fileCounters.setCounter(FileCounters.LOCAL_PROCESSED_FILES,\n+                              new MutableLong(localProcessedFileCount));\n+      fileCounters.setCounter(FileCounters.GLOBAL_NUMBER_OF_FAILURES,\n+                              new MutableLong(globalNumberOfFailures));\n+      fileCounters.setCounter(FileCounters.LOCAL_NUMBER_OF_FAILURES,\n+                              new MutableLong(localNumberOfFailures));\n+      fileCounters.setCounter(FileCounters.GLOBAL_NUMBER_OF_RETRIES,\n+                              new MutableLong(globalNumberOfRetries));\n+      fileCounters.setCounter(FileCounters.LOCAL_NUMBER_OF_RETRIES,\n+                              new MutableLong(localNumberOfRetries));\n+      fileCounters.setCounter(FileCounters.PENDING_FILES,\n+                              new MutableLong(pendingFileCount));\n+\n+      context.setCounters(fileCounters);\n+    }\n   }\n \n   @Override\n   public void emitTuples()\n   {\n-    //emit will be true if the operator is not idempotent. If the operator is \n+    //emit will be true if the operator is not idempotent. If the operator is\n     //idempotent then emit will be true the first time emitTuples is called\n     //within a window and false the other times emit tuples is called within a\n     //window\n@@ -273,23 +486,10 @@ else if (!failedFiles.isEmpty()) {\n             retryFailedFile(failedFiles.poll());\n           }\n           else {\n-            if (System.currentTimeMillis() - scanIntervalMillis >= lastScanMillis) {\n-              Set<Path> newPaths = scanner.scan(fs, filePath, processedFiles);\n-      \n-              for(Path newPath: newPaths) {\n-                String newPathString = newPath.toString();\n-                pendingFiles.add(newPathString);\n-                processedFiles.add(newPathString);\n-              }\n-              lastScanMillis = System.currentTimeMillis();\n-            }\n+            scanDirectory();\n           }\n         } catch (IOException ex) {\n-          if(maxRetryCount <= 0) {\n-            throw new RuntimeException(ex);\n-          }\n-          LOG.error(\"FS reader error\", ex);\n-          addToFailedList();\n+          failureHandling(ex);\n         }\n       }\n \n@@ -316,11 +516,7 @@ else if (!failedFiles.isEmpty()) {\n               skipCount--;\n           }\n         } catch (IOException e) {\n-          if(maxRetryCount <= 0) {\n-            throw new RuntimeException(e);\n-          }\n-          LOG.error(\"FS reader error\", e);\n-          addToFailedList();\n+          failureHandling(e);\n         }\n       }\n       //If the operator is idempotent, do nothing on other calls to emittuples\n@@ -332,6 +528,39 @@ else if (!failedFiles.isEmpty()) {\n     }\n   }\n \n+  /**\n+   * Scans the directory for new files.\n+   */\n+  protected void scanDirectory()\n+  {\n+    if(System.currentTimeMillis() - scanIntervalMillis >= lastScanMillis) {\n+      Set<Path> newPaths = scanner.scan(fs, filePath, processedFiles);\n+\n+      for(Path newPath : newPaths) {\n+        String newPathString = newPath.toString();\n+        pendingFiles.add(newPathString);\n+        processedFiles.add(newPathString);\n+        localProcessedFileCount++;\n+      }\n+\n+      lastScanMillis = System.currentTimeMillis();\n+    }\n+  }\n+\n+  /**\n+   * Helper method for handling IOExceptions.\n+   * @param e The caught IOException.\n+   */\n+  private void failureHandling(Exception e)\n+  {\n+    localNumberOfFailures++;\n+    if(maxRetryCount <= 0) {\n+      throw new RuntimeException(e);\n+    }\n+    LOG.error(\"FS reader error\", e);\n+    addToFailedList();\n+  }\n+\n   protected void addToFailedList() {\n \n     FailedFile ff = new FailedFile(currentFile, offset, retryCount);\n@@ -341,6 +570,7 @@ protected void addToFailedList() {\n       if (this.inputStream != null)\n         this.inputStream.close();\n     } catch(IOException e) {\n+      localNumberOfFailures++;\n       LOG.error(\"Could not close input stream on: \" + currentFile);\n     }\n \n@@ -356,6 +586,7 @@ protected void addToFailedList() {\n     if (ff.retryCount > maxRetryCount)\n       return;\n \n+    localNumberOfRetries++;\n     LOG.info(\"adding to failed list path {} offset {} retry {}\", ff.path, ff.offset, ff.retryCount);\n     failedFiles.add(ff);\n   }\n@@ -397,15 +628,20 @@ protected void closeFile(InputStream is) throws IOException\n   public Collection<Partition<AbstractFSDirectoryInputOperator<T>>> definePartitions(Collection<Partition<AbstractFSDirectoryInputOperator<T>>> partitions, int incrementalCapacity)\n   {\n     lastRepartition = System.currentTimeMillis();\n-    \n+\n     int totalCount = computedNewPartitionCount(partitions, incrementalCapacity);\n \n     LOG.debug(\"Computed new partitions: {}\", totalCount);\n-    \n+\n     if (totalCount == partitions.size()) {\n       return partitions;\n     }\n-    \n+\n+    AbstractFSDirectoryInputOperator<T> tempOperator = partitions.iterator().next().getPartitionedInstance();\n+\n+    long tempGlobalNumberOfRetries = tempOperator.globalNumberOfRetries;\n+    long tempGlobalNumberOfFailures = tempOperator.globalNumberOfRetries;\n+\n     /*\n      * Build collective state from all instances of the operator.\n      */\n@@ -420,6 +656,8 @@ protected void closeFile(InputStream is) throws IOException\n       totalFailedFiles.addAll(oper.failedFiles);\n       totalPendingFiles.addAll(oper.pendingFiles);\n       currentFiles.addAll(unfinishedFiles);\n+      tempGlobalNumberOfRetries += oper.localNumberOfRetries;\n+      tempGlobalNumberOfFailures += oper.localNumberOfFailures;\n       if (oper.currentFile != null)\n         currentFiles.add(new FailedFile(oper.currentFile, oper.offset));\n       oldscanners.add(oper.getScanner());\n@@ -440,6 +678,10 @@ protected void closeFile(InputStream is) throws IOException\n \n       // Do state transfer for processed files.\n       oper.processedFiles.addAll(totalProcessedFiles);\n+      oper.globalNumberOfFailures = tempGlobalNumberOfRetries;\n+      oper.localNumberOfFailures = 0;\n+      oper.globalNumberOfRetries = tempGlobalNumberOfFailures;\n+      oper.localNumberOfRetries = 0;\n \n       /* redistribute unfinished files properly */\n       oper.unfinishedFiles.clear();\n@@ -481,11 +723,11 @@ protected void closeFile(InputStream is) throws IOException\n     LOG.info(\"definePartitions called returning {} partitions\", newPartitions.size());\n     return newPartitions;\n   }\n-  \n+\n   protected int computedNewPartitionCount(Collection<Partition<AbstractFSDirectoryInputOperator<T>>> partitions, int incrementalCapacity)\n   {\n     boolean isInitialParitition = partitions.iterator().next().getStats() == null;\n-    \n+\n     if (isInitialParitition && partitionCount == 1) {\n       partitionCount = currentPartitions = partitions.size() + incrementalCapacity;\n     } else {\n@@ -506,6 +748,7 @@ public void partitioned(Map<Integer, Partition<AbstractFSDirectoryInputOperator<\n   /**\n    * Read the next item from the stream. Depending on the type of stream, this could be a byte array, line or object.\n    * Upon return of null, the stream will be considered fully consumed.\n+   * @throws IOException\n    */\n   abstract protected T readEntity() throws IOException;\n \n@@ -519,6 +762,7 @@ public void partitioned(Map<Integer, Partition<AbstractFSDirectoryInputOperator<\n   /**\n    * Repartition is required when number of partitions are not equal to required\n    * partitions.\n+   * @param batchedOperatorStats the stats to use when repartitioning.\n    */\n   @Override\n   public Response processStats(BatchedOperatorStats batchedOperatorStats)\n@@ -532,16 +776,32 @@ public Response processStats(BatchedOperatorStats batchedOperatorStats)\n     return res;\n   }\n \n+  /**\n+   * Returns the maximum number of times the operator will attempt to process\n+   * a file on which it encounters an error.\n+   * @return The maximum number of times the operator will attempt to process a\n+   * file on which it encounters an error.\n+   */\n   public int getMaxRetryCount()\n   {\n     return maxRetryCount;\n   }\n \n+  /**\n+   * Sets the maximum number of times the operator will attempt to process\n+   * a file on which it encounters an error.\n+   * @param maxRetryCount The maximum number of times the operator will attempt\n+   * to process a file on which it encounters an error.\n+   */\n   public void setMaxRetryCount(int maxRetryCount)\n   {\n     this.maxRetryCount = maxRetryCount;\n   }\n \n+  /**\n+   * The class that is used to scan for new files in the directory for the\n+   * AbstractFSDirectoryInputOperator.\n+   */\n   public static class DirectoryScanner implements Serializable\n   {\n     private static final long serialVersionUID = 4535844463258899929L;\n@@ -609,7 +869,7 @@ public int getPartitionIndex() {\n         }\n       } catch (FileNotFoundException e) {\n         LOG.warn(\"Failed to list directory {}\", filePath, e);\n-      } catch (Exception e) {\n+      } catch (IOException e) {\n         throw new RuntimeException(e);\n       }\n       return pathSet;",
                "deletions": 38
            },
            {
                "sha": "1670e8bb3067d2454eb833c12028cefc79161d43",
                "filename": "library/src/main/java/com/datatorrent/lib/io/fs/AbstractThroughputFSDirectoryInputOperator.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/c1fc919a23ff50377899407aaf8910242eb1fbed/library/src/main/java/com/datatorrent/lib/io/fs/AbstractThroughputFSDirectoryInputOperator.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/c1fc919a23ff50377899407aaf8910242eb1fbed/library/src/main/java/com/datatorrent/lib/io/fs/AbstractThroughputFSDirectoryInputOperator.java",
                "status": "renamed",
                "changes": 124,
                "additions": 68,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/com/datatorrent/lib/io/fs/AbstractThroughputFSDirectoryInputOperator.java?ref=c1fc919a23ff50377899407aaf8910242eb1fbed",
                "patch": "@@ -15,11 +15,10 @@\n  */\n package com.datatorrent.lib.io.fs;\n \n-import com.datatorrent.api.Context.OperatorContext;\n import com.datatorrent.api.Stats.OperatorStats;\n+import com.datatorrent.lib.counters.BasicCounters;\n import java.util.Collection;\n-import java.util.Set;\n-import org.apache.hadoop.fs.Path;\n+import org.apache.commons.lang.mutable.MutableLong;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -29,110 +28,121 @@\n  * Provides the same functionality as the AbstractFSDirectoryInputOperator\n  * except that this utilized dynamic partitioning where the user can set the\n  * preferred number of pending files per operator as well as the max number of\n- * operators and define a repartition interval. When the repartition interval\n- * passes then a new number of operators are created to accommodate the\n- * remaining pending files.\n+ * operators and define a repartition interval. If a physical operator runs out\n+ * of files to process and an amount of time greater than or equal to the\n+ * repartition interval has passed then a new number of operators are created\n+ * to accommodate the remaining pending files.\n  * <p/>\n- * \n+ *\n  * @since 1.0.4\n  */\n-public abstract class AbstractThroughputHashFSDirectoryInputOperator<T> extends AbstractFSDirectoryInputOperator<T>\n+public abstract class AbstractThroughputFSDirectoryInputOperator<T> extends AbstractFSDirectoryInputOperator<T>\n {\n-  private static final Logger LOG = LoggerFactory.getLogger(AbstractThroughputHashFSDirectoryInputOperator.class);\n+  private static final Logger LOG = LoggerFactory.getLogger(AbstractThroughputFSDirectoryInputOperator.class);\n \n   private long repartitionInterval = 5L * 60L * 1000L;\n   private int preferredMaxPendingFilesPerOperator = 10;\n-  private transient OperatorContext context;\n-  \n-  public void setup(OperatorContext context)\n-  {\n-    super.setup(context);\n-    this.context = context;\n-  }\n-    \n+\n+  /**\n+   * Sets the minimum amount of time that must pass in milliseconds before the\n+   * operator can be repartitioned.\n+   * @param repartitionInterval The minimum amount of time that must pass in\n+   * milliseconds before the operator can be repartitioned.\n+   */\n   public void setRepartitionInterval(long repartitionInterval)\n   {\n     this.repartitionInterval = repartitionInterval;\n   }\n \n+  /**\n+   * Gets the minimum amount of time that must pass in milliseconds before the\n+   * operator can be repartitioned.\n+   * @return The minimum amount of time that must pass in milliseconds before\n+   * the operator can be repartitioned.\n+   */\n   public long getRepartitionInterval()\n   {\n     return repartitionInterval;\n   }\n \n+  /**\n+   * Sets the preferred number of pending files per operator.\n+   * @param pendingFilesPerOperator The preferred number of pending files\n+   * per operator.\n+   */\n   public void setPreferredMaxPendingFilesPerOperator(int pendingFilesPerOperator)\n   {\n     this.preferredMaxPendingFilesPerOperator = pendingFilesPerOperator;\n   }\n \n+  /**\n+   * Returns the preferred number of pending files per operator.\n+   * @return The preferred number of pending files per operator.\n+   */\n   public int getPreferredMaxPendingFilesPerOperator()\n   {\n     return preferredMaxPendingFilesPerOperator;\n   }\n \n+  /**\n+   * Returns the maximum number of partitions for the operator.\n+   * @return The maximum number of partitions for the operator.\n+   */\n   @Override\n-  public void emitTuples()\n+  public int getPartitionCount()\n   {\n-    if(System.currentTimeMillis() - scanIntervalMillis >= lastScanMillis) {\n-      Set<Path> newPaths = scanner.scan(fs, filePath, processedFiles);\n-\n-      for(Path newPath : newPaths) {\n-        String newPathString = newPath.toString();\n-        pendingFiles.add(newPathString);\n-        processedFiles.add(newPathString);\n-      }\n-      lastScanMillis = System.currentTimeMillis();\n-    }\n+    return super.getPartitionCount();\n+  }\n \n-    super.emitTuples();\n+  /**\n+   * Sets the maximum number of partitions for the operator.\n+   * @param requiredPartitions The maximum number of partitions for the\n+   * operator.\n+   */\n+  @Override\n+  public void setPartitionCount(int requiredPartitions)\n+  {\n+    super.setPartitionCount(requiredPartitions);\n   }\n-  \n+\n   @Override\n-  public void endWindow()\n+  public void emitTuples()\n   {\n-    super.endWindow();\n-    \n-    if(context != null) {\n-      int fileCount = failedFiles.size() + \n-                      pendingFiles.size() +\n-                      unfinishedFiles.size();\n-      if(currentFile != null) {\n-        fileCount++;\n-      }\n-      context.setCounters(fileCount);\n-    }\n+    scanDirectory();\n+\n+    super.emitTuples();\n   }\n-  \n+\n   @Override\n   protected int computedNewPartitionCount(Collection<Partition<AbstractFSDirectoryInputOperator<T>>> partitions, int incrementalCapacity)\n   {\n     LOG.debug(\"Called throughput.\");\n     boolean isInitialParitition = partitions.iterator().next().getStats() == null;\n     int newOperatorCount;\n     int totalFileCount = 0;\n-    \n+\n     for(Partition<AbstractFSDirectoryInputOperator<T>> partition : partitions) {\n       AbstractFSDirectoryInputOperator<T> oper = partition.getPartitionedInstance();\n       totalFileCount += oper.failedFiles.size();\n       totalFileCount += oper.pendingFiles.size();\n       totalFileCount += oper.unfinishedFiles.size();\n-      \n+\n       if (oper.currentFile != null) {\n         totalFileCount++;\n       }\n     }\n-    \n+\n     if(!isInitialParitition) {\n       LOG.debug(\"definePartitions: Total File Count: {}\", totalFileCount);\n       newOperatorCount = computeOperatorCount(totalFileCount);\n     }\n     else {\n       newOperatorCount = partitionCount;\n     }\n-    \n+\n     return newOperatorCount;\n   }\n-  \n+\n   private int computeOperatorCount(int totalFileCount)\n   {\n     int newOperatorCount = totalFileCount / preferredMaxPendingFilesPerOperator;\n@@ -151,24 +161,26 @@ private int computeOperatorCount(int totalFileCount)\n   }\n \n   @Override\n+  @SuppressWarnings(\"unchecked\")\n   public Response processStats(BatchedOperatorStats batchedOperatorStats)\n   {\n-    int fileCount = 0;\n-    \n+    BasicCounters<MutableLong> fileCounters = null;\n+\n     for(OperatorStats operatorStats: batchedOperatorStats.getLastWindowedStats()) {\n       if(operatorStats.counters != null) {\n-        fileCount = (Integer) operatorStats.counters;\n+        fileCounters = (BasicCounters<MutableLong>) operatorStats.counters;\n       }\n     }\n-    \n+\n     Response response = new Response();\n-    \n-    if(fileCount > 0 ||\n-      System.currentTimeMillis() - repartitionInterval <= lastRepartition) {\n+\n+    if(fileCounters != null &&\n+       fileCounters.getCounter(FileCounters.PENDING_FILES).longValue() > 0L ||\n+       System.currentTimeMillis() - repartitionInterval <= lastRepartition) {\n       response.repartitionRequired = false;\n       return response;\n     }\n-    \n+\n     response.repartitionRequired = true;\n     return response;\n   }",
                "deletions": 56,
                "previous_filename": "library/src/main/java/com/datatorrent/lib/io/fs/AbstractThroughputHashFSDirectoryInputOperator.java"
            }
        ],
        "patched_files": [
            "AbstractFSDirectoryInputOperator.java"
        ],
        "unit_tests": [
            "AbstractFSDirectoryInputOperatorTest.java"
        ]
    },
    "apex-malhar_c4f788d": {
        "repo": "apex-malhar",
        "message": "APEXMALHAR-2312 Fix NullPointerException for FileSplitterInput Operator if filepath is specified.\n\nProblem Description:\n-------------------\n1) TimeBasedDirectoryScanner threads part of scanservice tries to scan the directories/files.\n2) Each thread checks with help of isIterationCompleted() [referenceTimes] method whether scanned of last iteration are processed by operator thread.\n3) Previously it used to work because HashMap (referenceTimes) used to return null even if last scanned directory path is null.\n4) Recently referenceTimes is changed to ConcurrentHashMap, so get() doesn't allow null key's passed to ConcurrentHashMap get() method.\n5) Hence NullPointerException is seen as if only file path is provided directory path would be empty hence key would be empty.\n\nSolution:\n---------\nPre-check that directory path is null then we have completed last iterations if only filepath is provided.",
        "commit": "https://github.com/apache/apex-malhar/commit/c4f788d5a140bcc14bf9782f4021c0af4712f075",
        "parent": "https://github.com/apache/apex-malhar/commit/ea1b58f72e69ce0e1bfd35eb8b4b6756b1a5e0a0",
        "bug_id": "apex-malhar_c4f788d",
        "file": [
            {
                "sha": "985c6678f7072e197c59d6611475ab8b00a31e65",
                "filename": "library/src/main/java/com/datatorrent/lib/io/fs/FileSplitterInput.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/c4f788d5a140bcc14bf9782f4021c0af4712f075/library/src/main/java/com/datatorrent/lib/io/fs/FileSplitterInput.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/c4f788d5a140bcc14bf9782f4021c0af4712f075/library/src/main/java/com/datatorrent/lib/io/fs/FileSplitterInput.java",
                "status": "modified",
                "changes": 12,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/com/datatorrent/lib/io/fs/FileSplitterInput.java?ref=c4f788d5a140bcc14bf9782f4021c0af4712f075",
                "patch": "@@ -393,6 +393,18 @@ private boolean isIterationCompleted()\n       if (lastScannedInfo == null) { // first iteration started\n         return true;\n       }\n+\n+      LOG.debug(\"Directory path: {} Sub-Directory or File path: {}\", lastScannedInfo.getDirectoryPath(), lastScannedInfo.getFilePath());\n+\n+      /*\n+       * As referenceTimes is now concurrentHashMap, it throws exception if key passed is null.\n+       * So in case where the last scanned directory is null which likely possible when\n+       * only file name is specified instead of directory path.\n+       */\n+      if (lastScannedInfo.getDirectoryPath() == null) {\n+        return true;\n+      }\n+\n       Map<String, Long> referenceTime = referenceTimes.get(lastScannedInfo.getDirectoryPath());\n       if (referenceTime != null) {\n         return referenceTime.get(lastScannedInfo.getFilePath()) != null;",
                "deletions": 0
            }
        ],
        "patched_files": [
            "FileSplitterInput.java"
        ],
        "unit_tests": [
            "FileSplitterInputTest.java"
        ]
    },
    "apex-malhar_216ce1e": {
        "repo": "apex-malhar",
        "message": "Merge pull request #1383 from tushargosavi/SPOI-4606\n\nMLHR-1721: Convert NullPointerException to IOException during seek to avoid operator failure.",
        "commit": "https://github.com/apache/apex-malhar/commit/216ce1e41038a657e3ec94a94bdf9fe77df6e44f",
        "parent": "https://github.com/apache/apex-malhar/commit/5b1b24b1ea175ec0ae5119e3e606710e51c80848",
        "bug_id": "apex-malhar_216ce1e",
        "file": [
            {
                "sha": "e38db886f76287be85665a7dfbc61af312b5d04a",
                "filename": "contrib/src/main/java/com/datatorrent/contrib/hdht/HDHTReader.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/216ce1e41038a657e3ec94a94bdf9fe77df6e44f/contrib/src/main/java/com/datatorrent/contrib/hdht/HDHTReader.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/216ce1e41038a657e3ec94a94bdf9fe77df6e44f/contrib/src/main/java/com/datatorrent/contrib/hdht/HDHTReader.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/contrib/src/main/java/com/datatorrent/contrib/hdht/HDHTReader.java?ref=216ce1e41038a657e3ec94a94bdf9fe77df6e44f",
                "patch": "@@ -377,7 +377,7 @@ protected BucketFileMeta addFile(long bucketKey, Slice startKey)\n   private static class BucketReader implements Closeable\n   {\n     BucketMeta bucketMeta;\n-    final HashMap<String, HDSFileReader> readers = Maps.newHashMap();\n+    final ConcurrentMap<String, HDSFileReader> readers = Maps.newConcurrentMap();\n \n     @Override\n     public void close() throws IOException",
                "deletions": 1
            },
            {
                "sha": "2f06da9e9540e7582fc87bb4bbf3a1e8919e3065",
                "filename": "contrib/src/main/java/com/datatorrent/contrib/hdht/tfile/TFileReader.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/216ce1e41038a657e3ec94a94bdf9fe77df6e44f/contrib/src/main/java/com/datatorrent/contrib/hdht/tfile/TFileReader.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/216ce1e41038a657e3ec94a94bdf9fe77df6e44f/contrib/src/main/java/com/datatorrent/contrib/hdht/tfile/TFileReader.java",
                "status": "modified",
                "changes": 12,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/contrib/src/main/java/com/datatorrent/contrib/hdht/tfile/TFileReader.java?ref=216ce1e41038a657e3ec94a94bdf9fe77df6e44f",
                "patch": "@@ -18,6 +18,7 @@\n import java.io.IOException;\n import java.util.TreeMap;\n \n+import com.datatorrent.common.util.DTThrowable;\n import com.datatorrent.common.util.Slice;\n \n import org.apache.hadoop.conf.Configuration;\n@@ -39,6 +40,7 @@\n   private final Reader reader;\n   private final Scanner scanner;\n   private final FSDataInputStream fsdis;\n+  private boolean closed = false;\n \n   public TFileReader(FSDataInputStream fsdis, long fileLength, Configuration conf) throws IOException\n   {\n@@ -54,6 +56,7 @@ public TFileReader(FSDataInputStream fsdis, long fileLength, Configuration conf)\n   @Override\n   public void close() throws IOException\n   {\n+    closed = true;\n     scanner.close();\n     reader.close();\n     fsdis.close();\n@@ -85,7 +88,14 @@ public void reset() throws IOException\n   @Override\n   public boolean seek(Slice key) throws IOException\n   {\n-    return scanner.seekTo(key.buffer, key.offset, key.length);\n+    try {\n+      return scanner.seekTo(key.buffer, key.offset, key.length);\n+    } catch (NullPointerException ex) {\n+      if (closed)\n+        throw new IOException(\"Stream was closed\");\n+      else\n+        throw ex;\n+    }\n   }\n \n   @Override",
                "deletions": 1
            }
        ],
        "patched_files": [
            "HDHTReader.java"
        ],
        "unit_tests": [
            "HDHTReaderTest.java"
        ]
    },
    "apex-malhar_50e5538": {
        "repo": "apex-malhar",
        "message": "MLHR-1721: Convert NullPointerException to IOException during seek\nto avoid operator failure.",
        "commit": "https://github.com/apache/apex-malhar/commit/50e55384231629b53c9d9c1b435e88962a0377a8",
        "parent": "https://github.com/apache/apex-malhar/commit/e58457012794d1759967b0b28d1f031f244552fc",
        "bug_id": "apex-malhar_50e5538",
        "file": [
            {
                "sha": "e38db886f76287be85665a7dfbc61af312b5d04a",
                "filename": "contrib/src/main/java/com/datatorrent/contrib/hdht/HDHTReader.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/50e55384231629b53c9d9c1b435e88962a0377a8/contrib/src/main/java/com/datatorrent/contrib/hdht/HDHTReader.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/50e55384231629b53c9d9c1b435e88962a0377a8/contrib/src/main/java/com/datatorrent/contrib/hdht/HDHTReader.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/contrib/src/main/java/com/datatorrent/contrib/hdht/HDHTReader.java?ref=50e55384231629b53c9d9c1b435e88962a0377a8",
                "patch": "@@ -377,7 +377,7 @@ protected BucketFileMeta addFile(long bucketKey, Slice startKey)\n   private static class BucketReader implements Closeable\n   {\n     BucketMeta bucketMeta;\n-    final HashMap<String, HDSFileReader> readers = Maps.newHashMap();\n+    final ConcurrentMap<String, HDSFileReader> readers = Maps.newConcurrentMap();\n \n     @Override\n     public void close() throws IOException",
                "deletions": 1
            },
            {
                "sha": "2f06da9e9540e7582fc87bb4bbf3a1e8919e3065",
                "filename": "contrib/src/main/java/com/datatorrent/contrib/hdht/tfile/TFileReader.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/50e55384231629b53c9d9c1b435e88962a0377a8/contrib/src/main/java/com/datatorrent/contrib/hdht/tfile/TFileReader.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/50e55384231629b53c9d9c1b435e88962a0377a8/contrib/src/main/java/com/datatorrent/contrib/hdht/tfile/TFileReader.java",
                "status": "modified",
                "changes": 12,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/contrib/src/main/java/com/datatorrent/contrib/hdht/tfile/TFileReader.java?ref=50e55384231629b53c9d9c1b435e88962a0377a8",
                "patch": "@@ -18,6 +18,7 @@\n import java.io.IOException;\n import java.util.TreeMap;\n \n+import com.datatorrent.common.util.DTThrowable;\n import com.datatorrent.common.util.Slice;\n \n import org.apache.hadoop.conf.Configuration;\n@@ -39,6 +40,7 @@\n   private final Reader reader;\n   private final Scanner scanner;\n   private final FSDataInputStream fsdis;\n+  private boolean closed = false;\n \n   public TFileReader(FSDataInputStream fsdis, long fileLength, Configuration conf) throws IOException\n   {\n@@ -54,6 +56,7 @@ public TFileReader(FSDataInputStream fsdis, long fileLength, Configuration conf)\n   @Override\n   public void close() throws IOException\n   {\n+    closed = true;\n     scanner.close();\n     reader.close();\n     fsdis.close();\n@@ -85,7 +88,14 @@ public void reset() throws IOException\n   @Override\n   public boolean seek(Slice key) throws IOException\n   {\n-    return scanner.seekTo(key.buffer, key.offset, key.length);\n+    try {\n+      return scanner.seekTo(key.buffer, key.offset, key.length);\n+    } catch (NullPointerException ex) {\n+      if (closed)\n+        throw new IOException(\"Stream was closed\");\n+      else\n+        throw ex;\n+    }\n   }\n \n   @Override",
                "deletions": 1
            }
        ],
        "patched_files": [
            "HDHTReader.java"
        ],
        "unit_tests": [
            "HDHTReaderTest.java"
        ]
    },
    "apex-malhar_7b019fa": {
        "repo": "apex-malhar",
        "message": "Merge branch 'APEXMALHAR-2136-NPE-RecoverWindows'",
        "commit": "https://github.com/apache/apex-malhar/commit/7b019fa1ba2cac60565c5ee0d9ebdcf396cd93b6",
        "parent": "https://github.com/apache/apex-malhar/commit/0a87bc0a526e7355bc2764c83f6c9ed092b6f228",
        "bug_id": "apex-malhar_7b019fa",
        "file": [
            {
                "sha": "b5b9f8ccc017b335563d34463791e3fb304b5214",
                "filename": "library/src/main/java/org/apache/apex/malhar/lib/state/managed/AbstractManagedStateImpl.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/7b019fa1ba2cac60565c5ee0d9ebdcf396cd93b6/library/src/main/java/org/apache/apex/malhar/lib/state/managed/AbstractManagedStateImpl.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/7b019fa1ba2cac60565c5ee0d9ebdcf396cd93b6/library/src/main/java/org/apache/apex/malhar/lib/state/managed/AbstractManagedStateImpl.java",
                "status": "modified",
                "changes": 42,
                "additions": 27,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/org/apache/apex/malhar/lib/state/managed/AbstractManagedStateImpl.java?ref=7b019fa1ba2cac60565c5ee0d9ebdcf396cd93b6",
                "patch": "@@ -144,7 +144,7 @@\n   protected transient ExecutorService readerService;\n \n   @NotNull\n-  protected IncrementalCheckpointManager checkpointManager = new IncrementalCheckpointManager();\n+  private IncrementalCheckpointManager checkpointManager = new IncrementalCheckpointManager();\n \n   @NotNull\n   protected BucketsFileSystem bucketsFileSystem = new BucketsFileSystem();\n@@ -203,22 +203,24 @@ public void setup(OperatorContext context)\n       //delete all the wal files with windows > activationWindow.\n       //All the wal files with windows <= activationWindow are loaded and kept separately as recovered data.\n       try {\n-        for (long recoveredWindow : checkpointManager.getWindowIds(operatorContext.getId())) {\n-          if (recoveredWindow <= activationWindow) {\n-            @SuppressWarnings(\"unchecked\")\n-            Map<Long, Map<Slice, Bucket.BucketedValue>> recoveredData = (Map<Long, Map<Slice, Bucket.BucketedValue>>)\n-                checkpointManager.load(operatorContext.getId(), recoveredWindow);\n-            if (recoveredData != null && !recoveredData.isEmpty()) {\n-              for (Map.Entry<Long, Map<Slice, Bucket.BucketedValue>> entry : recoveredData.entrySet()) {\n-                int bucketIdx = prepareBucket(entry.getKey());\n-                buckets[bucketIdx].recoveredData(recoveredWindow, entry.getValue());\n+        long[] recoveredWindows = checkpointManager.getWindowIds(operatorContext.getId());\n+        if (recoveredWindows != null) {\n+          for (long recoveredWindow : recoveredWindows) {\n+            if (recoveredWindow <= activationWindow) {\n+              @SuppressWarnings(\"unchecked\")\n+              Map<Long, Map<Slice, Bucket.BucketedValue>> recoveredData = (Map<Long, Map<Slice, Bucket.BucketedValue>>)\n+                  checkpointManager.load(operatorContext.getId(), recoveredWindow);\n+              if (recoveredData != null && !recoveredData.isEmpty()) {\n+                for (Map.Entry<Long, Map<Slice, Bucket.BucketedValue>> entry : recoveredData.entrySet()) {\n+                  int bucketIdx = prepareBucket(entry.getKey());\n+                  buckets[bucketIdx].recoveredData(recoveredWindow, entry.getValue());\n+                }\n               }\n+              checkpointManager.save(recoveredData, operatorContext.getId(), recoveredWindow,\n+                  true /*skipWritingToWindowFile*/);\n+            } else {\n+              checkpointManager.delete(operatorContext.getId(), recoveredWindow);\n             }\n-            checkpointManager.save(recoveredData, operatorContext.getId(), recoveredWindow,\n-                true /*skipWritingToWindowFile*/);\n-\n-          } else {\n-            checkpointManager.delete(operatorContext.getId(), recoveredWindow);\n           }\n         }\n       } catch (IOException e) {\n@@ -536,6 +538,16 @@ public void setDurationPreventingFreeingSpace(Duration durationPreventingFreeing\n     this.durationPreventingFreeingSpace = durationPreventingFreeingSpace;\n   }\n \n+  public IncrementalCheckpointManager getCheckpointManager()\n+  {\n+    return checkpointManager;\n+  }\n+\n+  public void setCheckpointManager(@NotNull IncrementalCheckpointManager checkpointManager)\n+  {\n+    this.checkpointManager = Preconditions.checkNotNull(checkpointManager);\n+  }\n+\n   static class ValueFetchTask implements Callable<Slice>\n   {\n     private final Bucket bucket;",
                "deletions": 15
            },
            {
                "sha": "56781074a925e76bccf871a4f0ec0e6ebec9092a",
                "filename": "library/src/main/java/org/apache/apex/malhar/lib/state/managed/StateTracker.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/7b019fa1ba2cac60565c5ee0d9ebdcf396cd93b6/library/src/main/java/org/apache/apex/malhar/lib/state/managed/StateTracker.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/7b019fa1ba2cac60565c5ee0d9ebdcf396cd93b6/library/src/main/java/org/apache/apex/malhar/lib/state/managed/StateTracker.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/org/apache/apex/malhar/lib/state/managed/StateTracker.java?ref=7b019fa1ba2cac60565c5ee0d9ebdcf396cd93b6",
                "patch": "@@ -122,7 +122,7 @@ public void run()\n             synchronized (bucket) {\n               long sizeFreed;\n               try {\n-                sizeFreed = bucket.freeMemory(managedStateImpl.checkpointManager.getLastTransferredWindow());\n+                sizeFreed = bucket.freeMemory(managedStateImpl.getCheckpointManager().getLastTransferredWindow());\n                 LOG.debug(\"bucket freed {} {}\", bucketId, sizeFreed);\n               } catch (IOException e) {\n                 managedStateImpl.throwable.set(e);",
                "deletions": 1
            }
        ],
        "patched_files": [
            "StateTracker.java"
        ],
        "unit_tests": [
            "StateTrackerTest.java"
        ]
    },
    "apex-malhar_289dad7": {
        "repo": "apex-malhar",
        "message": "APEXMALHAR-2003 NPE in blockMetaDataIterator after recovery",
        "commit": "https://github.com/apache/apex-malhar/commit/289dad7426ade1779733cead614d52946154211f",
        "parent": "https://github.com/apache/apex-malhar/commit/d23e283453a37e8f2385f6ca9bf46683ecf926e6",
        "bug_id": "apex-malhar_289dad7",
        "file": [
            {
                "sha": "cd47d480b54bce43380e0622134afd8db6aaefd4",
                "filename": "library/src/main/java/com/datatorrent/lib/io/fs/AbstractFileSplitter.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/289dad7426ade1779733cead614d52946154211f/library/src/main/java/com/datatorrent/lib/io/fs/AbstractFileSplitter.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/289dad7426ade1779733cead614d52946154211f/library/src/main/java/com/datatorrent/lib/io/fs/AbstractFileSplitter.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/com/datatorrent/lib/io/fs/AbstractFileSplitter.java?ref=289dad7426ade1779733cead614d52946154211f",
                "patch": "@@ -285,7 +285,7 @@ public int getBlocksThreshold()\n     private long pos;\n     private int blockNumber;\n \n-    private final transient AbstractFileSplitter splitter;\n+    private final AbstractFileSplitter splitter;\n \n     protected BlockMetadataIterator()\n     {\n@@ -319,8 +319,8 @@ public boolean hasNext()\n       }\n       boolean isLast = length >= fileMetadata.getFileLength();\n       long lengthOfFileInBlock = isLast ? fileMetadata.getFileLength() : length;\n-      BlockMetadata.FileBlockMetadata fileBlock = splitter\n-          .buildBlockMetadata(pos, lengthOfFileInBlock, blockNumber, fileMetadata, isLast);\n+      BlockMetadata.FileBlockMetadata fileBlock = splitter.buildBlockMetadata(pos, lengthOfFileInBlock, blockNumber,\n+          fileMetadata, isLast);\n       pos = lengthOfFileInBlock;\n       return fileBlock;\n     }",
                "deletions": 3
            },
            {
                "sha": "cd0de2de87a11850dec3f6237eb9f113bbe24c06",
                "filename": "library/src/test/java/com/datatorrent/lib/io/fs/FileSplitterInputTest.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/289dad7426ade1779733cead614d52946154211f/library/src/test/java/com/datatorrent/lib/io/fs/FileSplitterInputTest.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/289dad7426ade1779733cead614d52946154211f/library/src/test/java/com/datatorrent/lib/io/fs/FileSplitterInputTest.java",
                "status": "modified",
                "changes": 44,
                "additions": 44,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/test/java/com/datatorrent/lib/io/fs/FileSplitterInputTest.java?ref=289dad7426ade1779733cead614d52946154211f",
                "patch": "@@ -51,6 +51,7 @@\n import com.datatorrent.lib.io.IdempotentStorageManager;\n import com.datatorrent.lib.io.block.BlockMetadata;\n import com.datatorrent.lib.testbench.CollectorTestSink;\n+import com.datatorrent.lib.util.KryoCloneUtils;\n import com.datatorrent.lib.util.TestUtils;\n \n /**\n@@ -455,6 +456,49 @@ public void testSingleFile() throws InterruptedException, IOException\n         testMeta.fileMetadataSink.collectedTuples.get(0).getFilePath());\n   }\n \n+  @Test\n+  public void testRecoveryOfBlockMetadataIterator() throws InterruptedException\n+  {\n+    IdempotentStorageManager.FSIdempotentStorageManager fsIdempotentStorageManager =\n+        new IdempotentStorageManager.FSIdempotentStorageManager();\n+\n+    testMeta.fileSplitterInput.setIdempotentStorageManager(fsIdempotentStorageManager);\n+    testMeta.fileSplitterInput.setBlockSize(2L);\n+    testMeta.fileSplitterInput.setBlocksThreshold(2);\n+    testMeta.fileSplitterInput.getScanner().setScanIntervalMillis(500);\n+\n+\n+    testMeta.fileSplitterInput.setup(testMeta.context);\n+\n+    testMeta.fileSplitterInput.beginWindow(1);\n+\n+    ((MockScanner)testMeta.fileSplitterInput.getScanner()).semaphore.acquire();\n+    testMeta.fileSplitterInput.emitTuples();\n+    testMeta.fileSplitterInput.endWindow();\n+\n+    //file0.txt has just 5 blocks. Since blocks threshold is 2, only 2 are emitted.\n+    Assert.assertEquals(\"Files\", 1, testMeta.fileMetadataSink.collectedTuples.size());\n+    Assert.assertEquals(\"Blocks\", 2, testMeta.blockMetadataSink.collectedTuples.size());\n+\n+    testMeta.fileMetadataSink.clear();\n+    testMeta.blockMetadataSink.clear();\n+\n+    //At this point the operator was check-pointed and then there was a failure.\n+    testMeta.fileSplitterInput.teardown();\n+\n+    //The operator was restored from persisted state and re-deployed.\n+    testMeta.fileSplitterInput = KryoCloneUtils.cloneObject(testMeta.fileSplitterInput);\n+    TestUtils.setSink(testMeta.fileSplitterInput.blocksMetadataOutput, testMeta.blockMetadataSink);\n+    TestUtils.setSink(testMeta.fileSplitterInput.filesMetadataOutput, testMeta.fileMetadataSink);\n+\n+    testMeta.fileSplitterInput.setup(testMeta.context);\n+    testMeta.fileSplitterInput.beginWindow(1);\n+\n+    Assert.assertEquals(\"Recovered Files\", 1, testMeta.fileMetadataSink.collectedTuples.size());\n+    Assert.assertEquals(\"Recovered Blocks\", 2, testMeta.blockMetadataSink.collectedTuples.size());\n+  }\n+\n+\n   private static class MockScanner extends FileSplitterInput.TimeBasedDirectoryScanner\n   {\n     transient Semaphore semaphore;",
                "deletions": 0
            }
        ],
        "patched_files": [
            "AbstractFileSplitter.java",
            "FileSplitterInput.java"
        ],
        "unit_tests": [
            "FileSplitterInputTest.java"
        ]
    },
    "apex-malhar_712027a": {
        "repo": "apex-malhar",
        "message": "APEXMALHAR-2535: change type of timeout variables from int to long\nFix NPE during teardown, when refreshtime is not set\nAllow setting expiration type",
        "commit": "https://github.com/apache/apex-malhar/commit/712027aec6388cb2af7709c63c6c59fa82a94307",
        "parent": "https://github.com/apache/apex-malhar/commit/d3f7faf58bff06a3c670241542476d1d4a07386f",
        "bug_id": "apex-malhar_712027a",
        "file": [
            {
                "sha": "c377b96260726d1a6d29859ee56afd30422d82e2",
                "filename": "contrib/src/main/java/com/datatorrent/contrib/enrich/AbstractEnricher.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/712027aec6388cb2af7709c63c6c59fa82a94307/contrib/src/main/java/com/datatorrent/contrib/enrich/AbstractEnricher.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/712027aec6388cb2af7709c63c6c59fa82a94307/contrib/src/main/java/com/datatorrent/contrib/enrich/AbstractEnricher.java",
                "status": "modified",
                "changes": 27,
                "additions": 20,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/contrib/src/main/java/com/datatorrent/contrib/enrich/AbstractEnricher.java?ref=712027aec6388cb2af7709c63c6c59fa82a94307",
                "patch": "@@ -25,11 +25,13 @@\n import org.apache.hadoop.classification.InterfaceStability;\n \n import com.esotericsoftware.kryo.NotNull;\n+\n import com.datatorrent.api.Context;\n import com.datatorrent.api.Operator;\n import com.datatorrent.common.util.BaseOperator;\n import com.datatorrent.lib.db.cache.CacheManager;\n import com.datatorrent.lib.db.cache.CacheStore;\n+import com.datatorrent.lib.db.cache.CacheStore.ExpiryType;\n import com.datatorrent.lib.util.FieldInfo;\n import com.datatorrent.lib.util.FieldInfo.SupportType;\n \n@@ -66,8 +68,9 @@\n   /**\n    * Optional parameters for enricher.\n    */\n-  private int cacheExpirationInterval = 1 * 60 * 60 * 1000;  // 1 hour\n-  private int cacheCleanupInterval = 1 * 60 * 60 * 1000; // 1 hour\n+  private long cacheExpirationInterval = 1 * 60 * 60 * 1000;  // 1 hour\n+  private long cacheCleanupInterval = 1 * 60 * 60 * 1000; // 1 hour\n+  private ExpiryType expiryType = ExpiryType.EXPIRE_AFTER_WRITE;\n   private int cacheSize = 1024; // 1024 records\n \n   /**\n@@ -160,7 +163,7 @@ public void setup(Context.OperatorContext context)\n     // set expiration to one day.\n     primaryCache.setEntryExpiryDurationInMillis(cacheExpirationInterval);\n     primaryCache.setCacheCleanupInMillis(cacheCleanupInterval);\n-    primaryCache.setEntryExpiryStrategy(CacheStore.ExpiryType.EXPIRE_AFTER_WRITE);\n+    primaryCache.setEntryExpiryStrategy(expiryType);\n     primaryCache.setMaxCacheSize(cacheSize);\n \n     cacheManager.setPrimary(primaryCache);\n@@ -268,7 +271,7 @@ public void setStore(BackendLoader store)\n    *\n    * @return Cache entry expiration interval in ms\n    */\n-  public int getCacheExpirationInterval()\n+  public long getCacheExpirationInterval()\n   {\n     return cacheExpirationInterval;\n   }\n@@ -279,7 +282,7 @@ public int getCacheExpirationInterval()\n    *\n    * @param cacheExpirationInterval Cache entry expiration interval in ms\n    */\n-  public void setCacheExpirationInterval(int cacheExpirationInterval)\n+  public void setCacheExpirationInterval(long cacheExpirationInterval)\n   {\n     this.cacheExpirationInterval = cacheExpirationInterval;\n   }\n@@ -290,7 +293,7 @@ public void setCacheExpirationInterval(int cacheExpirationInterval)\n    *\n    * @return cache cleanup interval in ms.\n    */\n-  public int getCacheCleanupInterval()\n+  public long getCacheCleanupInterval()\n   {\n     return cacheCleanupInterval;\n   }\n@@ -301,7 +304,7 @@ public int getCacheCleanupInterval()\n    *\n    * @param cacheCleanupInterval cache cleanup interval in ms.\n    */\n-  public void setCacheCleanupInterval(int cacheCleanupInterval)\n+  public void setCacheCleanupInterval(long cacheCleanupInterval)\n   {\n     this.cacheCleanupInterval = cacheCleanupInterval;\n   }\n@@ -326,6 +329,16 @@ public void setCacheSize(int cacheSize)\n     this.cacheSize = cacheSize;\n   }\n \n+  public ExpiryType getExpiryType()\n+  {\n+    return expiryType;\n+  }\n+\n+  public void setExpiryType(ExpiryType expiryType)\n+  {\n+    this.expiryType = expiryType;\n+  }\n+\n   public CacheManager getCacheManager()\n   {\n     return cacheManager;",
                "deletions": 7
            },
            {
                "sha": "c073affc4a1fd2b82bcd174e1fb6b57aa6ade1d8",
                "filename": "library/src/main/java/com/datatorrent/lib/db/cache/CacheStore.java",
                "blob_url": "https://github.com/apache/apex-malhar/blob/712027aec6388cb2af7709c63c6c59fa82a94307/library/src/main/java/com/datatorrent/lib/db/cache/CacheStore.java",
                "raw_url": "https://github.com/apache/apex-malhar/raw/712027aec6388cb2af7709c63c6c59fa82a94307/library/src/main/java/com/datatorrent/lib/db/cache/CacheStore.java",
                "status": "modified",
                "changes": 8,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/apex-malhar/contents/library/src/main/java/com/datatorrent/lib/db/cache/CacheStore.java?ref=712027aec6388cb2af7709c63c6c59fa82a94307",
                "patch": "@@ -58,10 +58,10 @@\n   protected long maxCacheSize = 2000;\n \n   @Min(0)\n-  protected int entryExpiryDurationInMillis = 60000; //1 minute\n+  protected long entryExpiryDurationInMillis = 60000; //1 minute\n \n   @Min(0)\n-  protected int cacheCleanupIntervalInMillis = 60500; //.5 seconds after entries are expired\n+  protected long cacheCleanupIntervalInMillis = 60500; //.5 seconds after entries are expired\n \n   @NotNull\n   protected ExpiryType entryExpiryStrategy = ExpiryType.EXPIRE_AFTER_ACCESS;\n@@ -190,7 +190,7 @@ public void setEntryExpiryStrategy(ExpiryType expiryType)\n    *\n    * @param durationInMillis the duration after which a cache entry is expired.\n    */\n-  public void setEntryExpiryDurationInMillis(int durationInMillis)\n+  public void setEntryExpiryDurationInMillis(long durationInMillis)\n   {\n     this.entryExpiryDurationInMillis = durationInMillis;\n   }\n@@ -200,7 +200,7 @@ public void setEntryExpiryDurationInMillis(int durationInMillis)\n    *\n    * @param durationInMillis the duration after which cache is cleaned up regularly.\n    */\n-  public void setCacheCleanupInMillis(int durationInMillis)\n+  public void setCacheCleanupInMillis(long durationInMillis)\n   {\n     this.cacheCleanupIntervalInMillis = durationInMillis;\n   }",
                "deletions": 4
            }
        ],
        "patched_files": [
            "CacheStore.java"
        ],
        "unit_tests": [
            "CacheStoreTest.java"
        ]
    }
}