{
    "hadoop-hdfs_2426d84": {
        "bug_id": "hadoop-hdfs_2426d84",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/2426d84a3a13421abb9f48c81e479e935733c3f8",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -390,6 +390,8 @@ Release 0.21.0 - Unreleased\n \n     HDFS-665. TestFileAppend2 sometimes hangs. (hairong)\n \n+    HDFS-676. Fix NPE in FSDataset.updateReplicaUnderRecovery() (shv)\n+\n Release 0.20.1 - 2009-09-01\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/CHANGES.txt",
                "sha": "5abba3a61bbc02969fd3476b07cd9bfd7ae376a4",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 19,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "patch": "@@ -1982,19 +1982,21 @@ static ReplicaRecoveryInfo initReplicaRecovery(\n     return rur.createInfo();\n   }\n \n-  /** Update a replica of a block. */\n-  synchronized void updateReplica(final Block block, final long recoveryId,\n-      final long newlength) throws IOException {\n+  @Override // FSDatasetInterface\n+  public synchronized ReplicaInfo updateReplicaUnderRecovery(\n+                                    final Block oldBlock,\n+                                    final long recoveryId,\n+                                    final long newlength) throws IOException {\n     //get replica\n-    final ReplicaInfo replica = volumeMap.get(block.getBlockId());\n-    DataNode.LOG.info(\"updateReplica: block=\" + block\n+    final ReplicaInfo replica = volumeMap.get(oldBlock.getBlockId());\n+    DataNode.LOG.info(\"updateReplica: block=\" + oldBlock\n         + \", recoveryId=\" + recoveryId\n         + \", length=\" + newlength\n         + \", replica=\" + replica);\n \n     //check replica\n     if (replica == null) {\n-      throw new ReplicaNotFoundException(block);\n+      throw new ReplicaNotFoundException(oldBlock);\n     }\n \n     //check replica state\n@@ -2007,26 +2009,18 @@ synchronized void updateReplica(final Block block, final long recoveryId,\n     checkReplicaFiles(replica);\n \n     //update replica\n-    final ReplicaInfo finalized = (ReplicaInfo)updateReplicaUnderRecovery(\n-                                    replica, recoveryId, newlength);\n+    final FinalizedReplica finalized = updateReplicaUnderRecovery(\n+        (ReplicaUnderRecovery)replica, recoveryId, newlength);\n \n     //check replica files after update\n     checkReplicaFiles(finalized);\n+    return finalized;\n   }\n \n-  @Override // FSDatasetInterface\n-  public synchronized FinalizedReplica updateReplicaUnderRecovery(\n-                                          Block oldBlock,\n+  private FinalizedReplica updateReplicaUnderRecovery(\n+                                          ReplicaUnderRecovery rur,\n                                           long recoveryId,\n                                           long newlength) throws IOException {\n-    Replica r = getReplica(oldBlock.getBlockId());\n-    if(r.getState() != ReplicaState.RUR)\n-      throw new IOException(\"Replica \" + r + \" must be under recovery.\");\n-    ReplicaUnderRecovery rur = (ReplicaUnderRecovery)r;\n-    DataNode.LOG.info(\"updateReplicaUnderRecovery: recoveryId=\" + recoveryId\n-        + \", newlength=\" + newlength\n-        + \", rur=\" + rur);\n-\n     //check recovery id\n     if (rur.getRecoveryID() != recoveryId) {\n       throw new IOException(\"rur.getRecoveryID() != recoveryId = \" + recoveryId",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "sha": "12d08168eac8d3840933a776ea336865423b2838",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "patch": "@@ -350,7 +350,8 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n   /**\n    * Update replica's generation stamp and length and finalize it.\n    */\n-  public FinalizedReplica updateReplicaUnderRecovery(Block oldBlock,\n+  public ReplicaInfo updateReplicaUnderRecovery(\n+                                          Block oldBlock,\n                                           long recoveryId,\n                                           long newLength) throws IOException;\n }",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "sha": "462e1fffc573d6230dc402b6e34066fd7dd325f1",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 3,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "patch": "@@ -804,10 +804,10 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n     return new ReplicaRecoveryInfo(rBlock.getBlock(), ReplicaState.FINALIZED);\n   }\n \n-  @Override\n+  @Override // FSDatasetInterface\n   public FinalizedReplica updateReplicaUnderRecovery(Block oldBlock,\n-                                          long recoveryId,\n-                                          long newlength) throws IOException {\n+                                        long recoveryId,\n+                                        long newlength) throws IOException {\n     return new FinalizedReplica(\n         oldBlock.getBlockId(), newlength, recoveryId, null, null);\n   }",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "sha": "1d9fe16525aab45d39a39a2ff7358ab4650c3c0e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 3,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "patch": "@@ -234,9 +234,8 @@ public void testUpdateReplicaUnderRecovery() throws IOException {\n       FSDataset.checkReplicaFiles(rur);\n \n       //update\n-      final ReplicaInfo finalized = \n-        (ReplicaInfo)fsdataset.updateReplicaUnderRecovery(\n-            rur, recoveryid, newlength);\n+      final ReplicaInfo finalized = fsdataset.updateReplicaUnderRecovery(\n+                                                rur, recoveryid, newlength);\n \n       //check meta data after update\n       FSDataset.checkReplicaFiles(finalized);",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "sha": "40d15a6f02c0cd211df29bfad0ebed066a4c5b4c",
                "status": "modified"
            }
        ],
        "message": "HDFS-676. Fix NPE in FSDataset.updateReplicaUnderRecovery(). Contributed by Konstantin Shvachko.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@823732 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/f2809529a9d80906c53ae6931e84255f89ff1f8b",
        "patched_files": [
            "FSDataset.java",
            "SimulatedFSDataset.java",
            "CHANGES.java",
            "FSDatasetInterface.java",
            "InterDatanodeProtocol.java"
        ],
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "TestInterDatanodeProtocol.java",
            "TestSimulatedFSDataset.java"
        ]
    },
    "hadoop-hdfs_28c1b2a": {
        "bug_id": "hadoop-hdfs_28c1b2a",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -930,6 +930,9 @@ Release 0.22.0 - Unreleased\n     HDFS-1544. Ivy resolve force mode should be turned off by default.\n     (Luke Lu via tomwhite)\n \n+    HDFS-1615. seek() on closed DFS input stream throws NullPointerException\n+    (Scott Carey via todd)\n+\n Release 0.21.1 - Unreleased\n     HDFS-1466. TestFcHdfsSymlink relies on /tmp/test not existing. (eli)\n ",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/CHANGES.txt",
                "sha": "fda22ec7b56dd6259763dc18fe543538ce17f4fc",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -741,6 +741,9 @@ public synchronized void seek(long targetPos) throws IOException {\n     if (targetPos > getFileLength()) {\n       throw new IOException(\"Cannot seek after EOF\");\n     }\n+    if (closed) {\n+      throw new IOException(\"Stream is closed!\");\n+    }\n     boolean done = false;\n     if (pos <= targetPos && targetPos <= blockEnd) {\n       //",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "b74b55dc2226ce1404d5478214bd56fb3180248c",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java?ref=28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
                "deletions": 0,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "patch": "@@ -100,6 +100,45 @@ public void testDFSClose() throws Exception {\n       if (cluster != null) {cluster.shutdown();}\n     }\n   }\n+  \n+  @Test\n+  public void testDFSSeekExceptions() throws IOException {\n+    Configuration conf = getTestConfiguration();\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+    FileSystem fileSys = cluster.getFileSystem();\n+\n+    try {\n+      String file = \"/test/fileclosethenseek/file-0\";\n+      Path path = new Path(file);\n+      // create file\n+      FSDataOutputStream output = fileSys.create(path);\n+      output.writeBytes(\"Some test data to write longer than 10 bytes\");\n+      output.close();\n+      FSDataInputStream input = fileSys.open(path);\n+      input.seek(10);\n+      boolean threw = false;\n+      try {\n+        input.seek(100);\n+      } catch (IOException e) {\n+        // success\n+        threw = true;\n+      }\n+      assertTrue(\"Failed to throw IOE when seeking past end\", threw);\n+      input.close();\n+      threw = false;\n+      try {\n+        input.seek(1);\n+      } catch (IOException e) {\n+        //success\n+        threw = true;\n+      }\n+      assertTrue(\"Failed to throw IOE when seeking after close\", threw);\n+      fileSys.close();\n+    }\n+    finally {\n+      if (cluster != null) {cluster.shutdown();}\n+    }\n+  }\n \n   @Test\n   public void testDFSClient() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "sha": "5184ceb782d552c16fb66f76c3649110ae1c30a6",
                "status": "modified"
            }
        ],
        "message": "HDFS-1615. seek() on closed DFS input stream throws NullPointerException. Contributed by Scott Carey.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1102094 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/1abbfe982f977c605db25b1bd9933e1d1a251de8",
        "patched_files": [
            "DFSInputStream.java",
            "CHANGES.java",
            "DistributedFileSystem.java"
        ],
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "TestDistributedFileSystem.java"
        ]
    },
    "hadoop-hdfs_2f4e2a8": {
        "bug_id": "hadoop-hdfs_2f4e2a8",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/2f4e2a8c45c764553db8cf17aeb074384cfa433e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2f4e2a8c45c764553db8cf17aeb074384cfa433e/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=2f4e2a8c45c764553db8cf17aeb074384cfa433e",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -96,6 +96,9 @@ Trunk (unreleased changes)\n \n     HDFS-825. Build fails to pull latest hadoop-core-* artifacts (cos)\n \n+    HDFS-812. FSNamesystem#internalReleaseLease throws NullPointerException on\n+    a single-block file's lease recovery. (cos)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2f4e2a8c45c764553db8cf17aeb074384cfa433e/CHANGES.txt",
                "sha": "8ed2191be9822ec88aba585b448686f8ee36fe02",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2f4e2a8c45c764553db8cf17aeb074384cfa433e/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=2f4e2a8c45c764553db8cf17aeb074384cfa433e",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -1953,8 +1953,17 @@ boolean internalReleaseLease(\n     BlockInfoUnderConstruction lastBlock = pendingFile.getLastBlock();\n     BlockUCState lastBlockState = lastBlock.getBlockUCState();\n     BlockInfo penultimateBlock = pendingFile.getPenultimateBlock();\n-    BlockUCState penultimateBlockState = (penultimateBlock == null ?\n-        BlockUCState.COMPLETE : penultimateBlock.getBlockUCState());\n+    boolean penultimateBlockMinReplication;\n+    BlockUCState penultimateBlockState;\n+    if (penultimateBlock == null) {\n+      penultimateBlockState = BlockUCState.COMPLETE;\n+      // If penultimate block doesn't exist then its minReplication is met\n+      penultimateBlockMinReplication = true;\n+    } else {\n+      penultimateBlockState = BlockUCState.COMMITTED;\n+      penultimateBlockMinReplication = \n+        blockManager.checkMinReplication(penultimateBlock);\n+    }\n     assert penultimateBlockState == BlockUCState.COMPLETE ||\n            penultimateBlockState == BlockUCState.COMMITTED :\n            \"Unexpected state of penultimate block in \" + src;\n@@ -1965,7 +1974,7 @@ boolean internalReleaseLease(\n       break;\n     case COMMITTED:\n       // Close file if committed blocks are minimally replicated\n-      if(blockManager.checkMinReplication(penultimateBlock) &&\n+      if(penultimateBlockMinReplication &&\n           blockManager.checkMinReplication(lastBlock)) {\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n         NameNode.stateChangeLog.warn(\"BLOCK*\"",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2f4e2a8c45c764553db8cf17aeb074384cfa433e/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "29d51bf3257ee747b01c4b2c25e9bede58679940",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/2f4e2a8c45c764553db8cf17aeb074384cfa433e/src/test/unit/org/apache/hadoop/hdfs/server/namenode/TestNNLeaseRecovery.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/unit/org/apache/hadoop/hdfs/server/namenode/TestNNLeaseRecovery.java?ref=2f4e2a8c45c764553db8cf17aeb074384cfa433e",
                "deletions": 13,
                "filename": "src/test/unit/org/apache/hadoop/hdfs/server/namenode/TestNNLeaseRecovery.java",
                "patch": "@@ -157,6 +157,27 @@ public void testInternalReleaseLease_COMM_COMM () throws IOException {\n     assertTrue(\"FSNamesystem.internalReleaseLease suppose to throw \" +\n       \"AlreadyBeingCreatedException here\", false);\n   }\n+\n+  /**\n+   * Mocks FSNamesystem instance, adds an empty file with 0 blocks\n+   * and invokes lease recovery method. \n+   * \n+   */\n+  @Test\n+  public void testInternalReleaseLease_0blocks () throws IOException {\n+    LOG.debug(\"Running \" + GenericTestUtils.getMethodName());\n+    LeaseManager.Lease lm = mock(LeaseManager.Lease.class);\n+    Path file = \n+      spy(new Path(\"/\" + GenericTestUtils.getMethodName() + \"_test.dat\"));\n+    DatanodeDescriptor dnd = mock(DatanodeDescriptor.class);\n+    PermissionStatus ps =\n+      new PermissionStatus(\"test\", \"test\", new FsPermission((short)0777));\n+\n+    mockFileBlocks(0, null, null, file, dnd, ps, false);\n+\n+    assertTrue(\"True has to be returned in this case\",\n+      fsn.internalReleaseLease(lm, file.toString(), null));\n+  }\n   \n   /**\n    * Mocks FSNamesystem instance, adds an empty file with 1 block\n@@ -346,16 +367,6 @@ private void mockFileBlocks(int fileBlocksNumber,\n     when(b.getBlockUCState()).thenReturn(penUltState);\n     when(b1.getBlockUCState()).thenReturn(lastState);\n     BlockInfo[] blocks;\n-    switch (fileBlocksNumber) {\n-      case 0:\n-        blocks = new BlockInfo[0];\n-        break;\n-      case 1:\n-        blocks = new BlockInfo[]{b1};\n-        break;\n-      default:\n-        blocks = new BlockInfo[]{b, b1};\n-    }\n \n     FSDirectory fsDir = mock(FSDirectory.class);\n     INodeFileUnderConstruction iNFmock = mock(INodeFileUnderConstruction.class);\n@@ -368,14 +379,29 @@ private void mockFileBlocks(int fileBlocksNumber,\n     when(fsn.getFSImage().getEditLog()).thenReturn(editLog);\n     fsn.getFSImage().setFSNamesystem(fsn);\n     \n+    switch (fileBlocksNumber) {\n+      case 0:\n+        blocks = new BlockInfo[0];\n+        break;\n+      case 1:\n+        blocks = new BlockInfo[]{b1};\n+        when(iNFmock.getLastBlock()).thenReturn(b1);\n+        break;\n+      default:\n+        when(iNFmock.getPenultimateBlock()).thenReturn(b);\n+        when(iNFmock.getLastBlock()).thenReturn(b1);\n+        blocks = new BlockInfo[]{b, b1};\n+    }\n+    \n     when(iNFmock.getBlocks()).thenReturn(blocks);\n-    when(iNFmock.numBlocks()).thenReturn(2);\n-    when(iNFmock.getPenultimateBlock()).thenReturn(b);\n-    when(iNFmock.getLastBlock()).thenReturn(b1);\n+    when(iNFmock.numBlocks()).thenReturn(blocks.length);\n     when(iNFmock.isUnderConstruction()).thenReturn(true);\n+    when(iNFmock.convertToInodeFile()).thenReturn(iNFmock);    \n     fsDir.addFile(file.toString(), ps, (short)3, 1l, \"test\", \n       \"test-machine\", dnd, 1001l);\n \n+    fsn.leaseManager = mock(LeaseManager.class);\n+    fsn.leaseManager.addLease(\"mock-lease\", file.toString());\n     if (setStoredBlock) {\n       when(b1.getINode()).thenReturn(iNFmock);\n       fsn.blockManager.blocksMap.addINode(b1, iNFmock);",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/2f4e2a8c45c764553db8cf17aeb074384cfa433e/src/test/unit/org/apache/hadoop/hdfs/server/namenode/TestNNLeaseRecovery.java",
                "sha": "f6125b328d109087f3549a1f4a5617a2c87b7c0d",
                "status": "modified"
            }
        ],
        "message": "HDFS-812. FSNamesystem#internalReleaseLease throws NullPointerException on a single-block file's lease recovery. Contributed by Konstantin Boudnik\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@891106 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/c344d269c457117eae9de38a6e91b60310f9a655",
        "patched_files": [
            "FSNamesystem.java",
            "CHANGES.java"
        ],
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "TestNNLeaseRecovery.java"
        ]
    },
    "hadoop-hdfs_3407cdc": {
        "bug_id": "hadoop-hdfs_3407cdc",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/3407cdcbe9b272421574682332bb634900497ed2",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/3407cdcbe9b272421574682332bb634900497ed2/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=3407cdcbe9b272421574682332bb634900497ed2",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -426,6 +426,8 @@ Release 0.22.0 - Unreleased\n \n     HDFS-1523. TestLargeBlock is failing on trunk. (cos)\n \n+    HDFS-1502. TestBlockRecovery triggers NPE in assert. (hairong via cos)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/3407cdcbe9b272421574682332bb634900497ed2/CHANGES.txt",
                "sha": "bc620cd5cb35b960aa8cb56cc24a0c5a94259f0f",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/3407cdcbe9b272421574682332bb634900497ed2/src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java?ref=3407cdcbe9b272421574682332bb634900497ed2",
                "deletions": 11,
                "filename": "src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java",
                "patch": "@@ -128,7 +128,8 @@ public void tearDown() throws IOException {\n   private void testSyncReplicas(ReplicaRecoveryInfo replica1, \n       ReplicaRecoveryInfo replica2,\n       InterDatanodeProtocol dn1,\n-      InterDatanodeProtocol dn2) throws IOException {\n+      InterDatanodeProtocol dn2,\n+      long expectLen) throws IOException {\n     \n     DatanodeInfo[] locs = new DatanodeInfo[]{\n         mock(DatanodeInfo.class), mock(DatanodeInfo.class)};\n@@ -141,6 +142,13 @@ private void testSyncReplicas(ReplicaRecoveryInfo replica1,\n         new DatanodeID(\"aa\", \"bb\", 11, 22), dn2, replica2);\n     syncList.add(record1);\n     syncList.add(record2);\n+    \n+    when(dn1.updateReplicaUnderRecovery((Block)anyObject(), anyLong(), \n+        anyLong())).thenReturn(new Block(block.getBlockId(), \n+            expectLen, block.getGenerationStamp()));\n+    when(dn2.updateReplicaUnderRecovery((Block)anyObject(), anyLong(), \n+        anyLong())).thenReturn(new Block(block.getBlockId(), \n+            expectLen, block.getGenerationStamp()));\n     dn.syncBlock(rBlock, syncList);\n   }\n   \n@@ -162,7 +170,7 @@ public void testFinalizedReplicas () throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);    \n \n@@ -173,7 +181,7 @@ public void testFinalizedReplicas () throws IOException {\n         REPLICA_LEN2, GEN_STAMP-2, ReplicaState.FINALIZED);\n \n     try {\n-      testSyncReplicas(replica1, replica2, dn1, dn2);\n+      testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n       Assert.fail(\"Two finalized replicas should not have different lengthes!\");\n     } catch (IOException e) {\n       Assert.assertTrue(e.getMessage().startsWith(\n@@ -201,7 +209,7 @@ public void testFinalizedRbwReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     \n@@ -214,7 +222,7 @@ public void testFinalizedRbwReplicas() throws IOException {\n     dn1 = mock(InterDatanodeProtocol.class);\n     dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);\n@@ -240,7 +248,7 @@ public void testFinalizedRwrReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);\n@@ -254,7 +262,7 @@ public void testFinalizedRwrReplicas() throws IOException {\n     dn1 = mock(InterDatanodeProtocol.class);\n     dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);\n@@ -278,8 +286,8 @@ public void testRBWReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n     long minLen = Math.min(REPLICA_LEN1, REPLICA_LEN2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, minLen);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);    \n   }\n@@ -302,7 +310,7 @@ public void testRBW_RWRReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);    \n@@ -326,9 +334,9 @@ public void testRWRReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n-    \n     long minLen = Math.min(REPLICA_LEN1, REPLICA_LEN2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, minLen);\n+    \n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);    \n   }  ",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/3407cdcbe9b272421574682332bb634900497ed2/src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java",
                "sha": "9061781d9c867f6df31677e72067f83b2330bcf5",
                "status": "modified"
            }
        ],
        "message": "HDFS-1502. TestBlockRecovery triggers NPE in assert. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1042517 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/15ce625e5f269cfe06803c54d21d5dff46ab5425",
        "patched_files": [
            "CHANGES.java"
        ],
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "TestBlockRecovery.java"
        ]
    },
    "hadoop-hdfs_37ef56a": {
        "bug_id": "hadoop-hdfs_37ef56a",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -171,6 +171,9 @@ Trunk (unreleased changes)\n \n     HDFS-1680. Fix TestBalancer. (szetszwo)\n \n+    HDFS-1705. Balancer command throws NullPointerException. (suresh via\n+    szetszwo)\n+\n Release 0.22.0 - Unreleased\n \n   NEW FEATURES",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a/CHANGES.txt",
                "sha": "3175574eecc7f888cf5475734b3a341b0385686f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java?ref=37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a",
                "deletions": 6,
                "filename": "src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
                "patch": "@@ -1461,17 +1461,12 @@ public String toString() {\n   }\n \n   static class Cli extends Configured implements Tool {\n-    @Override\n-    public void setConf(Configuration conf) {\n-      super.setConf(conf);\n-      WIN_WIDTH = conf.getLong(\"dfs.balancer.movedWinWidth\", WIN_WIDTH);\n-    }\n-\n     /** Parse arguments and then run Balancer */\n     @Override\n     public int run(String[] args) {\n       final long startTime = Util.now();\n       final Configuration conf = getConf();\n+      WIN_WIDTH = conf.getLong(\"dfs.balancer.movedWinWidth\", WIN_WIDTH);\n \n       try {\n         checkReplicationPolicyCompatibility(conf);",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
                "sha": "175107cce93c69f8736868eede0eef4b27554c63",
                "status": "modified"
            }
        ],
        "message": "HDFS-1705. Balancer command throws NullPointerException. Contributed by suresh\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/branches/HDFS-1052@1076479 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/c10ba8f02c34de23b520303d2907d96d5d2a8a52",
        "patched_files": [
            "Balancer.java"
        ],
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "TestBalancer.java"
        ]
    },
    "hadoop-hdfs_74d19b5": {
        "bug_id": "hadoop-hdfs_74d19b5",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java?ref=74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                "patch": "@@ -98,6 +98,7 @@\n   private int imagePort;\n   private String infoBindAddress;\n \n+  private FSNamesystem namesystem;\n   private Collection<URI> checkpointDirs;\n   private Collection<URI> checkpointEditsDirs;\n   private long checkpointPeriod;    // in seconds\n@@ -481,8 +482,9 @@ private void startCheckpoint() throws IOException {\n    */\n   private void doMerge(CheckpointSignature sig, boolean loadImage)\n   throws IOException {\n-    FSNamesystem namesystem = \n-            new FSNamesystem(checkpointImage, conf);\n+    if (loadImage) {\n+      namesystem = new FSNamesystem(checkpointImage, conf);\n+    }\n     assert namesystem.dir.fsImage == checkpointImage;\n     checkpointImage.doMerge(sig, loadImage);\n   }",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                "sha": "08df62a988f388de0d26b89bc0da42f8e25f6398",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java?ref=74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f",
                "deletions": 0,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java",
                "patch": "@@ -684,6 +684,7 @@ public void testCheckpoint() throws IOException {\n     cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).build();\n     cluster.waitActive();\n     fileSys = cluster.getFileSystem();\n+    Path tmpDir = new Path(\"/tmp_tmp\");\n     try {\n       // check that file1 still exists\n       checkFile(fileSys, file1, replication);\n@@ -698,6 +699,11 @@ public void testCheckpoint() throws IOException {\n       //\n       SecondaryNameNode secondary = startSecondaryNameNode(conf);\n       secondary.doCheckpoint();\n+      \n+      fileSys.delete(tmpDir, true);\n+      fileSys.mkdirs(tmpDir);\n+      secondary.doCheckpoint();\n+      \n       secondary.shutdown();\n     } finally {\n       fileSys.close();\n@@ -713,6 +719,7 @@ public void testCheckpoint() throws IOException {\n     fileSys = cluster.getFileSystem();\n \n     assertTrue(!fileSys.exists(file1));\n+    assertTrue(fileSys.exists(tmpDir));\n \n     try {\n       // verify that file2 exists",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java",
                "sha": "2187a9ef59fe05e92eb18865e45a7fde1785451a",
                "status": "modified"
            }
        ],
        "message": "Fix NullPointerException in Secondary NameNode. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1102465 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/01ffd46e3fec9037c17e887f932fb1765c1ab219",
        "patched_files": [
            "SecondaryNameNode.java"
        ],
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "TestCheckpoint.java"
        ]
    },
    "hadoop-hdfs_8747d46": {
        "bug_id": "hadoop-hdfs_8747d46",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/8747d46d184a5764624131f0f9412602b3a24d9d",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/8747d46d184a5764624131f0f9412602b3a24d9d/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=8747d46d184a5764624131f0f9412602b3a24d9d",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -456,6 +456,8 @@ Release 0.21.0 - Unreleased\n     HDFS-725. Support the build error fix for HADOOP-6327.  (Sanjay Radia via\n     szetszwo)\n \n+    HDFS-625. Fix NullPointerException thrown from ListPathServlet. (suresh)\n+\n Release 0.20.2 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/8747d46d184a5764624131f0f9412602b3a24d9d/CHANGES.txt",
                "sha": "4be7428e5d20198f5f8cf8ade1ce255410278661",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/8747d46d184a5764624131f0f9412602b3a24d9d/src/java/org/apache/hadoop/hdfs/server/namenode/ListPathsServlet.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/namenode/ListPathsServlet.java?ref=8747d46d184a5764624131f0f9412602b3a24d9d",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hdfs/server/namenode/ListPathsServlet.java",
                "patch": "@@ -152,7 +152,12 @@ public void doGet(HttpServletRequest request, HttpServletResponse response)\n       while (!pathstack.empty()) {\n         String p = pathstack.pop();\n         try {\n-          for (FileStatus i : nnproxy.getListing(p)) {\n+          FileStatus[] listing = nnproxy.getListing(p);\n+          if (listing == null) {\n+            LOG.warn(\"ListPathsServlet - Path \" + p + \" does not exist\");\n+            continue;\n+          }\n+          for (FileStatus i : listing) {\n             if (exclude.matcher(i.getPath().getName()).matches()\n                 || !filter.matcher(i.getPath().getName()).matches()) {\n               continue;",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/8747d46d184a5764624131f0f9412602b3a24d9d/src/java/org/apache/hadoop/hdfs/server/namenode/ListPathsServlet.java",
                "sha": "dd20aa1437a92f8ec9d3b07ff9b717446a088f42",
                "status": "modified"
            },
            {
                "additions": 136,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/8747d46d184a5764624131f0f9412602b3a24d9d/src/test/hdfs/org/apache/hadoop/hdfs/TestListPathServlet.java",
                "changes": 136,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/TestListPathServlet.java?ref=8747d46d184a5764624131f0f9412602b3a24d9d",
                "deletions": 0,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/TestListPathServlet.java",
                "patch": "@@ -0,0 +1,136 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.server.namenode.ListPathsServlet;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+/**\n+ * Test for {@link ListPathsServlet} that serves the URL\n+ * http://<namenodeaddress:httpport?/listPaths\n+ * \n+ * This test does not use the servlet directly. Instead it is based on\n+ * {@link HftpFileSystem}, which uses this servlet to implement\n+ * {@link HftpFileSystem#listStatus(Path)} method.\n+ */\n+public class TestListPathServlet {\n+  private static final Configuration CONF = new HdfsConfiguration();\n+  private static MiniDFSCluster cluster;\n+  private static FileSystem fs;\n+  private static URI hftpURI;\n+  private static HftpFileSystem hftpFs;\n+  private Random r = new Random();\n+  private List<String> filelist = new ArrayList<String>();\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    // start a cluster with single datanode\n+    cluster = new MiniDFSCluster(CONF, 1, true, null);\n+    cluster.waitActive();\n+    fs = cluster.getFileSystem();\n+\n+    final String str = \"hftp://\"\n+        + CONF.get(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY);\n+    hftpURI = new URI(str);\n+    hftpFs = (HftpFileSystem) FileSystem.newInstance(hftpURI, CONF);\n+  }\n+\n+  @AfterClass\n+  public static void teardown() {\n+    cluster.shutdown();\n+  }\n+\n+  /** create a file with a length of <code>fileLen</code> */\n+  private void createFile(String fileName, long fileLen) throws IOException {\n+    filelist.add(hftpURI + fileName);\n+    final Path filePath = new Path(fileName);\n+    DFSTestUtil.createFile(fs, filePath, fileLen, (short) 1, r.nextLong());\n+  }\n+\n+  private void mkdirs(String dirName) throws IOException {\n+    filelist.add(hftpURI + dirName);\n+    fs.mkdirs(new Path(dirName));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    // Empty root directory\n+    checkStatus(\"/\");\n+\n+    // Root directory with files and directories\n+    createFile(\"/a\", 1);\n+    createFile(\"/b\", 1);\n+    mkdirs(\"/dir\");\n+    checkStatus(\"/\");\n+\n+    // A directory with files and directories\n+    createFile(\"/dir/a\", 1);\n+    createFile(\"/dir/b\", 1);\n+    mkdirs(\"/dir/dir1\");\n+    checkStatus(\"/dir\");\n+\n+    // Non existent path\n+    checkStatus(\"/nonexistent\");\n+    checkStatus(\"/nonexistent/a\");\n+  }\n+\n+  private void checkStatus(String listdir) throws IOException {\n+    final Path listpath = hftpFs.makeQualified(new Path(listdir));\n+    listdir = listpath.toString();\n+    final FileStatus[] statuslist = hftpFs.listStatus(listpath);\n+    for (String directory : filelist) {\n+      System.out.println(\"dir:\" + directory);\n+    }\n+    for (String file : filelist) {\n+      System.out.println(\"file:\" + file);\n+    }\n+    for (FileStatus status : statuslist) {\n+      System.out.println(\"status:\" + status.getPath().toString() + \" type \"\n+          + (status.isDir() ? \"directory\" : \"file\"));\n+    }\n+    for (String file : filelist) {\n+      boolean found = false;\n+      // Consider only file under the list path\n+      if (!file.startsWith(listpath.toString()) ||\n+          file.equals(listpath.toString())) {\n+        continue;\n+      }\n+      for (FileStatus status : statuslist) {\n+        if (status.getPath().toString().equals(file)) {\n+          found = true;\n+          break;\n+        }\n+      }\n+      Assert.assertTrue(\"Directory/file not returned in list status \" + file,\n+          found);\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/8747d46d184a5764624131f0f9412602b3a24d9d/src/test/hdfs/org/apache/hadoop/hdfs/TestListPathServlet.java",
                "sha": "3e1148fcc79fff827bea00fbbeeeb71cc73debd2",
                "status": "added"
            }
        ],
        "message": "HDFS-625. Fix NullPointerException thrown from ListPathServlet. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@829990 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/2fb7cc7cf9b48ac15ce7053bb23de9d50f8d80ed",
        "patched_files": [
            "ListPathsServlet.java",
            "CHANGES.java"
        ],
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "TestListPathServlet.java"
        ]
    },
    "hadoop-hdfs_88a0e02": {
        "bug_id": "hadoop-hdfs_88a0e02",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -287,6 +287,9 @@ Trunk (unreleased changes)\n \n   IMPROVEMENTS\n \n+    HDFS-1934. Fix NullPointerException when certain File APIs return null\n+    (Bharath Mundlapudi via mattf)\n+\n     HDFS-1510. Added test-patch.properties required by test-patch.sh (nigel)\n \n     HDFS-1628. Display full path in AccessControlException.  (John George",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/CHANGES.txt",
                "sha": "b5791e72f8dba408e2e601a225900db85354175d",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java?ref=88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hdfs.server.datanode;\n \n import java.io.File;\n+import java.io.IOException;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n@@ -38,11 +39,13 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.protocol.Block;\n import org.apache.hadoop.hdfs.server.common.GenerationStamp;\n import org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume;\n import org.apache.hadoop.util.Daemon;\n+import org.apache.hadoop.util.StringUtils;\n \n /**\n  * Periodically scans the data directories for block and block metadata files.\n@@ -480,9 +483,15 @@ public ScanInfoPerBlockPool call() throws Exception {\n     /** Compile list {@link ScanInfo} for the blocks in the directory <dir> */\n     private LinkedList<ScanInfo> compileReport(FSVolume vol, File dir,\n         LinkedList<ScanInfo> report) {\n-      File[] files = dir.listFiles();\n+      File[] files;\n+      try {\n+        files = FileUtil.listFiles(dir);\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Exception occured while compiling report: \", ioe);\n+        // Ignore this directory and proceed.\n+        return report;\n+      }\n       Arrays.sort(files);\n-\n       /*\n        * Assumption: In the sorted list of files block file appears immediately\n        * before block metadata file. This is true for the current naming",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "sha": "9b38a5f3f3d1b5d4d483712fc4092e6ee638c338",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java?ref=88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
                "deletions": 7,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "patch": "@@ -99,7 +99,7 @@ public FSDir(File dir)\n                                 dir.toString());\n         }\n       } else {\n-        File[] files = dir.listFiles();\n+        File[] files = FileUtil.listFiles(dir); \n         int numChildren = 0;\n         for (int idx = 0; idx < files.length; idx++) {\n           if (files[idx].isDirectory()) {\n@@ -187,7 +187,7 @@ void getVolumeMap(String bpid, ReplicasMap volumeMap, FSVolume volume)\n      * original file name; otherwise the tmp file is deleted.\n      */\n     private void recoverTempUnlinkedBlock() throws IOException {\n-      File files[] = dir.listFiles();\n+      File files[] = FileUtil.listFiles(dir);\n       for (File file : files) {\n         if (!FSDataset.isUnlinkTmpFile(file)) {\n           continue;\n@@ -420,9 +420,9 @@ void getVolumeMap(ReplicasMap volumeMap) throws IOException {\n      * @param isFinalized true if the directory has finalized replicas;\n      *                    false if the directory has rbw replicas\n      */\n-    private void addToReplicasMap(ReplicasMap volumeMap, \n-        File dir, boolean isFinalized) {\n-      File blockFiles[] = dir.listFiles();\n+    private void addToReplicasMap(ReplicasMap volumeMap, File dir,\n+        boolean isFinalized) throws IOException {\n+      File blockFiles[] = FileUtil.listFiles(dir);\n       for (File blockFile : blockFiles) {\n         if (!Block.isBlockFilename(blockFile))\n           continue;\n@@ -756,15 +756,15 @@ private void deleteBPDirectories(String bpid, boolean force)\n           throw new IOException(\"Failed to delete \" + finalizedDir);\n         }\n         FileUtil.fullyDelete(tmpDir);\n-        for (File f : bpCurrentDir.listFiles()) {\n+        for (File f : FileUtil.listFiles(bpCurrentDir)) {\n           if (!f.delete()) {\n             throw new IOException(\"Failed to delete \" + f);\n           }\n         }\n         if (!bpCurrentDir.delete()) {\n           throw new IOException(\"Failed to delete \" + bpCurrentDir);\n         }\n-        for (File f : bpDir.listFiles()) {\n+        for (File f : FileUtil.listFiles(bpDir)) {\n           if (!f.delete()) {\n             throw new IOException(\"Failed to delete \" + f);\n           }",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "sha": "41b47973883fd482cb6b8e4c5bb5ffbc8ba9cbe2",
                "status": "modified"
            }
        ],
        "message": "HDFS-1934. Fix NullPointerException when certain File APIs return null. Contributed by Bharath Mundlapudi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1130262 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/1d79d91876e75ec025028b216971b65b6ae1c581",
        "patched_files": [
            "DirectoryScanner.java"
        ],
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "TestDirectoryScanner.java"
        ]
    },
    "hadoop-hdfs_b72e15a": {
        "bug_id": "hadoop-hdfs_b72e15a",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/b72e15aa512a6bd1cd948ea76add1ad465478b8a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -526,6 +526,9 @@ Trunk (unreleased changes)\n     HDFS-1827. Fix timeout problem in TestBlockReplacement.  (Matt Foley\n     via szetszwo)\n \n+    HDFS-1908. Fix a NullPointerException in fi.DataTransferTestUtil.\n+    (szetszwo)\n+\n Release 0.22.0 - Unreleased\n \n   NEW FEATURES",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/CHANGES.txt",
                "sha": "5b8bd0515e314c6b3a9931e339dce7f659b325e7",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "deletions": 30,
                "filename": "src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.hadoop.fi.FiTestUtil.CountdownConstraint;\n import org.apache.hadoop.fi.FiTestUtil.MarkerConstraint;\n import org.apache.hadoop.hdfs.protocol.DatanodeID;\n-import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;\n \n@@ -55,7 +54,7 @@ public static DataTransferTest getDataTransferTest() {\n    * and some actions.\n    */\n   public static class DataTransferTest implements PipelineTest {\n-    private List<Pipeline> pipelines = new ArrayList<Pipeline>();\n+    private final List<Pipeline> pipelines = new ArrayList<Pipeline>();\n     private volatile boolean isSuccess = false;\n \n     /** Simulate action for the receiverOpWriteBlock pointcut */\n@@ -101,7 +100,8 @@ public void markSuccess() {\n     }\n \n     /** Initialize the pipeline. */\n-    public Pipeline initPipeline(LocatedBlock lb) {\n+    @Override\n+    public synchronized Pipeline initPipeline(LocatedBlock lb) {\n       final Pipeline pl = new Pipeline(lb);\n       if (pipelines.contains(pl)) {\n         throw new IllegalStateException(\"thepipeline != null\");\n@@ -110,20 +110,31 @@ public Pipeline initPipeline(LocatedBlock lb) {\n       return pl;\n     }\n \n-    /** Return the pipeline. */\n-    public Pipeline getPipeline(DatanodeID id) {\n-      if (pipelines == null) {\n-        throw new IllegalStateException(\"thepipeline == null\");\n-      }\n-      StringBuilder dnString = new StringBuilder();\n-      for (Pipeline pipeline : pipelines) {\n-        for (DatanodeInfo dni : pipeline.getDataNodes())\n-          dnString.append(dni.getStorageID());\n-        if (dnString.toString().contains(id.getStorageID()))\n-          return pipeline;\n+    /** Return the pipeline for the datanode. */\n+    @Override\n+    public synchronized Pipeline getPipelineForDatanode(DatanodeID id) {\n+      for (Pipeline p : pipelines) {\n+        if (p.contains(id)){\n+          return p;\n+        }\n       }\n+      FiTestUtil.LOG.info(\"FI: pipeline not found; id=\" + id\n+          + \", pipelines=\" + pipelines);\n       return null;\n     }\n+\n+    /**\n+     * Is the test not yet success\n+     * and the last pipeline contains the given datanode?\n+     */\n+    private synchronized boolean isNotSuccessAndLastPipelineContains(\n+        int index, DatanodeID id) {\n+      if (isSuccess()) {\n+        return false;\n+      }\n+      final int n = pipelines.size();\n+      return n == 0? false: pipelines.get(n-1).contains(index, id);\n+    }\n   }\n \n   /** Action for DataNode */\n@@ -171,8 +182,7 @@ public DatanodeMarkingAction(String currentTest, int index,\n     @Override\n     public void run(DatanodeID datanodeid) throws IOException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(datanodeid);\n-      if (p.contains(index, datanodeid)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, datanodeid)) {\n         marker.mark();\n       }\n     }\n@@ -193,8 +203,7 @@ public OomAction(String currentTest, int i) {\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (!test.isSuccess() && p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new OutOfMemoryError(s);\n@@ -215,8 +224,8 @@ public CountdownOomAction(String currentTest, int i, int count) {\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id) && countdown.isSatisfied()) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)\n+          && countdown.isSatisfied()) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new OutOfMemoryError(s);\n@@ -234,8 +243,7 @@ public DoosAction(String currentTest, int i) {\n     @Override\n     public void run(DatanodeID id) throws DiskOutOfSpaceException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new DiskOutOfSpaceException(s);\n@@ -256,8 +264,7 @@ public IoeAction(String currentTest, int i, String error) {\n     @Override\n     public void run(DatanodeID id) throws IOException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new IOException(s);\n@@ -284,8 +291,8 @@ public CountdownDoosAction(String currentTest, int i, int count) {\n     @Override\n     public void run(DatanodeID id) throws DiskOutOfSpaceException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id) && countdown.isSatisfied()) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)\n+          && countdown.isSatisfied()) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new DiskOutOfSpaceException(s);\n@@ -339,8 +346,7 @@ public SleepAction(String currentTest, int i,\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (!test.isSuccess() && p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         FiTestUtil.LOG.info(toString(id));\n         if (maxDuration <= 0) {\n           for(; FiTestUtil.sleep(1000); ); //sleep forever until interrupt\n@@ -385,8 +391,8 @@ public CountdownSleepAction(String currentTest, int i,\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id) && countdown.isSatisfied()) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)\n+          && countdown.isSatisfied()) {\n         final String s = toString(id) + \", duration = [\"\n         + minDuration + \",\" + maxDuration + \")\";\n         FiTestUtil.LOG.info(s);",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java",
                "sha": "4724595d4af6fceb707ea826818795342e38b82c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "deletions": 2,
                "filename": "src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java",
                "patch": "@@ -44,9 +44,8 @@ public DerrAction(String currentTest, int index) {\n \n     /** {@inheritDoc} */\n     public void run(DatanodeID id) throws IOException {\n-      final Pipeline p = getPipelineTest().getPipeline(id);\n+      final Pipeline p = getPipelineTest().getPipelineForDatanode(id);\n       if (p == null) {\n-        FiTestUtil.LOG.info(\"FI: couldn't find a pipeline for \" + id);\n         return;\n       }\n       if (p.contains(index, id)) {",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java",
                "sha": "0df95abde5a8cfc9f83e6e716493317fb34a2717",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/Pipeline.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/aop/org/apache/hadoop/fi/Pipeline.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "deletions": 8,
                "filename": "src/test/aop/org/apache/hadoop/fi/Pipeline.java",
                "patch": "@@ -26,26 +26,24 @@\n \n public class Pipeline {\n   private final List<String> datanodes = new ArrayList<String>();\n-  private DatanodeInfo[] nodes;\n \n   Pipeline(LocatedBlock lb) {\n     for(DatanodeInfo d : lb.getLocations()) {\n       datanodes.add(d.getName());\n     }\n-    nodes = lb.getLocations();\n+  }\n+\n+  /** Does the pipeline contains d? */\n+  public boolean contains(DatanodeID d) {\n+    return datanodes.contains(d.getName());\n   }\n \n   /** Does the pipeline contains d at the n th position? */\n   public boolean contains(int n, DatanodeID d) {\n     return d.getName().equals(datanodes.get(n));\n   }\n \n-  /** Returns DatanodeInfo[] of the nodes of the constructed pipiline*/\n-  public DatanodeInfo[] getDataNodes () {\n-    return nodes;\n-  }\n-\n-  /** {@inheritDoc} */\n+  @Override\n   public String toString() {\n     return getClass().getSimpleName() + datanodes;\n   }",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/Pipeline.java",
                "sha": "877b100e4c6e6a1fd589f9d51060dce098b3b135",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/PipelineTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/aop/org/apache/hadoop/fi/PipelineTest.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "deletions": 1,
                "filename": "src/test/aop/org/apache/hadoop/fi/PipelineTest.java",
                "patch": "@@ -23,5 +23,5 @@\n /** A pipeline contains a list of datanodes. */\n public interface PipelineTest {\n   public Pipeline initPipeline(LocatedBlock lb);\n-  public Pipeline getPipeline(DatanodeID id);\n+  public Pipeline getPipelineForDatanode(DatanodeID id);\n }",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/PipelineTest.java",
                "sha": "838d5b99d29f33546a9da9a93d4a302e46ffe428",
                "status": "modified"
            }
        ],
        "message": "HDFS-1908. Fix a NullPointerException in fi.DataTransferTestUtil.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1101675 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/06568fb9f4bbc9492b81dabc79087672e9b78697",
        "patched_files": [
            "DataTransferTestUtil.java",
            "Pipeline.java",
            "CHANGES.java",
            "FiHFlushTestUtil.java"
        ],
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "PipelineTest.java"
        ]
    },
    "hadoop-hdfs_be31fd0": {
        "bug_id": "hadoop-hdfs_be31fd0",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/be31fd05dae345e62772ecc235f14f3f59c37e04",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -471,6 +471,8 @@ Release 0.22.0 - Unreleased\n     HDFS-1560. dfs.data.dir permissions should default to 700. \n     (Todd Lipcon via eli)\n \n+    HDFS-1550. NPE when listing a file with no location. (hairong)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/CHANGES.txt",
                "sha": "71ebb27b84bef74df28bb21ef9fa2bd398da28cc",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -204,6 +204,9 @@ public static String byteArray2String(byte[][] pathComponents) {\n     }\n     int nrBlocks = blocks.locatedBlockCount();\n     BlockLocation[] blkLocations = new BlockLocation[nrBlocks];\n+    if (nrBlocks == 0) {\n+      return blkLocations;\n+    }\n     int idx = 0;\n     for (LocatedBlock blk : blocks.getLocatedBlocks()) {\n       assert idx < nrBlocks : \"Incorrect index\";",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "6975c53a25cd37604d11b0327e0db7a473aa136c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "patch": "@@ -19,6 +19,8 @@\n package org.apache.hadoop.hdfs;\n \n import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n \n import java.util.Arrays;\n@@ -65,5 +67,9 @@ public void testLocatedBlocks2Locations() {\n \n     assertTrue(\"expected 1 corrupt files but got \" + corruptCount, \n                corruptCount == 1);\n+    \n+    // test an empty location\n+    bs = DFSUtil.locatedBlocks2Locations(new LocatedBlocks());\n+    assertEquals(0, bs.length);\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "sha": "03e6b39c735c4110059773a65fe0ee61de79ec4d",
                "status": "modified"
            }
        ],
        "message": "HDFS-1550. NPE when listing a file with no location. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1054807 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/d1d9265d1758813b62c20157297e8cfd824628ee",
        "patched_files": [
            "DFSUtil.java",
            "CHANGES.java"
        ],
        "repo": "hadoop-hdfs",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    }
}