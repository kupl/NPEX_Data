{
    "hadoop-hdfs_88a0e02": {
        "repo": "hadoop-hdfs",
        "message": "HDFS-1934. Fix NullPointerException when certain File APIs return null. Contributed by Bharath Mundlapudi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1130262 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/1d79d91876e75ec025028b216971b65b6ae1c581",
        "bug_id": "hadoop-hdfs_88a0e02",
        "file": [
            {
                "sha": "b5791e72f8dba408e2e601a225900db85354175d",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
                "patch": "@@ -287,6 +287,9 @@ Trunk (unreleased changes)\n \n   IMPROVEMENTS\n \n+    HDFS-1934. Fix NullPointerException when certain File APIs return null\n+    (Bharath Mundlapudi via mattf)\n+\n     HDFS-1510. Added test-patch.properties required by test-patch.sh (nigel)\n \n     HDFS-1628. Display full path in AccessControlException.  (John George",
                "deletions": 0
            },
            {
                "sha": "9b38a5f3f3d1b5d4d483712fc4092e6ee638c338",
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "status": "modified",
                "changes": 13,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java?ref=88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hdfs.server.datanode;\n \n import java.io.File;\n+import java.io.IOException;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n@@ -38,11 +39,13 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.protocol.Block;\n import org.apache.hadoop.hdfs.server.common.GenerationStamp;\n import org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume;\n import org.apache.hadoop.util.Daemon;\n+import org.apache.hadoop.util.StringUtils;\n \n /**\n  * Periodically scans the data directories for block and block metadata files.\n@@ -480,9 +483,15 @@ public ScanInfoPerBlockPool call() throws Exception {\n     /** Compile list {@link ScanInfo} for the blocks in the directory <dir> */\n     private LinkedList<ScanInfo> compileReport(FSVolume vol, File dir,\n         LinkedList<ScanInfo> report) {\n-      File[] files = dir.listFiles();\n+      File[] files;\n+      try {\n+        files = FileUtil.listFiles(dir);\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Exception occured while compiling report: \", ioe);\n+        // Ignore this directory and proceed.\n+        return report;\n+      }\n       Arrays.sort(files);\n-\n       /*\n        * Assumption: In the sorted list of files block file appears immediately\n        * before block metadata file. This is true for the current naming",
                "deletions": 2
            },
            {
                "sha": "41b47973883fd482cb6b8e4c5bb5ffbc8ba9cbe2",
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "status": "modified",
                "changes": 14,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java?ref=88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
                "patch": "@@ -99,7 +99,7 @@ public FSDir(File dir)\n                                 dir.toString());\n         }\n       } else {\n-        File[] files = dir.listFiles();\n+        File[] files = FileUtil.listFiles(dir); \n         int numChildren = 0;\n         for (int idx = 0; idx < files.length; idx++) {\n           if (files[idx].isDirectory()) {\n@@ -187,7 +187,7 @@ void getVolumeMap(String bpid, ReplicasMap volumeMap, FSVolume volume)\n      * original file name; otherwise the tmp file is deleted.\n      */\n     private void recoverTempUnlinkedBlock() throws IOException {\n-      File files[] = dir.listFiles();\n+      File files[] = FileUtil.listFiles(dir);\n       for (File file : files) {\n         if (!FSDataset.isUnlinkTmpFile(file)) {\n           continue;\n@@ -420,9 +420,9 @@ void getVolumeMap(ReplicasMap volumeMap) throws IOException {\n      * @param isFinalized true if the directory has finalized replicas;\n      *                    false if the directory has rbw replicas\n      */\n-    private void addToReplicasMap(ReplicasMap volumeMap, \n-        File dir, boolean isFinalized) {\n-      File blockFiles[] = dir.listFiles();\n+    private void addToReplicasMap(ReplicasMap volumeMap, File dir,\n+        boolean isFinalized) throws IOException {\n+      File blockFiles[] = FileUtil.listFiles(dir);\n       for (File blockFile : blockFiles) {\n         if (!Block.isBlockFilename(blockFile))\n           continue;\n@@ -756,15 +756,15 @@ private void deleteBPDirectories(String bpid, boolean force)\n           throw new IOException(\"Failed to delete \" + finalizedDir);\n         }\n         FileUtil.fullyDelete(tmpDir);\n-        for (File f : bpCurrentDir.listFiles()) {\n+        for (File f : FileUtil.listFiles(bpCurrentDir)) {\n           if (!f.delete()) {\n             throw new IOException(\"Failed to delete \" + f);\n           }\n         }\n         if (!bpCurrentDir.delete()) {\n           throw new IOException(\"Failed to delete \" + bpCurrentDir);\n         }\n-        for (File f : bpDir.listFiles()) {\n+        for (File f : FileUtil.listFiles(bpDir)) {\n           if (!f.delete()) {\n             throw new IOException(\"Failed to delete \" + f);\n           }",
                "deletions": 7
            }
        ],
        "patched_files": [
            "DirectoryScanner.java"
        ],
        "unit_tests": [
            "TestDirectoryScanner.java"
        ]
    },
    "hadoop-hdfs_28c1b2a": {
        "repo": "hadoop-hdfs",
        "message": "HDFS-1615. seek() on closed DFS input stream throws NullPointerException. Contributed by Scott Carey.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1102094 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/1abbfe982f977c605db25b1bd9933e1d1a251de8",
        "bug_id": "hadoop-hdfs_28c1b2a",
        "file": [
            {
                "sha": "fda22ec7b56dd6259763dc18fe543538ce17f4fc",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
                "patch": "@@ -930,6 +930,9 @@ Release 0.22.0 - Unreleased\n     HDFS-1544. Ivy resolve force mode should be turned off by default.\n     (Luke Lu via tomwhite)\n \n+    HDFS-1615. seek() on closed DFS input stream throws NullPointerException\n+    (Scott Carey via todd)\n+\n Release 0.21.1 - Unreleased\n     HDFS-1466. TestFcHdfsSymlink relies on /tmp/test not existing. (eli)\n ",
                "deletions": 0
            },
            {
                "sha": "b74b55dc2226ce1404d5478214bd56fb3180248c",
                "filename": "src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
                "patch": "@@ -741,6 +741,9 @@ public synchronized void seek(long targetPos) throws IOException {\n     if (targetPos > getFileLength()) {\n       throw new IOException(\"Cannot seek after EOF\");\n     }\n+    if (closed) {\n+      throw new IOException(\"Stream is closed!\");\n+    }\n     boolean done = false;\n     if (pos <= targetPos && targetPos <= blockEnd) {\n       //",
                "deletions": 0
            },
            {
                "sha": "5184ceb782d552c16fb66f76c3649110ae1c30a6",
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "status": "modified",
                "changes": 39,
                "additions": 39,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java?ref=28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
                "patch": "@@ -100,6 +100,45 @@ public void testDFSClose() throws Exception {\n       if (cluster != null) {cluster.shutdown();}\n     }\n   }\n+  \n+  @Test\n+  public void testDFSSeekExceptions() throws IOException {\n+    Configuration conf = getTestConfiguration();\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+    FileSystem fileSys = cluster.getFileSystem();\n+\n+    try {\n+      String file = \"/test/fileclosethenseek/file-0\";\n+      Path path = new Path(file);\n+      // create file\n+      FSDataOutputStream output = fileSys.create(path);\n+      output.writeBytes(\"Some test data to write longer than 10 bytes\");\n+      output.close();\n+      FSDataInputStream input = fileSys.open(path);\n+      input.seek(10);\n+      boolean threw = false;\n+      try {\n+        input.seek(100);\n+      } catch (IOException e) {\n+        // success\n+        threw = true;\n+      }\n+      assertTrue(\"Failed to throw IOE when seeking past end\", threw);\n+      input.close();\n+      threw = false;\n+      try {\n+        input.seek(1);\n+      } catch (IOException e) {\n+        //success\n+        threw = true;\n+      }\n+      assertTrue(\"Failed to throw IOE when seeking after close\", threw);\n+      fileSys.close();\n+    }\n+    finally {\n+      if (cluster != null) {cluster.shutdown();}\n+    }\n+  }\n \n   @Test\n   public void testDFSClient() throws Exception {",
                "deletions": 0
            }
        ],
        "patched_files": [
            "DistributedFileSystem.java",
            "DFSInputStream.java"
        ],
        "unit_tests": [
            "TestDistributedFileSystem.java"
        ]
    },
    "hadoop-hdfs_74d19b5": {
        "repo": "hadoop-hdfs",
        "message": "Fix NullPointerException in Secondary NameNode. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1102465 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/01ffd46e3fec9037c17e887f932fb1765c1ab219",
        "bug_id": "hadoop-hdfs_74d19b5",
        "file": [
            {
                "sha": "08df62a988f388de0d26b89bc0da42f8e25f6398",
                "filename": "src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java?ref=74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f",
                "patch": "@@ -98,6 +98,7 @@\n   private int imagePort;\n   private String infoBindAddress;\n \n+  private FSNamesystem namesystem;\n   private Collection<URI> checkpointDirs;\n   private Collection<URI> checkpointEditsDirs;\n   private long checkpointPeriod;    // in seconds\n@@ -481,8 +482,9 @@ private void startCheckpoint() throws IOException {\n    */\n   private void doMerge(CheckpointSignature sig, boolean loadImage)\n   throws IOException {\n-    FSNamesystem namesystem = \n-            new FSNamesystem(checkpointImage, conf);\n+    if (loadImage) {\n+      namesystem = new FSNamesystem(checkpointImage, conf);\n+    }\n     assert namesystem.dir.fsImage == checkpointImage;\n     checkpointImage.doMerge(sig, loadImage);\n   }",
                "deletions": 2
            },
            {
                "sha": "2187a9ef59fe05e92eb18865e45a7fde1785451a",
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java",
                "status": "modified",
                "changes": 7,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java?ref=74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f",
                "patch": "@@ -684,6 +684,7 @@ public void testCheckpoint() throws IOException {\n     cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).build();\n     cluster.waitActive();\n     fileSys = cluster.getFileSystem();\n+    Path tmpDir = new Path(\"/tmp_tmp\");\n     try {\n       // check that file1 still exists\n       checkFile(fileSys, file1, replication);\n@@ -698,6 +699,11 @@ public void testCheckpoint() throws IOException {\n       //\n       SecondaryNameNode secondary = startSecondaryNameNode(conf);\n       secondary.doCheckpoint();\n+      \n+      fileSys.delete(tmpDir, true);\n+      fileSys.mkdirs(tmpDir);\n+      secondary.doCheckpoint();\n+      \n       secondary.shutdown();\n     } finally {\n       fileSys.close();\n@@ -713,6 +719,7 @@ public void testCheckpoint() throws IOException {\n     fileSys = cluster.getFileSystem();\n \n     assertTrue(!fileSys.exists(file1));\n+    assertTrue(fileSys.exists(tmpDir));\n \n     try {\n       // verify that file2 exists",
                "deletions": 0
            }
        ],
        "patched_files": [
            "SecondaryNameNode.java"
        ],
        "unit_tests": [
            "TestCheckpoint.java"
        ]
    },
    "hadoop-hdfs_b72e15a": {
        "repo": "hadoop-hdfs",
        "message": "HDFS-1908. Fix a NullPointerException in fi.DataTransferTestUtil.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1101675 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hadoop-hdfs/commit/b72e15aa512a6bd1cd948ea76add1ad465478b8a",
        "parent": "https://github.com/apache/hadoop-hdfs/commit/06568fb9f4bbc9492b81dabc79087672e9b78697",
        "bug_id": "hadoop-hdfs_b72e15a",
        "file": [
            {
                "sha": "5b8bd0515e314c6b3a9931e339dce7f659b325e7",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/CHANGES.txt",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/CHANGES.txt?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "patch": "@@ -526,6 +526,9 @@ Trunk (unreleased changes)\n     HDFS-1827. Fix timeout problem in TestBlockReplacement.  (Matt Foley\n     via szetszwo)\n \n+    HDFS-1908. Fix a NullPointerException in fi.DataTransferTestUtil.\n+    (szetszwo)\n+\n Release 0.22.0 - Unreleased\n \n   NEW FEATURES",
                "deletions": 0
            },
            {
                "sha": "4724595d4af6fceb707ea826818795342e38b82c",
                "filename": "src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java",
                "status": "modified",
                "changes": 66,
                "additions": 36,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.hadoop.fi.FiTestUtil.CountdownConstraint;\n import org.apache.hadoop.fi.FiTestUtil.MarkerConstraint;\n import org.apache.hadoop.hdfs.protocol.DatanodeID;\n-import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;\n \n@@ -55,7 +54,7 @@ public static DataTransferTest getDataTransferTest() {\n    * and some actions.\n    */\n   public static class DataTransferTest implements PipelineTest {\n-    private List<Pipeline> pipelines = new ArrayList<Pipeline>();\n+    private final List<Pipeline> pipelines = new ArrayList<Pipeline>();\n     private volatile boolean isSuccess = false;\n \n     /** Simulate action for the receiverOpWriteBlock pointcut */\n@@ -101,7 +100,8 @@ public void markSuccess() {\n     }\n \n     /** Initialize the pipeline. */\n-    public Pipeline initPipeline(LocatedBlock lb) {\n+    @Override\n+    public synchronized Pipeline initPipeline(LocatedBlock lb) {\n       final Pipeline pl = new Pipeline(lb);\n       if (pipelines.contains(pl)) {\n         throw new IllegalStateException(\"thepipeline != null\");\n@@ -110,20 +110,31 @@ public Pipeline initPipeline(LocatedBlock lb) {\n       return pl;\n     }\n \n-    /** Return the pipeline. */\n-    public Pipeline getPipeline(DatanodeID id) {\n-      if (pipelines == null) {\n-        throw new IllegalStateException(\"thepipeline == null\");\n-      }\n-      StringBuilder dnString = new StringBuilder();\n-      for (Pipeline pipeline : pipelines) {\n-        for (DatanodeInfo dni : pipeline.getDataNodes())\n-          dnString.append(dni.getStorageID());\n-        if (dnString.toString().contains(id.getStorageID()))\n-          return pipeline;\n+    /** Return the pipeline for the datanode. */\n+    @Override\n+    public synchronized Pipeline getPipelineForDatanode(DatanodeID id) {\n+      for (Pipeline p : pipelines) {\n+        if (p.contains(id)){\n+          return p;\n+        }\n       }\n+      FiTestUtil.LOG.info(\"FI: pipeline not found; id=\" + id\n+          + \", pipelines=\" + pipelines);\n       return null;\n     }\n+\n+    /**\n+     * Is the test not yet success\n+     * and the last pipeline contains the given datanode?\n+     */\n+    private synchronized boolean isNotSuccessAndLastPipelineContains(\n+        int index, DatanodeID id) {\n+      if (isSuccess()) {\n+        return false;\n+      }\n+      final int n = pipelines.size();\n+      return n == 0? false: pipelines.get(n-1).contains(index, id);\n+    }\n   }\n \n   /** Action for DataNode */\n@@ -171,8 +182,7 @@ public DatanodeMarkingAction(String currentTest, int index,\n     @Override\n     public void run(DatanodeID datanodeid) throws IOException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(datanodeid);\n-      if (p.contains(index, datanodeid)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, datanodeid)) {\n         marker.mark();\n       }\n     }\n@@ -193,8 +203,7 @@ public OomAction(String currentTest, int i) {\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (!test.isSuccess() && p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new OutOfMemoryError(s);\n@@ -215,8 +224,8 @@ public CountdownOomAction(String currentTest, int i, int count) {\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id) && countdown.isSatisfied()) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)\n+          && countdown.isSatisfied()) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new OutOfMemoryError(s);\n@@ -234,8 +243,7 @@ public DoosAction(String currentTest, int i) {\n     @Override\n     public void run(DatanodeID id) throws DiskOutOfSpaceException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new DiskOutOfSpaceException(s);\n@@ -256,8 +264,7 @@ public IoeAction(String currentTest, int i, String error) {\n     @Override\n     public void run(DatanodeID id) throws IOException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new IOException(s);\n@@ -284,8 +291,8 @@ public CountdownDoosAction(String currentTest, int i, int count) {\n     @Override\n     public void run(DatanodeID id) throws DiskOutOfSpaceException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id) && countdown.isSatisfied()) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)\n+          && countdown.isSatisfied()) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new DiskOutOfSpaceException(s);\n@@ -339,8 +346,7 @@ public SleepAction(String currentTest, int i,\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (!test.isSuccess() && p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         FiTestUtil.LOG.info(toString(id));\n         if (maxDuration <= 0) {\n           for(; FiTestUtil.sleep(1000); ); //sleep forever until interrupt\n@@ -385,8 +391,8 @@ public CountdownSleepAction(String currentTest, int i,\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id) && countdown.isSatisfied()) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)\n+          && countdown.isSatisfied()) {\n         final String s = toString(id) + \", duration = [\"\n         + minDuration + \",\" + maxDuration + \")\";\n         FiTestUtil.LOG.info(s);",
                "deletions": 30
            },
            {
                "sha": "0df95abde5a8cfc9f83e6e716493317fb34a2717",
                "filename": "src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java",
                "status": "modified",
                "changes": 3,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "patch": "@@ -44,9 +44,8 @@ public DerrAction(String currentTest, int index) {\n \n     /** {@inheritDoc} */\n     public void run(DatanodeID id) throws IOException {\n-      final Pipeline p = getPipelineTest().getPipeline(id);\n+      final Pipeline p = getPipelineTest().getPipelineForDatanode(id);\n       if (p == null) {\n-        FiTestUtil.LOG.info(\"FI: couldn't find a pipeline for \" + id);\n         return;\n       }\n       if (p.contains(index, id)) {",
                "deletions": 2
            },
            {
                "sha": "877b100e4c6e6a1fd589f9d51060dce098b3b135",
                "filename": "src/test/aop/org/apache/hadoop/fi/Pipeline.java",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/Pipeline.java",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/Pipeline.java",
                "status": "modified",
                "changes": 14,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/aop/org/apache/hadoop/fi/Pipeline.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "patch": "@@ -26,26 +26,24 @@\n \n public class Pipeline {\n   private final List<String> datanodes = new ArrayList<String>();\n-  private DatanodeInfo[] nodes;\n \n   Pipeline(LocatedBlock lb) {\n     for(DatanodeInfo d : lb.getLocations()) {\n       datanodes.add(d.getName());\n     }\n-    nodes = lb.getLocations();\n+  }\n+\n+  /** Does the pipeline contains d? */\n+  public boolean contains(DatanodeID d) {\n+    return datanodes.contains(d.getName());\n   }\n \n   /** Does the pipeline contains d at the n th position? */\n   public boolean contains(int n, DatanodeID d) {\n     return d.getName().equals(datanodes.get(n));\n   }\n \n-  /** Returns DatanodeInfo[] of the nodes of the constructed pipiline*/\n-  public DatanodeInfo[] getDataNodes () {\n-    return nodes;\n-  }\n-\n-  /** {@inheritDoc} */\n+  @Override\n   public String toString() {\n     return getClass().getSimpleName() + datanodes;\n   }",
                "deletions": 8
            },
            {
                "sha": "838d5b99d29f33546a9da9a93d4a302e46ffe428",
                "filename": "src/test/aop/org/apache/hadoop/fi/PipelineTest.java",
                "blob_url": "https://github.com/apache/hadoop-hdfs/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/PipelineTest.java",
                "raw_url": "https://github.com/apache/hadoop-hdfs/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/PipelineTest.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-hdfs/contents/src/test/aop/org/apache/hadoop/fi/PipelineTest.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "patch": "@@ -23,5 +23,5 @@\n /** A pipeline contains a list of datanodes. */\n public interface PipelineTest {\n   public Pipeline initPipeline(LocatedBlock lb);\n-  public Pipeline getPipeline(DatanodeID id);\n+  public Pipeline getPipelineForDatanode(DatanodeID id);\n }",
                "deletions": 1
            }
        ],
        "patched_files": [
            "FiHFlushTestUtil.java",
            "DataTransferTestUtil.java",
            "Pipeline.java"
        ],
        "unit_tests": [
            "PipelineTest.java"
        ]
    }
}