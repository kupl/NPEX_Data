{
    "hbase_01db60d": {
        "bug_id": "hbase_01db60d",
        "commit": "https://github.com/apache/hbase/commit/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "deletions": 0,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java",
                "patch": "@@ -520,6 +520,9 @@ private void checkQueuesDeleted(String peerId) throws ReplicationException {\n     if (queuesClient == null) return;\n     try {\n       List<String> replicators = queuesClient.getListOfReplicators();\n+      if (replicators == null || replicators.isEmpty()) {\n+        return;\n+      }\n       for (String replicator : replicators) {\n         List<String> queueIds = queuesClient.getAllQueues(replicator);\n         for (String queueId : queueIds) {",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java",
                "sha": "751e45441f8006bb8dd1f65ef81bf976147b429c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java",
                "patch": "@@ -98,7 +98,7 @@ public void init() throws ReplicationException {\n     for (int retry = 0; ; retry++) {\n       int v0 = getQueuesZNodeCversion();\n       List<String> rss = getListOfReplicators();\n-      if (rss == null) {\n+      if (rss == null || rss.isEmpty()) {\n         LOG.debug(\"Didn't find any region server that replicates, won't prevent any deletions.\");\n         return ImmutableSet.of();\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java",
                "sha": "0115b6f812796382dad1af9fb7f299a150dfd95f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java",
                "patch": "@@ -77,6 +77,9 @@ public ReplicationZKNodeCleaner(Configuration conf, ZooKeeperWatcher zkw, Aborta\n     Set<String> peerIds = new HashSet<>(this.replicationPeers.getAllPeerIds());\n     try {\n       List<String> replicators = this.queuesClient.getListOfReplicators();\n+      if (replicators == null || replicators.isEmpty()) {\n+        return undeletedQueues;\n+      }\n       for (String replicator : replicators) {\n         List<String> queueIds = this.queuesClient.getAllQueues(replicator);\n         for (String queueId : queueIds) {",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java",
                "sha": "6d8962e9234b6fd1866319240cac3ba0caedd90d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
                "patch": "@@ -323,6 +323,9 @@ public String dumpQueues(ClusterConnection connection, ZooKeeperWatcher zkw, Set\n     // Loops each peer on each RS and dumps the queues\n     try {\n       List<String> regionservers = queuesClient.getListOfReplicators();\n+      if (regionservers == null || regionservers.isEmpty()) {\n+        return sb.toString();\n+      }\n       for (String regionserver : regionservers) {\n         List<String> queueIds = queuesClient.getAllQueues(regionserver);\n         replicationQueues.init(regionserver);",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
                "sha": "4bda75be366d7f77854a9262521f9ab20dafbea1",
                "status": "modified"
            }
        ],
        "message": "HBASE-18330 NPE in ReplicationZKLockCleanerChore",
        "parent": "https://github.com/apache/hbase/commit/5f54e28510fdbdc1a08688168f8df19904bcd975",
        "repo": "hbase",
        "unit_tests": [
            "TestDumpReplicationQueues.java"
        ]
    },
    "hbase_022f70b": {
        "bug_id": "hbase_022f70b",
        "commit": "https://github.com/apache/hbase/commit/022f70b7220fdd791fcb5eef23ac69852156201a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/022f70b7220fdd791fcb5eef23ac69852156201a/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=022f70b7220fdd791fcb5eef23ac69852156201a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -1056,6 +1056,8 @@ Release 0.21.0 - Unreleased\n                (Kannan Muthukkaruppan via Stack)\n    HBASE-3102  Enhance HBase rMetrics for Long-running Stats\n                (Nicolas Spiegelberg via Stack)\n+   HBASE-3169  NPE when master joins running cluster if a RIT references\n+               a RS no longer present\n \n   NEW FEATURES\n    HBASE-1961  HBase EC2 scripts",
                "raw_url": "https://github.com/apache/hbase/raw/022f70b7220fdd791fcb5eef23ac69852156201a/CHANGES.txt",
                "sha": "91446ab4b4ac2b53ee1292a817de93c02428bf64",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hbase/blob/022f70b7220fdd791fcb5eef23ac69852156201a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=022f70b7220fdd791fcb5eef23ac69852156201a",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -292,8 +292,17 @@ void processRegionsInTransition(final RegionTransitionData data,\n         // Region is opened, insert into RIT and handle it\n         regionsInTransition.put(encodedRegionName, new RegionState(\n             regionInfo, RegionState.State.OPENING, data.getStamp()));\n-        new OpenedRegionHandler(master, this, data, regionInfo,\n-            serverManager.getServerInfo(data.getServerName())).process();\n+        HServerInfo hsi = serverManager.getServerInfo(data.getServerName());\n+        // hsi could be null if this server is no longer online.  If\n+        // that the case, just let this RIT timeout; it'll be assigned\n+        // to new server then.\n+        if (hsi == null) {\n+          LOG.warn(\"Region in transition \" + regionInfo.getEncodedName() +\n+            \" references a server no longer up \" + data.getServerName() +\n+            \"; letting RIT timeout so will be assigned elsewhere\");\n+          break;\n+        }\n+        new OpenedRegionHandler(master, this, data, regionInfo, hsi).process();\n         break;\n       }\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/022f70b7220fdd791fcb5eef23ac69852156201a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "78fc7c3f4061d6bda3e15be7ae7f317a8fae1785",
                "status": "modified"
            }
        ],
        "message": "HBASE-3169  NPE when master joins running cluster if a RIT references a RS no longer present\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1028872 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/73727e534a8590d233255db780b6eb8784ce8beb",
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_02f6104": {
        "bug_id": "hbase_02f6104",
        "commit": "https://github.com/apache/hbase/commit/02f6104dc289a3b9d691fdf5abcd4bf226600610",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/02f6104dc289a3b9d691fdf5abcd4bf226600610/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=02f6104dc289a3b9d691fdf5abcd4bf226600610",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -455,6 +455,8 @@ Release 0.92.0 - Unreleased\n    HBASE-4745  LRU statistics thread should be a daemon\n    HBASE-4749  TestMasterFailover#testMasterFailoverWithMockedRITOnDeadRS\n                occasionally fails\n+   HBASE-4753  org.apache.hadoop.hbase.regionserver.TestHRegionInfo#testGetSetOfHTD\n+               throws NPE on trunk (nkeywal)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "raw_url": "https://github.com/apache/hbase/raw/02f6104dc289a3b9d691fdf5abcd4bf226600610/CHANGES.txt",
                "sha": "803c7eb6c04b8c94159126507eafb0816c205fa0",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/02f6104dc289a3b9d691fdf5abcd4bf226600610/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java?ref=02f6104dc289a3b9d691fdf5abcd4bf226600610",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java",
                "patch": "@@ -430,7 +430,9 @@ public static void deleteTableDescriptorIfExists(String tableName,\n     FileSystem fs = FSUtils.getCurrentFileSystem(conf);\n     FileStatus status = getTableInfoPath(fs, FSUtils.getRootDir(conf), tableName);\n     // The below deleteDirectory works for either file or directory.\n-    if (fs.exists(status.getPath())) FSUtils.deleteDirectory(fs, status.getPath());\n+    if (status != null && fs.exists(status.getPath()))  {\n+      FSUtils.deleteDirectory(fs, status.getPath());\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/02f6104dc289a3b9d691fdf5abcd4bf226600610/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java",
                "sha": "b2783aedbb08c62dc8a5ab1d1d4ed8ff09a88754",
                "status": "modified"
            }
        ],
        "message": "HBASE-4753 org.apache.hadoop.hbase.regionserver.TestHRegionInfo#testGetSetOfHTD throws NPE on trunk\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1198581 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/7d8d42d01bdfca946425cabf84dedecbf0d8602b",
        "repo": "hbase",
        "unit_tests": [
            "TestFSTableDescriptors.java"
        ]
    },
    "hbase_0caad46": {
        "bug_id": "hbase_0caad46",
        "commit": "https://github.com/apache/hbase/commit/0caad4616e2c67cc21e50ec4d8efce89f3469006",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hbase/blob/0caad4616e2c67cc21e50ec4d8efce89f3469006/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=0caad4616e2c67cc21e50ec4d8efce89f3469006",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -4112,7 +4112,6 @@ public Result get(final Get get, final Integer lockid) throws IOException {\n   private List<KeyValue> get(Get get, boolean withCoprocessor)\n   throws IOException {\n     long now = EnvironmentEdgeManager.currentTimeMillis();\n-    Scan scan = new Scan(get);\n \n     List<KeyValue> results = new ArrayList<KeyValue>();\n \n@@ -4123,8 +4122,6 @@ public Result get(final Get get, final Integer lockid) throws IOException {\n        }\n     }\n \n-    Scan scan = new Scan(get);\n-\n     RegionScanner scanner = null;\n     try {\n       scanner = getScanner(scan);",
                "raw_url": "https://github.com/apache/hbase/raw/0caad4616e2c67cc21e50ec4d8efce89f3469006/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "54675ffb7e3fae75eeaf0b688accd91f577f79d7",
                "status": "modified"
            }
        ],
        "message": "HBASE-5279 NPE in Master after upgrading to 0.92.0 -- REVERT OVERCOMMIT TO HREGION\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1245768 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/5994c5014382bb09d040c0f9e9e0aae13d025e8b",
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_0f7285a": {
        "bug_id": "hbase_0f7285a",
        "commit": "https://github.com/apache/hbase/commit/0f7285a81a75b14be937e411f949081d465d1b92",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hbase/blob/0f7285a81a75b14be937e411f949081d465d1b92/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=0f7285a81a75b14be937e411f949081d465d1b92",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "patch": "@@ -676,7 +676,17 @@ protected void cleanupCalls(long rpcTimeout) {\n         Call c = itor.next().getValue();\n         long waitTime = System.currentTimeMillis() - c.getStartTime();\n         if (waitTime >= rpcTimeout) {\n-          c.setException(closeException); // local exception\n+          if (this.closeException == null) {\n+            // There may be no exception in the case that there are many calls\n+            // being multiplexed over this connection and these are succeeding\n+            // fine while this Call object is taking a long time to finish\n+            // over on the server; e.g. I just asked the regionserver to bulk\n+            // open 3k regions or its a big fat multiput into a heavily-loaded\n+            // server (Perhaps this only happens at the extremes?)\n+            this.closeException = new CallTimeoutException(\"Call id=\" + c.id +\n+              \", waitTime=\" + waitTime + \", rpcTimetout=\" + rpcTimeout);\n+          }\n+          c.setException(this.closeException);\n           synchronized (c) {\n             c.notifyAll();\n           }\n@@ -705,6 +715,15 @@ protected void cleanupCalls(long rpcTimeout) {\n     }\n   }\n \n+  /**\n+   * Client-side call timeout\n+   */\n+  public static class CallTimeoutException extends IOException {\n+    public CallTimeoutException(final String msg) {\n+      super(msg);\n+    }\n+  }\n+\n   /** Call implementation used for parallel calls. */\n   protected class ParallelCall extends Call {\n     private final ParallelResults results;",
                "raw_url": "https://github.com/apache/hbase/raw/0f7285a81a75b14be937e411f949081d465d1b92/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "sha": "2602461c58e6278b6c1b41abf622c685f402def9",
                "status": "modified"
            }
        ],
        "message": "HBASE-4890 fix possible NPE in HConnectionManager\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1298272 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/edc6696bc5a611a20fb1e713eb00b48d2cb7dd87",
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseClient.java"
        ]
    },
    "hbase_13c5af3": {
        "bug_id": "hbase_13c5af3",
        "commit": "https://github.com/apache/hbase/commit/13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java?ref=13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2",
                "deletions": 3,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "patch": "@@ -60,9 +60,9 @@\n   // i.e. empty column and a timestamp of LATEST_TIMESTAMP.\n   protected final byte [] splitkey;\n \n-  protected final Cell splitCell;\n+  private final Cell splitCell;\n \n-  private Optional<Cell> firstKey = null;\n+  private Optional<Cell> firstKey = Optional.empty();\n \n   private boolean firstKeySeeked = false;\n \n@@ -269,7 +269,8 @@ public int reseekTo(Cell key) throws IOException {\n       public boolean seekBefore(Cell key) throws IOException {\n         if (top) {\n           Optional<Cell> fk = getFirstKey();\n-          if (PrivateCellUtil.compareKeyIgnoresMvcc(getComparator(), key, fk.get()) <= 0) {\n+          if (fk.isPresent() &&\n+                  PrivateCellUtil.compareKeyIgnoresMvcc(getComparator(), key, fk.get()) <= 0) {\n             return false;\n           }\n         } else {",
                "raw_url": "https://github.com/apache/hbase/raw/13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "sha": "11ab068ef3d15b358faeec46102b53ada57244d2",
                "status": "modified"
            }
        ],
        "message": "HBASE-22520 Avoid possible NPE while performing seekBefore in Hal\u2026 (#281)\n\nHBASE-22520 Avoid possible NPE while performing seekBefore in HalfStoreFileReader",
        "parent": "https://github.com/apache/hbase/commit/6ea2566ac3f1d1cb76cedf87f00cda6583013b2f",
        "repo": "hbase",
        "unit_tests": [
            "TestHalfStoreFileReader.java"
        ]
    },
    "hbase_157a60f": {
        "bug_id": "hbase_157a60f",
        "commit": "https://github.com/apache/hbase/commit/157a60f1b396ab9adc7f934a15352f2dbc5493a9",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/157a60f1b396ab9adc7f934a15352f2dbc5493a9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=157a60f1b396ab9adc7f934a15352f2dbc5493a9",
                "deletions": 12,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "patch": "@@ -17,10 +17,7 @@\n  */\n package org.apache.hadoop.hbase;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n-\n+import javax.annotation.Nullable;\n import java.io.File;\n import java.io.IOException;\n import java.io.OutputStream;\n@@ -47,8 +44,6 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n \n-import javax.annotation.Nullable;\n-\n import org.apache.commons.lang.RandomStringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -63,6 +58,7 @@\n import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.ClusterConnection;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.client.Consistency;\n@@ -88,7 +84,6 @@\n import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n import org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim;\n-import org.apache.hadoop.hbase.master.AssignmentManager;\n import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.RegionStates;\n import org.apache.hadoop.hbase.master.ServerManager;\n@@ -134,6 +129,10 @@\n import org.apache.zookeeper.ZooKeeper;\n import org.apache.zookeeper.ZooKeeper.States;\n \n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n /**\n  * Facility for testing HBase. Replacement for\n  * old HBaseTestCase and HBaseClusterTestCase functionality.\n@@ -3768,11 +3767,8 @@ public String explainFailure() throws IOException {\n \n       @Override\n       public boolean evaluate() throws IOException {\n-        HMaster master = getMiniHBaseCluster().getMaster();\n-        if (master == null) return false;\n-        AssignmentManager am = master.getAssignmentManager();\n-        if (am == null) return false;\n-        final RegionStates regionStates = am.getRegionStates();\n+        final RegionStates regionStates = getMiniHBaseCluster().getMaster()\n+            .getAssignmentManager().getRegionStates();\n         return !regionStates.isRegionsInTransition();\n       }\n     };",
                "raw_url": "https://github.com/apache/hbase/raw/157a60f1b396ab9adc7f934a15352f2dbc5493a9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "sha": "5bb25dbdc0efebb03d279ffa916bd87c35b5f3b9",
                "status": "modified"
            }
        ],
        "message": "Revert \"HBASE-14909 NPE testing for RIT\"\n\nThis reverts commit da0cc598feab995eed12527d90805dd627674035.",
        "parent": "https://github.com/apache/hbase/commit/35a7b56e530f3e4a12f1968df5aee9d3b63815bb",
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseTestingUtility.java"
        ]
    },
    "hbase_1758553": {
        "bug_id": "hbase_1758553",
        "commit": "https://github.com/apache/hbase/commit/1758553f45e4a38e493f0cb5dee37e483c33905a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/1758553f45e4a38e493f0cb5dee37e483c33905a/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=1758553f45e4a38e493f0cb5dee37e483c33905a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -803,6 +803,8 @@ Release 0.90.0 - Unreleased\n    HBASE-3343  Server not shutting down after losing log lease\n    HBASE-3381  Interrupt of a region open comes across as a successful open\n    HBASE-3386  NPE in TableRecordReaderImpl.restart\n+   HBASE-3388  NPE processRegionInTransition(AssignmentManager.java:264)\n+               doing rolling-restart.sh\n \n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hbase/raw/1758553f45e4a38e493f0cb5dee37e483c33905a/CHANGES.txt",
                "sha": "030ae6e4bed5625f79d3c5276e0bb2828ef839df",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/1758553f45e4a38e493f0cb5dee37e483c33905a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=1758553f45e4a38e493f0cb5dee37e483c33905a",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -261,8 +261,13 @@ boolean processRegionInTransition(final String encodedRegionName,\n   throws KeeperException, IOException {\n     RegionTransitionData data = ZKAssign.getData(watcher, encodedRegionName);\n     if (data == null) return false;\n-    HRegionInfo hri = (regionInfo != null)? regionInfo:\n-      MetaReader.getRegion(catalogTracker, data.getRegionName()).getFirst();\n+    HRegionInfo hri = regionInfo;\n+    if (hri == null) {\n+      Pair<HRegionInfo, HServerAddress> p =\n+        MetaReader.getRegion(catalogTracker, data.getRegionName());\n+      if (p == null) return false;\n+      hri = p.getFirst();\n+    }\n     processRegionsInTransition(data, hri);\n     return true;\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/1758553f45e4a38e493f0cb5dee37e483c33905a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "2b345fb2ff5007351702ceee493a4f9442240453",
                "status": "modified"
            }
        ],
        "message": "HBASE-3388 NPE processRegionInTransition(AssignmentManager.java:264) doing rolling-restart.sh\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1052058 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/bfe27f5764b6a9b1b23599e140b6c699bba572ed",
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_231e0c0": {
        "bug_id": "hbase_231e0c0",
        "commit": "https://github.com/apache/hbase/commit/231e0c0043ddbe5297cb8f0716e6de5a7381892d",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/231e0c0043ddbe5297cb8f0716e6de5a7381892d/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=231e0c0043ddbe5297cb8f0716e6de5a7381892d",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -362,6 +362,7 @@ Release 0.20.0 - Unreleased\n                hudson too\n    HBASE-1780  HTable.flushCommits clears write buffer in finally clause\n    HBASE-1784  Missing rows after medium intensity insert\n+   HBASE-1809  NPE thrown in BoundedRangeFileInputStream\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "raw_url": "https://github.com/apache/hbase/raw/231e0c0043ddbe5297cb8f0716e6de5a7381892d/CHANGES.txt",
                "sha": "b057b966b1a05ead1000a665159ecb2c3c7d3625",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hbase/blob/231e0c0043ddbe5297cb8f0716e6de5a7381892d/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java?ref=231e0c0043ddbe5297cb8f0716e6de5a7381892d",
                "deletions": 12,
                "filename": "src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "patch": "@@ -967,18 +967,26 @@ ByteBuffer readBlock(int block, boolean cacheBlock) throws IOException {\n     private ByteBuffer decompress(final long offset, final int compressedSize,\n       final int decompressedSize) \n     throws IOException {\n-      Decompressor decompressor = this.compressAlgo.getDecompressor();\n-      // My guess is that the bounded range fis is needed to stop the \n-      // decompressor reading into next block -- IIRC, it just grabs a\n-      // bunch of data w/o regard to whether decompressor is coming to end of a\n-      // decompression.\n-      InputStream is = this.compressAlgo.createDecompressionStream(\n-        new BoundedRangeFileInputStream(this.istream, offset, compressedSize),\n-        decompressor, 0);\n-      ByteBuffer buf = ByteBuffer.allocate(decompressedSize);\n-      IOUtils.readFully(is, buf.array(), 0, buf.capacity());\n-      is.close();\n-      this.compressAlgo.returnDecompressor(decompressor);\n+      \n+      Decompressor decompressor = null;\n+      \n+      try {\n+        decompressor = this.compressAlgo.getDecompressor();\n+        // My guess is that the bounded range fis is needed to stop the \n+        // decompressor reading into next block -- IIRC, it just grabs a\n+        // bunch of data w/o regard to whether decompressor is coming to end of a\n+        // decompression.\n+        InputStream is = this.compressAlgo.createDecompressionStream(\n+          new BoundedRangeFileInputStream(this.istream, offset, compressedSize),\n+          decompressor, 0);\n+        ByteBuffer buf = ByteBuffer.allocate(decompressedSize);\n+        IOUtils.readFully(is, buf.array(), 0, buf.capacity());\n+        is.close();        \n+      } finally {\n+        if (null != decompressor) {\n+          this.compressAlgo.returnDecompressor(decompressor);          \n+        }\n+      }\n       return buf;\n     }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/231e0c0043ddbe5297cb8f0716e6de5a7381892d/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "sha": "f67fea35dc4ad04a89a1386536972f4d3303426b",
                "status": "modified"
            }
        ],
        "message": "HBASE-1809 NPE thrown in BoundedRangeFileInputStream\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@810301 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/dea3c1207cd0b60ddb3bbda3ae120d5efba97121",
        "repo": "hbase",
        "unit_tests": [
            "TestHFile.java"
        ]
    },
    "hbase_261aa94": {
        "bug_id": "hbase_261aa94",
        "commit": "https://github.com/apache/hbase/commit/261aa9445c3c52e09c10d06168a77d11d0c9b4b4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/261aa9445c3c52e09c10d06168a77d11d0c9b4b4/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=261aa9445c3c52e09c10d06168a77d11d0c9b4b4",
                "deletions": 1,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "patch": "@@ -43,6 +43,7 @@\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n \n+import org.apache.commons.lang.StringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n@@ -325,7 +326,8 @@ static boolean checkTable(Admin admin, TestOptions opts) throws IOException {\n     // recreate the table when user has requested presplit or when existing\n     // {RegionSplitPolicy,replica count} does not match requested.\n     if ((exists && opts.presplitRegions != DEFAULT_OPTS.presplitRegions)\n-      || (!isReadCmd && desc != null && !desc.getRegionSplitPolicyClassName().equals(opts.splitPolicy))\n+      || (!isReadCmd && desc != null &&\n+          !StringUtils.equals(desc.getRegionSplitPolicyClassName(), opts.splitPolicy))\n       || (!isReadCmd && desc != null && desc.getRegionReplication() != opts.replicas)) {\n       needsDelete = true;\n       // wait, why did it delete my table?!?",
                "raw_url": "https://github.com/apache/hbase/raw/261aa9445c3c52e09c10d06168a77d11d0c9b4b4/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "sha": "40e50cfc60d91c7a21c660a0dcdd2af5ebdb254b",
                "status": "modified"
            }
        ],
        "message": "HBASE-17803 Addendum fix NPE",
        "parent": "https://github.com/apache/hbase/commit/23abc90068f0ea75f09c3eecf6ef758f1aee9219",
        "repo": "hbase",
        "unit_tests": [
            "TestPerformanceEvaluation.java"
        ]
    },
    "hbase_28a0350": {
        "bug_id": "hbase_28a0350",
        "commit": "https://github.com/apache/hbase/commit/28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hbase/blob/28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java?ref=28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -221,6 +222,24 @@ protected InternalScanner postCreateCoprocScanner(final CompactionRequest reques\n     return store.getCoprocessorHost().preCompact(store, scanner, scanType, request);\n   }\n \n+  /**\n+   * Used to prevent compaction name conflict when multiple compactions running parallel on the\n+   * same store.\n+   */\n+  private static final AtomicInteger NAME_COUNTER = new AtomicInteger(0);\n+\n+  private String generateCompactionName() {\n+    int counter;\n+    for (;;) {\n+      counter = NAME_COUNTER.get();\n+      int next = counter == Integer.MAX_VALUE ? 0 : counter + 1;\n+      if (NAME_COUNTER.compareAndSet(counter, next)) {\n+        break;\n+      }\n+    }\n+    return store.getRegionInfo().getRegionNameAsString() + \"#\"\n+        + store.getFamily().getNameAsString() + \"#\" + counter;\n+  }\n   /**\n    * Performs the compaction.\n    * @param scanner Where to read from.\n@@ -242,8 +261,7 @@ protected boolean performCompaction(InternalScanner scanner, CellSink writer,\n     if (LOG.isDebugEnabled()) {\n       lastMillis = EnvironmentEdgeManager.currentTime();\n     }\n-    String compactionName =\n-        store.getRegionInfo().getRegionNameAsString() + \"#\" + store.getFamily().getNameAsString();\n+    String compactionName = generateCompactionName();\n     long now = 0;\n     boolean hasMore;\n     ScannerContext scannerContext =",
                "raw_url": "https://github.com/apache/hbase/raw/28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java",
                "sha": "78a5cac659d152c7a67801e6a083d4af3af7150f",
                "status": "modified"
            }
        ],
        "message": "HBASE-13970 NPE during compaction in trunk",
        "parent": "https://github.com/apache/hbase/commit/272b025b25fed979da0e59ffd41615bbb9e105ea",
        "repo": "hbase",
        "unit_tests": [
            "TestCompactor.java"
        ]
    },
    "hbase_2a4f052": {
        "bug_id": "hbase_2a4f052",
        "commit": "https://github.com/apache/hbase/commit/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -93,6 +93,8 @@ Release 0.91.0 - Unreleased\n                cluster and was returning master hostname for rs to use\n    HBASE-3829  TestMasterFailover failures in jenkins\n    HBASE-3843  splitLogWorker starts too early (Prakash Khemani)\n+   HBASE-3838  RegionCoprocesorHost.preWALRestore throws npe in case there is\n+               no RegionObserver registered (Himanshu Vashishtha)\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "raw_url": "https://github.com/apache/hbase/raw/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/CHANGES.txt",
                "sha": "331392dc81ed4ddcb4156c11e3af4456162839ce",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java?ref=2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a",
                "deletions": 7,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java",
                "patch": "@@ -920,11 +920,12 @@ public boolean preWALRestore(HRegionInfo info, HLogKey logKey,\n         ctx = ObserverContext.createAndPrepare(env, ctx);\n         ((RegionObserver)env.getInstance()).preWALRestore(ctx, info, logKey,\n             logEdit);\n+        bypass |= ctx.shouldBypass();\n+        if (ctx.shouldComplete()) {\n+          break;\n+        }\n       }\n-      bypass |= ctx.shouldBypass();\n-      if (ctx.shouldComplete()) {\n-        break;\n-      }\n+     \n     }\n     return bypass;\n   }\n@@ -943,10 +944,11 @@ public void postWALRestore(HRegionInfo info, HLogKey logKey,\n         ctx = ObserverContext.createAndPrepare(env, ctx);\n         ((RegionObserver)env.getInstance()).postWALRestore(ctx, info,\n             logKey, logEdit);\n+        if (ctx.shouldComplete()) {\n+          break;\n+        }\n       }\n-      if (ctx.shouldComplete()) {\n-        break;\n-      }\n+      \n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java",
                "sha": "ac76db421126e8a49d1aa476b368d9e4f5bcbe78",
                "status": "modified"
            }
        ],
        "message": "HBASE-3838 RegionCoprocesorHost.preWALRestore throws npe in case there is no RegionObserver registered\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1098705 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/6c32f36250df2538598fcb7c8f99a615a8e4846d",
        "repo": "hbase",
        "unit_tests": [
            "TestRegionCoprocessorHost.java"
        ]
    },
    "hbase_2ff6d0f": {
        "bug_id": "hbase_2ff6d0f",
        "commit": "https://github.com/apache/hbase/commit/2ff6d0fe4789857ab51685949711d755dedd459a",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 3,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "patch": "@@ -23,6 +23,7 @@\n \n import java.io.DataInput;\n import java.io.DataOutput;\n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n@@ -2400,8 +2401,7 @@ public static KeyValue cloneAndAddTags(Cell c, List<Tag> newTags) {\n    * Create a KeyValue reading from the raw InputStream.\n    * Named <code>iscreate</code> so doesn't clash with {@link #create(DataInput)}\n    * @param in\n-   * @return Created KeyValue OR if we find a length of zero, we will return null which\n-   * can be useful marking a stream as done.\n+   * @return Created KeyValue or throws an exception\n    * @throws IOException\n    * {@link Deprecated} As of 1.2. Use {@link KeyValueUtil#iscreate(InputStream, boolean)} instead.\n    */\n@@ -2412,7 +2412,9 @@ public static KeyValue iscreate(final InputStream in) throws IOException {\n     while (bytesRead < intBytes.length) {\n       int n = in.read(intBytes, bytesRead, intBytes.length - bytesRead);\n       if (n < 0) {\n-        if (bytesRead == 0) return null; // EOF at start is ok\n+        if (bytesRead == 0) {\n+          throw new EOFException();\n+        }\n         throw new IOException(\"Failed read of int, read \" + bytesRead + \" bytes\");\n       }\n       bytesRead += n;",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "sha": "7534e9df6a2f690d57f5dd9e90feee26879e4144",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 5,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import java.io.DataInput;\n import java.io.DataOutput;\n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n@@ -187,7 +188,7 @@ public static void appendToByteBuffer(final ByteBuffer bb, final KeyValue kv,\n    * position to the start of the next KeyValue. Does not allocate a new array or copy data.\n    * @param bb\n    * @param includesMvccVersion\n-   * @param includesTags \n+   * @param includesTags\n    */\n   public static KeyValue nextShallowCopy(final ByteBuffer bb, final boolean includesMvccVersion,\n       boolean includesTags) {\n@@ -231,7 +232,7 @@ public static KeyValue previousKey(final KeyValue in) {\n     return createFirstOnRow(CellUtil.cloneRow(in), CellUtil.cloneFamily(in),\n       CellUtil.cloneQualifier(in), in.getTimestamp() - 1);\n   }\n-  \n+\n \n   /**\n    * Create a KeyValue for the specified row, family and qualifier that would be\n@@ -449,6 +450,7 @@ public static KeyValue ensureKeyValue(final Cell cell) {\n   @Deprecated\n   public static List<KeyValue> ensureKeyValues(List<Cell> cells) {\n     List<KeyValue> lazyList = Lists.transform(cells, new Function<Cell, KeyValue>() {\n+      @Override\n       public KeyValue apply(Cell arg0) {\n         return KeyValueUtil.ensureKeyValue(arg0);\n       }\n@@ -491,8 +493,9 @@ public static KeyValue iscreate(final InputStream in, boolean withTags) throws I\n     while (bytesRead < intBytes.length) {\n       int n = in.read(intBytes, bytesRead, intBytes.length - bytesRead);\n       if (n < 0) {\n-        if (bytesRead == 0)\n-          return null; // EOF at start is ok\n+        if (bytesRead == 0) {\n+          throw new EOFException();\n+        }\n         throw new IOException(\"Failed read of int, read \" + bytesRead + \" bytes\");\n       }\n       bytesRead += n;\n@@ -555,7 +558,7 @@ public static KeyValue create(final DataInput in) throws IOException {\n \n   /**\n    * Create a KeyValue reading <code>length</code> from <code>in</code>\n-   * \n+   *\n    * @param length\n    * @param in\n    * @return Created KeyValue OR if we find a length of zero, we will return",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java",
                "sha": "98e2205c9c75b49d91d7f4e6aa550719c16e63a3",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 9,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java",
                "patch": "@@ -20,6 +20,9 @@\n import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.PushbackInputStream;\n+\n+import javax.annotation.Nonnull;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -32,27 +35,41 @@\n @InterfaceAudience.Private\n public abstract class BaseDecoder implements Codec.Decoder {\n   protected static final Log LOG = LogFactory.getLog(BaseDecoder.class);\n-  protected final InputStream in;\n-  private boolean hasNext = true;\n+\n+  protected final PBIS in;\n   private Cell current = null;\n \n+  protected static class PBIS extends PushbackInputStream {\n+    public PBIS(InputStream in, int size) {\n+      super(in, size);\n+    }\n+\n+    public void resetBuf(int size) {\n+      this.buf = new byte[size];\n+      this.pos = size;\n+    }\n+  }\n+\n   public BaseDecoder(final InputStream in) {\n-    this.in = in;\n+    this.in = new PBIS(in, 1);\n   }\n \n   @Override\n   public boolean advance() throws IOException {\n-    if (!this.hasNext) return this.hasNext;\n-    if (this.in.available() == 0) {\n-      this.hasNext = false;\n-      return this.hasNext;\n+    int firstByte = in.read();\n+    if (firstByte == -1) {\n+      return false;\n+    } else {\n+      in.unread(firstByte);\n     }\n+\n     try {\n       this.current = parseCell();\n     } catch (IOException ioEx) {\n+      in.resetBuf(1); // reset the buffer in case the underlying stream is read from upper layers\n       rethrowEofException(ioEx);\n     }\n-    return this.hasNext;\n+    return true;\n   }\n \n   private void rethrowEofException(IOException ioEx) throws IOException {\n@@ -72,9 +89,12 @@ private void rethrowEofException(IOException ioEx) throws IOException {\n   }\n \n   /**\n-   * @return extract a Cell\n+   * Extract a Cell.\n+   * @return a parsed Cell or throws an Exception. EOFException or a generic IOException maybe\n+   * thrown if EOF is reached prematurely. Does not return null.\n    * @throws IOException\n    */\n+  @Nonnull\n   protected abstract Cell parseCell() throws IOException;\n \n   @Override",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java",
                "sha": "09dc37fd3674d64d89aaaf0470f18876cd0b2e80",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 0,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java",
                "patch": "@@ -79,6 +79,7 @@ public CellDecoder(final InputStream in) {\n       super(in);\n     }\n \n+    @Override\n     protected Cell parseCell() throws IOException {\n       byte [] row = readByteArray(this.in);\n       byte [] family = readByteArray(in);",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java",
                "sha": "666f440eea643ae3cf56f3b10352642a76a270aa",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 7,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java",
                "patch": "@@ -19,7 +19,6 @@\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n-import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n@@ -84,12 +83,8 @@ protected Cell parseCell() throws IOException {\n         return super.parseCell();\n       }\n       int ivLength = 0;\n-      try {\n-        ivLength = StreamUtils.readRawVarint32(in);\n-      } catch (EOFException e) {\n-        // EOF at start is OK\n-        return null;\n-      }\n+\n+      ivLength = StreamUtils.readRawVarint32(in);\n \n       // TODO: An IV length of 0 could signify an unwrapped cell, when the\n       // encoder supports that just read the remainder in directly",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java",
                "sha": "69181e5e94d6d82b8baca91823580f2263c7858e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "patch": "@@ -855,9 +855,10 @@ private int countDistinctRowKeys(WALEdit edit) {\n       int distinctRowKeys = 1;\n       Cell lastCell = cells.get(0);\n       for (int i = 0; i < edit.size(); i++) {\n-        if (!CellUtil.matchingRow(cells.get(i), lastCell)) {\n+        if (!CellUtil.matchingRows(cells.get(i), lastCell)) {\n           distinctRowKeys++;\n         }\n+        lastCell = cells.get(i);\n       }\n       return distinctRowKeys;\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "sha": "3d9952324e5ef8bf560261746d43b8db0a7a814f",
                "status": "modified"
            }
        ],
        "message": "HBASE-14501 NPE in replication with TDE",
        "parent": "https://github.com/apache/hbase/commit/6143b7694cc02e905b931de86462c6125ca8b3b6",
        "repo": "hbase",
        "unit_tests": [
            "TestReplicationSource.java"
        ]
    },
    "hbase_34db492": {
        "bug_id": "hbase_34db492",
        "commit": "https://github.com/apache/hbase/commit/34db492b4627298cde804a565f076ce1bb3fa2c6",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -642,6 +642,7 @@ Release 0.21.0 - Unreleased\n    HBASE-3185  User-triggered compactions are triggering splits!\n    HBASE-1932  Encourage use of 'lzo' compression... add the wiki page to\n                getting started\n+   HBASE-3151  NPE when trying to read regioninfo from .META.\n \n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/CHANGES.txt",
                "sha": "381471c096018a191c3088661917b9c543b6759c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "patch": "@@ -205,7 +205,7 @@ public static void deleteDaughterReferenceInParent(CatalogTracker catalogTracker\n     catalogTracker.waitForMetaServerConnectionDefault().\n       delete(CatalogTracker.META_REGION, delete);\n     LOG.info(\"Deleted daughter \" + daughter.getRegionNameAsString() +\n-      \" \" + Bytes.toString(qualifier) + \" from parent \" +\n+      \" \" + Bytes.toString(qualifier) + \" reference in parent \" +\n       parent.getRegionNameAsString());\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "sha": "53bc9bef78aacb06d814ce126e57652abcf26d41",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "patch": "@@ -136,6 +136,7 @@ private static boolean isMetaRegion(final byte [] regionName) {\n       public boolean visit(Result r) throws IOException {\n         if (r ==  null || r.isEmpty()) return true;\n         Pair<HRegionInfo,HServerAddress> region = metaRowToRegionPair(r);\n+        if (region == null) return true;\n         regions.put(region.getFirst(), region.getSecond());\n         return true;\n       }\n@@ -296,13 +297,16 @@ private static HServerAddress readLocation(HRegionInterface metaServer,\n   /**\n    * @param data A .META. table row.\n    * @return A pair of the regioninfo and the server address from <code>data</code>\n-   * (or null for server address if no address set in .META.).\n+   * or null for server address if no address set in .META. or null for a result\n+   * if no HRegionInfo found.\n    * @throws IOException\n    */\n   public static Pair<HRegionInfo, HServerAddress> metaRowToRegionPair(\n       Result data) throws IOException {\n-    HRegionInfo info = Writables.getHRegionInfo(\n-      data.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER));\n+    byte [] bytes =\n+      data.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);\n+    if (bytes == null) return null;\n+    HRegionInfo info = Writables.getHRegionInfo(bytes);\n     final byte[] value = data.getValue(HConstants.CATALOG_FAMILY,\n       HConstants.SERVER_QUALIFIER);\n     if (value != null && value.length > 0) {\n@@ -463,6 +467,7 @@ public static boolean tableExists(CatalogTracker catalogTracker,\n       while((data = metaServer.next(scannerid)) != null) {\n         if (data != null && data.size() > 0) {\n           Pair<HRegionInfo, HServerAddress> region = metaRowToRegionPair(data);\n+          if (region == null) continue;\n           if (region.getFirst().getTableDesc().getNameAsString().equals(\n               tableName)) {\n             regions.add(region);",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "sha": "8556595745dba52ef4743547fef1c34c318602e3",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 8,
                "filename": "src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "patch": "@@ -24,6 +24,8 @@\n import java.util.ArrayList;\n import java.util.List;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HRegionInfo;\n@@ -40,6 +42,7 @@\n  * minor releases.\n  */\n public class MetaScanner {\n+  private static final Log LOG = LogFactory.getLog(MetaScanner.class);\n   /**\n    * Scans the meta table and calls a visitor on each RowResult and uses a empty\n    * start row value as table name.\n@@ -93,7 +96,8 @@ public static void metaScan(Configuration configuration,\n     // if row is not null, we want to use the startKey of the row's region as\n     // the startRow for the meta scan.\n     byte[] startRow;\n-    if (row != null) {\n+    // row could be non-null but empty -- the HConstants.EMPTY_START_ROW\n+    if (row != null && row.length > 0) {\n       // Scan starting at a particular row in a particular table\n       assert tableName != null;\n       byte[] searchRow =\n@@ -118,7 +122,9 @@ public static void metaScan(Configuration configuration,\n       byte[] rowBefore = regionInfo.getStartKey();\n       startRow = HRegionInfo.createRegionName(tableName, rowBefore,\n           HConstants.ZEROES, false);\n-    } else if (tableName == null || tableName.length == 0) {\n+    } else if (row == null ||\n+        (row != null && Bytes.equals(HConstants.EMPTY_START_ROW, row)) ||\n+        (tableName == null || tableName.length == 0)) {\n       // Full META scan\n       startRow = HConstants.EMPTY_START_ROW;\n     } else {\n@@ -133,8 +139,12 @@ public static void metaScan(Configuration configuration,\n         configuration.getInt(\"hbase.meta.scanner.caching\", 100));\n     do {\n       final Scan scan = new Scan(startRow).addFamily(HConstants.CATALOG_FAMILY);\n-      callable = new ScannerCallable(connection, HConstants.META_TABLE_NAME,\n-          scan);\n+      byte [] metaTableName = Bytes.equals(tableName, HConstants.ROOT_TABLE_NAME)?\n+        HConstants.ROOT_TABLE_NAME: HConstants.META_TABLE_NAME;\n+      LOG.debug(\"Scanning \" + Bytes.toString(metaTableName) +\n+        \" starting at row=\" + Bytes.toString(startRow) + \" for max=\" +\n+        rowUpperLimit + \" rows\");\n+      callable = new ScannerCallable(connection, metaTableName, scan);\n       // Open scanner\n       connection.getRegionServerWithRetries(callable);\n \n@@ -172,10 +182,24 @@ public static void metaScan(Configuration configuration,\n \n   /**\n    * Lists all of the regions currently in META.\n-   * @return\n+   * @param conf\n+   * @return List of all user-space regions.\n    * @throws IOException\n    */\n   public static List<HRegionInfo> listAllRegions(Configuration conf)\n+  throws IOException {\n+    return listAllRegions(conf, true);\n+  }\n+\n+  /**\n+   * Lists all of the regions currently in META.\n+   * @param conf\n+   * @param offlined True if we are to include offlined regions, false and we'll\n+   * leave out offlined regions from returned list.\n+   * @return List of all user-space regions.\n+   * @throws IOException\n+   */\n+  public static List<HRegionInfo> listAllRegions(Configuration conf, final boolean offlined)\n   throws IOException {\n     final List<HRegionInfo> regions = new ArrayList<HRegionInfo>();\n     MetaScannerVisitor visitor =\n@@ -185,9 +209,15 @@ public boolean processRow(Result result) throws IOException {\n           if (result == null || result.isEmpty()) {\n             return true;\n           }\n-          HRegionInfo regionInfo = Writables.getHRegionInfo(\n-              result.getValue(HConstants.CATALOG_FAMILY,\n-                  HConstants.REGIONINFO_QUALIFIER));\n+          byte [] bytes = result.getValue(HConstants.CATALOG_FAMILY,\n+            HConstants.REGIONINFO_QUALIFIER);\n+          if (bytes == null) {\n+            LOG.warn(\"Null REGIONINFO_QUALIFIER: \" + result);\n+            return true;\n+          }\n+          HRegionInfo regionInfo = Writables.getHRegionInfo(bytes);\n+          // If region offline AND we are not to include offlined regions, return.\n+          if (regionInfo.isOffline() && !offlined) return true;\n           regions.add(regionInfo);\n           return true;\n         }",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "sha": "03ac47df1d277faeb43b1e90f243afafe688972a",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 6,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -519,9 +519,6 @@ public void regionOnline(HRegionInfo regionInfo, HServerInfo serverInfo) {\n         this.regionsInTransition.remove(regionInfo.getEncodedName());\n       if (rs != null) {\n         this.regionsInTransition.notifyAll();\n-      } else {\n-        LOG.warn(\"Asked online a region that was not in \" +\n-          \"regionsInTransition: \" + rs);\n       }\n     }\n     synchronized (this.regions) {\n@@ -597,7 +594,10 @@ public void setOffline(HRegionInfo regionInfo) {\n       HServerInfo serverInfo = this.regions.remove(regionInfo);\n       if (serverInfo != null) {\n         List<HRegionInfo> serverRegions = this.servers.get(serverInfo);\n-        serverRegions.remove(regionInfo);\n+        if (!serverRegions.remove(regionInfo)) {\n+          LOG.warn(\"Asked offline a region that was not on expected server: \" +\n+            regionInfo + \", \" + serverInfo.getServerName());\n+        }\n       } else {\n         LOG.warn(\"Asked offline a region that was not online: \" + regionInfo);\n       }\n@@ -1104,9 +1104,9 @@ public void assignAllUserRegions() throws IOException {\n     // First experiment at synchronous assignment\n     // Simpler because just wait for no regions in transition\n \n-    // Scan META for all user regions\n+    // Scan META for all user regions; do not include offlined regions in list.\n     List<HRegionInfo> allRegions =\n-      MetaScanner.listAllRegions(master.getConfiguration());\n+      MetaScanner.listAllRegions(master.getConfiguration(), false);\n     if (allRegions == null || allRegions.isEmpty()) return;\n \n     // Get all available servers",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "d383e2bf28463d866f33b25c044a2cc0c81aaa9d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "patch": "@@ -98,6 +98,7 @@ public boolean visit(Result r) throws IOException {\n         if (r == null || r.isEmpty()) return true;\n         count.incrementAndGet();\n         HRegionInfo info = getHRegionInfo(r);\n+        if (info == null) return true; // Keep scanning\n         if (info.isSplitParent()) splitParents.put(info, r);\n         // Returning true means \"keep scanning\"\n         return true;\n@@ -157,7 +158,7 @@ boolean cleanParent(final HRegionInfo parent,\n     boolean hasReferencesB =\n       checkDaughter(parent, rowContent, HConstants.SPLITB_QUALIFIER);\n     if (!hasReferencesA && !hasReferencesB) {\n-      LOG.info(\"Deleting region \" + parent.getRegionNameAsString() +\n+      LOG.debug(\"Deleting region \" + parent.getRegionNameAsString() +\n         \" because daughter splits no longer hold references\");\n       FileSystem fs = this.services.getMasterFileSystem().getFileSystem();\n       Path rootdir = this.services.getMasterFileSystem().getRootDir();",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "sha": "9fbc376f41dd8b1d72b4c972f27402558491fb05",
                "status": "modified"
            }
        ],
        "message": "HBASE-3151 NPE when trying to read regioninfo from .META.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1030293 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/c767ca49d13d3a296cc4a1fa34e3003db87626bd",
        "repo": "hbase",
        "unit_tests": [
            "TestCatalogJanitor.java"
        ]
    },
    "hbase_3a652ba": {
        "bug_id": "hbase_3a652ba",
        "commit": "https://github.com/apache/hbase/commit/3a652ba41c86aa566ea5377234611888da5a8bd8",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/3a652ba41c86aa566ea5377234611888da5a8bd8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java?ref=3a652ba41c86aa566ea5377234611888da5a8bd8",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java",
                "patch": "@@ -927,7 +927,9 @@ public void stop(String why) {\n       restoreHandler.cancel(why);\n     }\n     try {\n-      coordinator.close();\n+      if (coordinator != null) {\n+        coordinator.close();\n+      }\n     } catch (IOException e) {\n       LOG.error(\"stop ProcedureCoordinator error\", e);\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/3a652ba41c86aa566ea5377234611888da5a8bd8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java",
                "sha": "b7a891d01bab67bb82d92c1e9931b386a5628ff8",
                "status": "modified"
            }
        ],
        "message": "HBASE-12692 NPE from SnapshotManager#stop (Ashish Singhi)",
        "parent": "https://github.com/apache/hbase/commit/afb753ecc3a94a5824a510121aa186948fb317df",
        "repo": "hbase",
        "unit_tests": [
            "TestSnapshotManager.java"
        ]
    },
    "hbase_3bb8daa": {
        "bug_id": "hbase_3bb8daa",
        "commit": "https://github.com/apache/hbase/commit/3bb8daa60565ec2f7955352e52c2f6379176d8c6",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/3bb8daa60565ec2f7955352e52c2f6379176d8c6/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java?ref=3bb8daa60565ec2f7955352e52c2f6379176d8c6",
                "deletions": 4,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java",
                "patch": "@@ -94,7 +94,6 @@\n   private RegionInfo daughter_1_RI;\n   private RegionInfo daughter_2_RI;\n   private byte[] bestSplitRow;\n-  private TableDescriptor htd;\n   private RegionSplitPolicy splitPolicy;\n \n   public SplitTableRegionProcedure() {\n@@ -120,14 +119,14 @@ public SplitTableRegionProcedure(final MasterProcedureEnv env,\n         .setSplit(false)\n         .setRegionId(rid)\n         .build();\n-    this.htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n-    if(this.htd.getRegionSplitPolicyClassName() != null) {\n+    TableDescriptor htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n+    if(htd.getRegionSplitPolicyClassName() != null) {\n       // Since we don't have region reference here, creating the split policy instance without it.\n       // This can be used to invoke methods which don't require Region reference. This instantiation\n       // of a class on Master-side though it only makes sense on the RegionServer-side is\n       // for Phoenix Local Indexing. Refer HBASE-12583 for more information.\n       Class<? extends RegionSplitPolicy> clazz =\n-          RegionSplitPolicy.getSplitPolicyClass(this.htd, env.getMasterConfiguration());\n+          RegionSplitPolicy.getSplitPolicyClass(htd, env.getMasterConfiguration());\n       this.splitPolicy = ReflectionUtils.newInstance(clazz, env.getMasterConfiguration());\n     }\n   }\n@@ -611,6 +610,7 @@ public void createDaughterRegions(final MasterProcedureEnv env) throws IOExcepti\n       maxThreads, Threads.getNamedThreadFactory(\"StoreFileSplitter-%1$d\"));\n     final List<Future<Pair<Path,Path>>> futures = new ArrayList<Future<Pair<Path,Path>>>(nbFiles);\n \n+    TableDescriptor htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n     // Split each store file.\n     for (Map.Entry<String, Collection<StoreFileInfo>>e: files.entrySet()) {\n       byte [] familyName = Bytes.toBytes(e.getKey());",
                "raw_url": "https://github.com/apache/hbase/raw/3bb8daa60565ec2f7955352e52c2f6379176d8c6/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java",
                "sha": "be0741d87305803f6448fd68ecfc8ff786682b83",
                "status": "modified"
            }
        ],
        "message": "HBASE-19939 Fixed NPE in tests TestSplitTableRegionProcedure#testSplitWithoutPONR() and testRecoveryAndDoubleExecution()\n\nValue of 'htd' is null as it is initialized in the constructor but when the object is deserialized its null. Got rid of member variable htd and made it local to method.",
        "parent": "https://github.com/apache/hbase/commit/f5197979aaac7e36b6af36b86ea8dc8d7774fabe",
        "repo": "hbase",
        "unit_tests": [
            "TestSplitTableRegionProcedure.java"
        ]
    },
    "hbase_3cb1168": {
        "bug_id": "hbase_3cb1168",
        "commit": "https://github.com/apache/hbase/commit/3cb1168601bee83e82a21de7c359d70d27c8c767",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/3cb1168601bee83e82a21de7c359d70d27c8c767/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=3cb1168601bee83e82a21de7c359d70d27c8c767",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -279,6 +279,8 @@ Release 0.21.0 - Unreleased\n                is a prefix (Todd Lipcon via Stack)\n    HBASE-2463  Various Bytes.* functions silently ignore invalid arguments\n                (Benoit Sigoure via Stack)\n+   HBASE-2443  IPC client can throw NPE if socket creation fails\n+               (Todd Lipcon via Stack)\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "raw_url": "https://github.com/apache/hbase/raw/3cb1168601bee83e82a21de7c359d70d27c8c767/CHANGES.txt",
                "sha": "fa5c3828acc854ca8071a0a6356abe21368c0c35",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/3cb1168601bee83e82a21de7c359d70d27c8c767/core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=3cb1168601bee83e82a21de7c359d70d27c8c767",
                "deletions": 4,
                "filename": "core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "patch": "@@ -351,10 +351,12 @@ protected synchronized void setupIOstreams() throws IOException {\n     private void handleConnectionFailure(\n         int curRetries, int maxRetries, IOException ioe) throws IOException {\n       // close the current connection\n-      try {\n-        socket.close();\n-      } catch (IOException e) {\n-        LOG.warn(\"Not able to close a socket\", e);\n+      if (socket != null) { // could be null if the socket creation failed\n+        try {\n+          socket.close();\n+        } catch (IOException e) {\n+          LOG.warn(\"Not able to close a socket\", e);\n+        }\n       }\n       // set socket to null so that the next call to setupIOstreams\n       // can start the process of connect all over again.",
                "raw_url": "https://github.com/apache/hbase/raw/3cb1168601bee83e82a21de7c359d70d27c8c767/core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "sha": "359a5b1b87d942fd18915c72729d94702b058217",
                "status": "modified"
            }
        ],
        "message": "HBASE-2443 IPC client can throw NPE if socket creation fails\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@936107 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/2b2f1d1670040f2e0d27921a3f002099ff6ba447",
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseClient.java"
        ]
    },
    "hbase_3ccfd50": {
        "bug_id": "hbase_3ccfd50",
        "commit": "https://github.com/apache/hbase/commit/3ccfd50bd937571aeed3033561b7ca52c967f105",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/3ccfd50bd937571aeed3033561b7ca52c967f105/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java?ref=3ccfd50bd937571aeed3033561b7ca52c967f105",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "patch": "@@ -492,7 +492,7 @@ public Void read() {\n           sink.publishReadTiming(serverName, region, column, stopWatch.getTime());\n         } catch (Exception e) {\n           sink.publishReadFailure(serverName, region, column, e);\n-          sink.updateReadFailures(region == null? \"NULL\": region.getRegionNameAsString(),\n+          sink.updateReadFailures(region.getRegionNameAsString(),\n               serverName == null? \"NULL\": serverName.getHostname());\n         } finally {\n           if (rs != null) {",
                "raw_url": "https://github.com/apache/hbase/raw/3ccfd50bd937571aeed3033561b7ca52c967f105/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "sha": "4f59cf3328463517244f41c3ee0ff38fb3c3a6d7",
                "status": "modified"
            }
        ],
        "message": "HBASE-23244 NPEs running Canary (#784)\n\nAddendum to fix findbugs complaint.",
        "parent": "https://github.com/apache/hbase/commit/c58e80fbe6db6426e652ec363149b92f9e33fbb0",
        "repo": "hbase",
        "unit_tests": [
            "TestCanaryTool.java"
        ]
    },
    "hbase_3d156a5": {
        "bug_id": "hbase_3d156a5",
        "commit": "https://github.com/apache/hbase/commit/3d156a5a24699b3fd3c76863c99aaf79f7c865c2",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=3d156a5a24699b3fd3c76863c99aaf79f7c865c2",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -339,6 +339,7 @@ Release 0.92.0 - Unreleased\n    HBASE-4494  AvroServer:: get fails with NPE on a non-existent row\n                (Kay Kay)\n    HBASE-4481  TestMergeTool failed in 0.92 build 20\n+   HBASE-4386  Fix a potential NPE in TaskMonitor (todd)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "raw_url": "https://github.com/apache/hbase/raw/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/CHANGES.txt",
                "sha": "295513df4de5e8663e2a93162d828c662e387163",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java?ref=3d156a5a24699b3fd3c76863c99aaf79f7c865c2",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java",
                "patch": "@@ -72,7 +72,9 @@ public MonitoredTask createStatus(String description) {\n         new Class<?>[] { MonitoredTask.class },\n         new PassthroughInvocationHandler<MonitoredTask>(stat));\n     TaskAndWeakRefPair pair = new TaskAndWeakRefPair(stat, proxy);\n-    tasks.add(pair);\n+    synchronized (this) {\n+      tasks.add(pair);\n+    }\n     return proxy;\n   }\n \n@@ -84,7 +86,9 @@ public MonitoredRPCHandler createRPCStatus(String description) {\n         new Class<?>[] { MonitoredRPCHandler.class },\n         new PassthroughInvocationHandler<MonitoredRPCHandler>(stat));\n     TaskAndWeakRefPair pair = new TaskAndWeakRefPair(stat, proxy);\n-    tasks.add(pair);\n+    synchronized (this) {\n+      tasks.add(pair);\n+    }\n     return proxy;\n   }\n \n@@ -142,7 +146,7 @@ private boolean canPurge(MonitoredTask stat) {\n   public void dumpAsText(PrintWriter out) {\n     long now = System.currentTimeMillis();\n     \n-    List<MonitoredTask> tasks = TaskMonitor.get().getTasks();\n+    List<MonitoredTask> tasks = getTasks();\n     for (MonitoredTask task : tasks) {\n       out.println(\"Task: \" + task.getDescription());\n       out.println(\"Status: \" + task.getState() + \":\" + task.getStatus());",
                "raw_url": "https://github.com/apache/hbase/raw/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java",
                "sha": "fc9c8301e470eda48495bcb8cd64954c32e3a56c",
                "status": "modified"
            }
        ],
        "message": "HBASE-4386  Fix a potential NPE in TaskMonitor\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1179479 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/ead9159ecd6b82b27dafce0ee3891d69444c1704",
        "repo": "hbase",
        "unit_tests": [
            "TestTaskMonitor.java"
        ]
    },
    "hbase_3e7b90a": {
        "bug_id": "hbase_3e7b90a",
        "commit": "https://github.com/apache/hbase/commit/3e7b90ac6d808c171ea988d8d32ef998146713ac",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/3e7b90ac6d808c171ea988d8d32ef998146713ac/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=3e7b90ac6d808c171ea988d8d32ef998146713ac",
                "deletions": 1,
                "filename": "hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "patch": "@@ -406,7 +406,7 @@ protected static HTableDescriptor getTableDescriptor(TestOptions opts) {\n     if (opts.replicas != DEFAULT_OPTS.replicas) {\n       desc.setRegionReplication(opts.replicas);\n     }\n-    if (opts.splitPolicy != DEFAULT_OPTS.splitPolicy) {\n+    if (opts.splitPolicy != null && !opts.splitPolicy.equals(DEFAULT_OPTS.splitPolicy)) {\n       desc.setRegionSplitPolicyClassName(opts.splitPolicy);\n     }\n     return desc;",
                "raw_url": "https://github.com/apache/hbase/raw/3e7b90ac6d808c171ea988d8d32ef998146713ac/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "sha": "8255573a2bc614f46904c4f3b4acf87abca08b82",
                "status": "modified"
            }
        ],
        "message": "HBASE-19445 PerformanceEvaluation NPE processing split policy option",
        "parent": "https://github.com/apache/hbase/commit/00750fe79acbb6a43daa62b9fcabbd1f5ce4cf6c",
        "repo": "hbase",
        "unit_tests": [
            "TestPerformanceEvaluation.java"
        ]
    },
    "hbase_3f5229c": {
        "bug_id": "hbase_3f5229c",
        "commit": "https://github.com/apache/hbase/commit/3f5229c66f93e6e36c36cd6793a0daefcadb55ca",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=3f5229c66f93e6e36c36cd6793a0daefcadb55ca",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -28,3 +28,4 @@ Trunk (unreleased changes)\n  15. HADOOP-1421 Failover detection, split log files.\n      For the files modified, also clean up javadoc, class, field and method \n      visibility (HADOOP-1466)\n+ 16. HADOOP-1479 Fix NPE in HStore#get if store file only has keys < passed key.",
                "raw_url": "https://github.com/apache/hbase/raw/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/CHANGES.txt",
                "sha": "2c20370f465fbbfe06d2c4b7bfa76d29ea995c2e",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/src/java/org/apache/hadoop/hbase/HStore.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=3f5229c66f93e6e36c36cd6793a0daefcadb55ca",
                "deletions": 6,
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "patch": "@@ -835,7 +835,7 @@ void getFull(HStoreKey key, TreeMap<Text, BytesWritable> results) throws IOExcep\n    * If 'numVersions' is negative, the method returns all available versions.\n    */\n   BytesWritable[] get(HStoreKey key, int numVersions) throws IOException {\n-    if(numVersions <= 0) {\n+    if (numVersions <= 0) {\n       throw new IllegalArgumentException(\"Number of versions must be > 0\");\n     }\n     \n@@ -852,15 +852,19 @@ void getFull(HStoreKey key, TreeMap<Text, BytesWritable> results) throws IOExcep\n           BytesWritable readval = new BytesWritable();\n           map.reset();\n           HStoreKey readkey = (HStoreKey)map.getClosest(key, readval);\n-          \n-          if(readkey.matchesRowCol(key)) {\n+          if (readkey == null) {\n+            // map.getClosest returns null if the passed key is > than the\n+            // last key in the map file.  getClosest is a bit of a misnomer\n+            // since it returns exact match or the next closest key AFTER not\n+            // BEFORE.\n+            continue;\n+          }\n+          if (readkey.matchesRowCol(key)) {\n             results.add(readval);\n             readval = new BytesWritable();\n-\n             while(map.next(readkey, readval) && readkey.matchesRowCol(key)) {\n-              if(numVersions > 0 && (results.size() >= numVersions)) {\n+              if (numVersions > 0 && (results.size() >= numVersions)) {\n                 break;\n-                \n               }\n               results.add(readval);\n               readval = new BytesWritable();",
                "raw_url": "https://github.com/apache/hbase/raw/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/src/java/org/apache/hadoop/hbase/HStore.java",
                "sha": "a627bede8d36a81ec8ef42d941fe41b294a7e783",
                "status": "modified"
            }
        ],
        "message": "HADOOP-1479 Fix NPE in HStore#get if store file only has keys < passed key.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@546275 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/09cf0a100f19517653806511b6b64e02fe5d31f8",
        "repo": "hbase",
        "unit_tests": [
            "TestHStore.java"
        ]
    },
    "hbase_433f7a3": {
        "bug_id": "hbase_433f7a3",
        "commit": "https://github.com/apache/hbase/commit/433f7a3ff6d7e18b44202cc950478cfac49daae0",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/433f7a3ff6d7e18b44202cc950478cfac49daae0/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=433f7a3ff6d7e18b44202cc950478cfac49daae0",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -677,6 +677,7 @@ Release 0.90.0 - Unreleased\n    HBASE-3219  Split parents are reassigned on restart and on disable/enable\n    HBASE-3222  Regionserver region listing in UI is no longer ordered\n    HBASE-3221  Race between splitting and disabling\n+   HBASE-3224  NPE in KeyValue$KVComparator.compare when compacting\n \n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hbase/raw/433f7a3ff6d7e18b44202cc950478cfac49daae0/CHANGES.txt",
                "sha": "b0d353359652589ca164f65a58ab90f0e1320941",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/docbkx/book.xml",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/docbkx/book.xml?ref=433f7a3ff6d7e18b44202cc950478cfac49daae0",
                "deletions": 0,
                "filename": "src/docbkx/book.xml",
                "patch": "@@ -508,6 +508,15 @@ index e70ebc6..96f8c27 100644\n       </section>\n \n       <section xml:id=\"recommended_configurations\"><title>Recommended Configuations</title>\n+      <section xml:id=\"big_memory\">\n+        <title>Configuration for large memory machines</title>\n+        <para>\n+          HBase ships with a reasonable configuration that will work on nearly all\n+          machine types that people might want to test with. If you have larger\n+          machines you might the following configuration options helpful.\n+        </para>\n+\n+      </section>\n       <section xml:id=\"lzo\">\n       <title>LZO compression</title>\n       <para>You should consider enabling LZO compression.  Its",
                "raw_url": "https://github.com/apache/hbase/raw/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/docbkx/book.xml",
                "sha": "056520122e0afbed171032d8f770dd249ccc8949",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java?ref=433f7a3ff6d7e18b44202cc950478cfac49daae0",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "patch": "@@ -229,6 +229,10 @@ public int reseekTo(byte[] key, int offset, int length)\n             return 1;\n           }\n         }\n+        if (atEnd) {\n+          // skip the 'reseek' and just return 1.\n+          return 1;\n+        }\n         return delegate.reseekTo(key, offset, length);\n       }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "sha": "40be64977075fbbe5856a09d6afd68a383876859",
                "status": "modified"
            }
        ],
        "message": "HBASE-3224  NPE in KeyValue$KVComparator.compare when compacting\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1034229 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/b5086781068232ace42d4e9687e2eb351a52cced",
        "repo": "hbase",
        "unit_tests": [
            "TestHalfStoreFileReader.java"
        ]
    },
    "hbase_4350632": {
        "bug_id": "hbase_4350632",
        "commit": "https://github.com/apache/hbase/commit/43506320a1bb6ca2193162edfb5dee21fffc08a9",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java?ref=43506320a1bb6ca2193162edfb5dee21fffc08a9",
                "deletions": 0,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java",
                "patch": "@@ -70,6 +70,7 @@ public void loadFiles(List<StoreFile> storeFiles) {\n \n   @Override\n   public final Collection<StoreFile> getStorefiles() {\n+    // TODO: I can return a null list of StoreFiles? That'll mess up clients. St.Ack 20151111\n     return storefiles;\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java",
                "sha": "9f38b9eb00dc344a71054484d8df76c182975b96",
                "status": "modified"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/hbase/blob/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=43506320a1bb6ca2193162edfb5dee21fffc08a9",
                "deletions": 20,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -960,32 +960,34 @@ private void initializeWarmup(final CancelableProgressable reporter) throws IOEx\n     initializeStores(reporter, status);\n   }\n \n-  private void writeRegionOpenMarker(WAL wal, long openSeqId) throws IOException {\n-    Map<byte[], List<Path>> storeFiles = new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);\n+  /**\n+   * @return Map of StoreFiles by column family\n+   */\n+  private NavigableMap<byte[], List<Path>> getStoreFiles() {\n+    NavigableMap<byte[], List<Path>> allStoreFiles =\n+      new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);\n     for (Store store: getStores()) {\n-      ArrayList<Path> storeFileNames = new ArrayList<Path>();\n-      for (StoreFile storeFile: store.getStorefiles()) {\n+      Collection<StoreFile> storeFiles = store.getStorefiles();\n+      if (storeFiles == null) continue;\n+      List<Path> storeFileNames = new ArrayList<Path>();\n+      for (StoreFile storeFile: storeFiles) {\n         storeFileNames.add(storeFile.getPath());\n       }\n-      storeFiles.put(store.getFamily().getName(), storeFileNames);\n+      allStoreFiles.put(store.getFamily().getName(), storeFileNames);\n     }\n+    return allStoreFiles;\n+  }\n \n+  private void writeRegionOpenMarker(WAL wal, long openSeqId) throws IOException {\n+    Map<byte[], List<Path>> storeFiles = getStoreFiles();\n     RegionEventDescriptor regionOpenDesc = ProtobufUtil.toRegionEventDescriptor(\n       RegionEventDescriptor.EventType.REGION_OPEN, getRegionInfo(), openSeqId,\n       getRegionServerServices().getServerName(), storeFiles);\n     WALUtil.writeRegionEventMarker(wal, getTableDesc(), getRegionInfo(), regionOpenDesc, mvcc);\n   }\n \n   private void writeRegionCloseMarker(WAL wal) throws IOException {\n-    Map<byte[], List<Path>> storeFiles = new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);\n-    for (Store store: getStores()) {\n-      ArrayList<Path> storeFileNames = new ArrayList<Path>();\n-      for (StoreFile storeFile: store.getStorefiles()) {\n-        storeFileNames.add(storeFile.getPath());\n-      }\n-      storeFiles.put(store.getFamily().getName(), storeFileNames);\n-    }\n-\n+    Map<byte[], List<Path>> storeFiles = getStoreFiles();\n     RegionEventDescriptor regionEventDesc = ProtobufUtil.toRegionEventDescriptor(\n       RegionEventDescriptor.EventType.REGION_CLOSE, getRegionInfo(), mvcc.getReadPoint(),\n       getRegionServerServices().getServerName(), storeFiles);\n@@ -1016,7 +1018,9 @@ public HDFSBlocksDistribution getHDFSBlocksDistribution() {\n       new HDFSBlocksDistribution();\n     synchronized (this.stores) {\n       for (Store store : this.stores.values()) {\n-        for (StoreFile sf : store.getStorefiles()) {\n+        Collection<StoreFile> storeFiles = store.getStorefiles();\n+        if (storeFiles == null) continue;\n+        for (StoreFile sf : storeFiles) {\n           HDFSBlocksDistribution storeFileBlocksDistribution =\n             sf.getHDFSBlockDistribution();\n           hdfsBlocksDistribution.add(storeFileBlocksDistribution);\n@@ -1059,7 +1063,6 @@ public static HDFSBlocksDistribution computeHDFSBlocksDistribution(final Configu\n     for (HColumnDescriptor family: tableDescriptor.getFamilies()) {\n       Collection<StoreFileInfo> storeFiles = regionFs.getStoreFiles(family.getNameAsString());\n       if (storeFiles == null) continue;\n-\n       for (StoreFileInfo storeFileInfo : storeFiles) {\n         try {\n           hdfsBlocksDistribution.add(storeFileInfo.computeHDFSBlocksDistribution(fs));\n@@ -1639,10 +1642,16 @@ public long getEarliestFlushTimeForAllStores() {\n   public long getOldestHfileTs(boolean majorCompactioOnly) throws IOException {\n     long result = Long.MAX_VALUE;\n     for (Store store : getStores()) {\n-      for (StoreFile file : store.getStorefiles()) {\n-        HFile.Reader reader = file.getReader().getHFileReader();\n+      Collection<StoreFile> storeFiles = store.getStorefiles();\n+      if (storeFiles == null) continue;\n+      for (StoreFile file : storeFiles) {\n+        StoreFile.Reader sfReader = file.getReader();\n+        if (sfReader == null) continue;\n+        HFile.Reader reader = sfReader.getHFileReader();\n+        if (reader == null) continue;\n         if (majorCompactioOnly) {\n           byte[] val = reader.loadFileInfo().get(StoreFile.MAJOR_COMPACTION_KEY);\n+          if (val == null) continue;\n           if (val == null || !Bytes.toBoolean(val)) {\n             continue;\n           }\n@@ -4898,7 +4907,9 @@ private void logRegionFiles() {\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(getRegionInfo().getEncodedName() + \" : Store files for region: \");\n       for (Store s : stores.values()) {\n-        for (StoreFile sf : s.getStorefiles()) {\n+        Collection<StoreFile> storeFiles = s.getStorefiles();\n+        if (storeFiles == null) continue;\n+        for (StoreFile sf : storeFiles) {\n           LOG.trace(getRegionInfo().getEncodedName() + \" : \" + sf);\n         }\n       }\n@@ -5006,7 +5017,9 @@ private Store getStore(Cell cell) {\n           throw new IllegalArgumentException(\"No column family : \" +\n               new String(column) + \" available\");\n         }\n-        for (StoreFile storeFile: store.getStorefiles()) {\n+        Collection<StoreFile> storeFiles = store.getStorefiles();\n+        if (storeFiles == null) continue;\n+        for (StoreFile storeFile: storeFiles) {\n           storeFileNames.add(storeFile.getPath().toString());\n         }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "994270b447590dd17e4d8e7909a107f08de4512c",
                "status": "modified"
            }
        ],
        "message": "HBASE-14798 NPE reporting server load causes regionserver abort; causes TestAcidGuarantee to fail",
        "parent": "https://github.com/apache/hbase/commit/1fa7b71cf82cc30757ecf5d2a8e0cfba654ed469",
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_4754202": {
        "bug_id": "hbase_4754202",
        "commit": "https://github.com/apache/hbase/commit/475420205c05adf821896d6e4c436a428ee8981a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/475420205c05adf821896d6e4c436a428ee8981a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java?ref=475420205c05adf821896d6e4c436a428ee8981a",
                "deletions": 0,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java",
                "patch": "@@ -183,6 +183,9 @@ public int compareKey(KVComparator comparator, byte[] key, int offset, int lengt\n \n     @Override\n     public void setCurrentBuffer(ByteBuffer buffer) {\n+      if (this.tagCompressionContext != null) {\n+        this.tagCompressionContext.clear();\n+      }\n       currentBuffer = buffer;\n       decodeFirst();\n       previous.invalidate();",
                "raw_url": "https://github.com/apache/hbase/raw/475420205c05adf821896d6e4c436a428ee8981a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java",
                "sha": "513e2e22e9b5615bfe14bfe7d4861dae1b7e70b1",
                "status": "modified"
            }
        ],
        "message": "HBASE-10438 NPE from LRUDictionary when size reaches the max init value (Ramkrishna S. Vasudevan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1562578 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/5ecb63b539bc99e019a317e47c328d2a4d4d9b18",
        "repo": "hbase",
        "unit_tests": [
            "TestBufferedDataBlockEncoder.java"
        ]
    },
    "hbase_5375ff0": {
        "bug_id": "hbase_5375ff0",
        "commit": "https://github.com/apache/hbase/commit/5375ff07bcb6451e45c09f23f010a4d051968896",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=5375ff07bcb6451e45c09f23f010a4d051968896",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -1377,6 +1377,9 @@ public void assign(Map<HRegionInfo, ServerName> regions)\n     // Reuse existing assignment info\n     Map<ServerName, List<HRegionInfo>> bulkPlan =\n       balancer.retainAssignment(regions, servers);\n+    if (bulkPlan == null) {\n+      throw new IOException(\"Unable to determine a plan to assign region(s)\");\n+    }\n \n     assign(regions.size(), servers.size(),\n       \"retainAssignment=true\", bulkPlan);\n@@ -1404,8 +1407,11 @@ public void assign(List<HRegionInfo> regions)\n     // Generate a round-robin bulk assignment plan\n     Map<ServerName, List<HRegionInfo>> bulkPlan\n       = balancer.roundRobinAssignment(regions, servers);\n-    processFavoredNodes(regions);\n+    if (bulkPlan == null) {\n+      throw new IOException(\"Unable to determine a plan to assign region(s)\");\n+    }\n \n+    processFavoredNodes(regions);\n     assign(regions.size(), servers.size(),\n       \"round-robin=true\", bulkPlan);\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "77c32c439177ac460afbda679f7c911edf6fd83d",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hbase/blob/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=5375ff07bcb6451e45c09f23f010a4d051968896",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -1079,15 +1079,30 @@ void move(final byte[] encodedRegionName,\n       final List<ServerName> destServers = this.serverManager.createDestinationServersList(\n         regionState.getServerName());\n       dest = balancer.randomAssignment(hri, destServers);\n+      if (dest == null) {\n+        LOG.debug(\"Unable to determine a plan to assign \" + hri);\n+        return;\n+      }\n     } else {\n       dest = ServerName.valueOf(Bytes.toString(destServerName));\n-      if (dest.equals(regionState.getServerName())) {\n+      if (dest.equals(serverName) && balancer instanceof BaseLoadBalancer\n+          && !((BaseLoadBalancer)balancer).shouldBeOnMaster(hri)) {\n+        // To avoid unnecessary region moving later by balancer. Don't put user\n+        // regions on master. Regions on master could be put on other region\n+        // server intentionally by test however.\n         LOG.debug(\"Skipping move of region \" + hri.getRegionNameAsString()\n-          + \" because region already assigned to the same server \" + dest + \".\");\n+          + \" to avoid unnecessary region moving later by load balancer,\"\n+          + \" because it should not be on master\");\n         return;\n       }\n     }\n \n+    if (dest.equals(regionState.getServerName())) {\n+      LOG.debug(\"Skipping move of region \" + hri.getRegionNameAsString()\n+        + \" because region already assigned to the same server \" + dest + \".\");\n+      return;\n+    }\n+\n     // Now we can do the move\n     RegionPlan rp = new RegionPlan(hri, regionState.getServerName(), dest);\n ",
                "raw_url": "https://github.com/apache/hbase/raw/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "eef0a09f3d0e54b1d239562e1d5ee8c7434f5b72",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java?ref=5375ff07bcb6451e45c09f23f010a4d051968896",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "patch": "@@ -858,7 +858,7 @@ protected void setSlop(Configuration conf) {\n    * Check if a region belongs to some small system table.\n    * If so, it may be expected to be put on the master regionserver.\n    */\n-  protected boolean shouldBeOnMaster(HRegionInfo region) {\n+  public boolean shouldBeOnMaster(HRegionInfo region) {\n     return tablesOnMaster.contains(region.getTable().getNameAsString());\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "sha": "a5e110b6d0018255ba9e82e11e956d9693d14129",
                "status": "modified"
            }
        ],
        "message": "HBASE-12167 NPE in AssignmentManager",
        "parent": "https://github.com/apache/hbase/commit/d8a7b67d798ab5fec399d4a0b97a025d5bff531c",
        "repo": "hbase",
        "unit_tests": [
            "TestBaseLoadBalancer.java"
        ]
    },
    "hbase_56e19a7": {
        "bug_id": "hbase_56e19a7",
        "commit": "https://github.com/apache/hbase/commit/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=56e19a72dcd13edd23b3f9c5a3696acdb5243dc7",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -449,6 +449,7 @@ Release 0.92.0 - Unreleased\n    HBASE-4719  HBase script assumes pre-Hadoop 0.21 layout of jar files\n                (Roman Shposhnik)\n    HBASE-4553  The update of .tableinfo is not atomic; we remove then rename\n+   HBASE-4725  NPE in AM#updateTimers\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "raw_url": "https://github.com/apache/hbase/raw/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/CHANGES.txt",
                "sha": "352113449e879f44dd5781b44af30c9dd2be3e61",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=56e19a72dcd13edd23b3f9c5a3696acdb5243dc7",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -61,7 +61,6 @@\n import org.apache.hadoop.hbase.executor.ExecutorService;\n import org.apache.hadoop.hbase.executor.RegionTransitionData;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n-import org.apache.hadoop.hbase.master.AssignmentManager.RegionState;\n import org.apache.hadoop.hbase.master.handler.ClosedRegionHandler;\n import org.apache.hadoop.hbase.master.handler.DisableTableHandler;\n import org.apache.hadoop.hbase.master.handler.EnableTableHandler;\n@@ -1056,6 +1055,7 @@ private void updateTimers(final ServerName sn) {\n       copy.putAll(this.regionPlans);\n     }\n     for (Map.Entry<String, RegionPlan> e: copy.entrySet()) {\n+      if (e.getValue() == null || e.getValue().getDestination() == null) continue;\n       if (!e.getValue().getDestination().equals(sn)) continue;\n       RegionState rs = null;\n       synchronized (this.regionsInTransition) {",
                "raw_url": "https://github.com/apache/hbase/raw/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "b5a2cc730e60dd8c7eca9b7a93ac9705e11a9f5f",
                "status": "modified"
            }
        ],
        "message": "HBASE-4725 NPE in AM#updateTimers\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1197815 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/bb0c9a11d8843344f287ac5919e4bea3f34fb240",
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_57c8671": {
        "bug_id": "hbase_57c8671",
        "commit": "https://github.com/apache/hbase/commit/57c86717285d74f4604a277ee034878398568d81",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/57c86717285d74f4604a277ee034878398568d81/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java?ref=57c86717285d74f4604a277ee034878398568d81",
                "deletions": 4,
                "filename": "hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
                "patch": "@@ -393,10 +393,10 @@ public long getWALPosition(ServerName serverName, String queueId, String fileNam\n             \" failed when creating the node for \" + destServerName,\n           e);\n     }\n+    String newQueueId = queueId + \"-\" + sourceServerName;\n     try {\n       String oldQueueNode = getQueueNode(sourceServerName, queueId);\n       List<String> wals = ZKUtil.listChildrenNoWatch(zookeeper, oldQueueNode);\n-      String newQueueId = queueId + \"-\" + sourceServerName;\n       if (CollectionUtils.isEmpty(wals)) {\n         ZKUtil.deleteNodeFailSilent(zookeeper, oldQueueNode);\n         LOG.info(\"Removed empty {}/{}\", sourceServerName, queueId);\n@@ -427,11 +427,12 @@ public long getWALPosition(ServerName serverName, String queueId, String fileNam\n       return new Pair<>(newQueueId, logQueue);\n     } catch (NoNodeException | NodeExistsException | NotEmptyException | BadVersionException e) {\n       // Multi call failed; it looks like some other regionserver took away the logs.\n-      // These exceptions mean that zk tells us the request can not be execute so it is safe to just\n-      // return a null. For other types of exception should be thrown out to notify the upper layer.\n+      // These exceptions mean that zk tells us the request can not be execute. So return an empty\n+      // queue to tell the upper layer that claim nothing. For other types of exception should be\n+      // thrown out to notify the upper layer.\n       LOG.info(\"Claim queue queueId={} from {} to {} failed with {}, someone else took the log?\",\n           queueId,sourceServerName, destServerName, e.toString());\n-      return null;\n+      return new Pair<>(newQueueId, Collections.emptySortedSet());\n     } catch (KeeperException | InterruptedException e) {\n       throw new ReplicationException(\"Claim queue queueId=\" + queueId + \" from \" +\n         sourceServerName + \" to \" + destServerName + \" failed\", e);",
                "raw_url": "https://github.com/apache/hbase/raw/57c86717285d74f4604a277ee034878398568d81/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
                "sha": "cca8bfcab3da50d5d8440f2bc5d7ace8e18140a0",
                "status": "modified"
            }
        ],
        "message": "HBASE-20678 NPE in ReplicationSourceManager#NodeFailoverWorker",
        "parent": "https://github.com/apache/hbase/commit/a45763df553edd006c2168df3a64f0f2cdf2366f",
        "repo": "hbase",
        "unit_tests": [
            "TestZKReplicationQueueStorage.java"
        ]
    },
    "hbase_5994c50": {
        "bug_id": "hbase_5994c50",
        "commit": "https://github.com/apache/hbase/commit/5994c5014382bb09d040c0f9e9e0aae13d025e8b",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=5994c5014382bb09d040c0f9e9e0aae13d025e8b",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "patch": "@@ -118,6 +118,8 @@ public boolean visit(Result r) throws IOException {\n         Pair<HRegionInfo, ServerName> region = parseCatalogResult(r);\n         if (region == null) return true;\n         HRegionInfo hri = region.getFirst();\n+        if (hri  == null) return true;\n+        if (hri.getTableNameAsString() == null) return true;\n         if (disabledTables.contains(\n             hri.getTableNameAsString())) return true;\n         // Are we to include split parents in the list?",
                "raw_url": "https://github.com/apache/hbase/raw/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "sha": "77a121b33a40b228fb2040f57e61a71b170b8000",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=5994c5014382bb09d040c0f9e9e0aae13d025e8b",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -2254,6 +2254,7 @@ boolean waitUntilNoRegionsInTransition(final long timeout, Set<HRegionInfo> regi\n       if (region == null) continue;\n       HRegionInfo regionInfo = region.getFirst();\n       ServerName regionLocation = region.getSecond();\n+      if (regionInfo == null) continue;\n       String tableName = regionInfo.getTableNameAsString();\n       if (regionLocation == null) {\n         // regionLocation could be null if createTable didn't finish properly.",
                "raw_url": "https://github.com/apache/hbase/raw/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "6748e5c87260cc06ccf9b8c14e45258555b5f798",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=5994c5014382bb09d040c0f9e9e0aae13d025e8b",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -4123,6 +4123,8 @@ public Result get(final Get get, final Integer lockid) throws IOException {\n        }\n     }\n \n+    Scan scan = new Scan(get);\n+\n     RegionScanner scanner = null;\n     try {\n       scanner = getScanner(scan);",
                "raw_url": "https://github.com/apache/hbase/raw/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "4c5b1dcad734d1117a8736f8e11d8d29291f7960",
                "status": "modified"
            }
        ],
        "message": "HBASE-5279 NPE in Master after upgrading to 0.92.0\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1245767 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/7160ecd133e687887091c5b62e498db960f4b6ac",
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_5c0d3a5": {
        "bug_id": "hbase_5c0d3a5",
        "commit": "https://github.com/apache/hbase/commit/5c0d3a5ec892b27197f098a97dc2177cb0d8135e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=5c0d3a5ec892b27197f098a97dc2177cb0d8135e",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -124,6 +124,7 @@ Release 0.19.0 - Unreleased\n    HBASE-1100  HBASE-1062 broke TestForceSplit\n    HBASE-1191  shell tools -> close_region does not work for regions that did\n                not deploy properly on startup\n+   HBASE-1093  NPE in HStore#compact\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "raw_url": "https://github.com/apache/hbase/raw/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/CHANGES.txt",
                "sha": "44c33ae9a93dc28e5824bb4ea1a1ec2464519470",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HStore.java?ref=5c0d3a5ec892b27197f098a97dc2177cb0d8135e",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "patch": "@@ -866,6 +866,10 @@ StoreSize compact(final boolean majorCompaction) throws IOException {\n       for (int i = 0; i < countOfFiles; i++) {\n         HStoreFile file = filesToCompact.get(i);\n         Path path = file.getMapFilePath();\n+        if (path == null) {\n+          LOG.warn(\"Path is null for \" + file);\n+          return null;\n+        }\n         int len = 0;\n         for (FileStatus fstatus:fs.listStatus(path)) {\n           len += fstatus.getLen();",
                "raw_url": "https://github.com/apache/hbase/raw/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "sha": "c6ad3b76e9e70a981c8190083c277301f4c1122c",
                "status": "modified"
            }
        ],
        "message": "HBASE-1093 NPE in HStore#compact\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@730068 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/9f9198e21eee3bbf73c609cfd2fb9d8978d46bc3",
        "repo": "hbase",
        "unit_tests": [
            "TestHStore.java"
        ]
    },
    "hbase_60d7a81": {
        "bug_id": "hbase_60d7a81",
        "commit": "https://github.com/apache/hbase/commit/60d7a81833369262e9b21af31d88ed9c81398f42",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/60d7a81833369262e9b21af31d88ed9c81398f42/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java?ref=60d7a81833369262e9b21af31d88ed9c81398f42",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java",
                "patch": "@@ -356,8 +356,12 @@ public static ScannerModel fromScan(Scan scan) throws Exception {\n     Map<byte [], NavigableSet<byte []>> families = scan.getFamilyMap();\n     if (families != null) {\n       for (Map.Entry<byte [], NavigableSet<byte []>> entry : families.entrySet()) {\n-        for (byte[] qualifier : entry.getValue()) {\n-          model.addColumn(Bytes.add(entry.getKey(), COLUMN_DIVIDER, qualifier));\n+        if (entry.getValue() != null) {\n+          for (byte[] qualifier: entry.getValue()) {\n+            model.addColumn(Bytes.add(entry.getKey(), COLUMN_DIVIDER, qualifier));\n+          }\n+        } else {\n+          model.addColumn(entry.getKey());\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hbase/raw/60d7a81833369262e9b21af31d88ed9c81398f42/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java",
                "sha": "7768612dceb9633478b047eec78c784e2a384092",
                "status": "modified"
            }
        ],
        "message": "HBASE-3912 [Stargate] Columns not handle by Scan; fix NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1126556 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/981f414cd3b6ef7c1b846e139977c06ff781802f",
        "repo": "hbase",
        "unit_tests": [
            "TestScannerModel.java"
        ]
    },
    "hbase_621dc88": {
        "bug_id": "hbase_621dc88",
        "commit": "https://github.com/apache/hbase/commit/621dc88c7940ac8ab3719872e4cc1643dcf87da8",
        "file": [
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hbase/blob/621dc88c7940ac8ab3719872e4cc1643dcf87da8/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java?ref=621dc88c7940ac8ab3719872e4cc1643dcf87da8",
                "deletions": 21,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "patch": "@@ -32,12 +32,15 @@\n import java.util.concurrent.atomic.AtomicReference;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.CellUtil;\n+import org.apache.hadoop.hbase.HBaseIOException;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.RegionLocations;\n import org.apache.hadoop.hbase.TableDescriptors;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotFoundException;\n import org.apache.hadoop.hbase.client.AsyncClusterConnection;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.RegionReplicaUtil;\n import org.apache.hadoop.hbase.client.TableDescriptor;\n import org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint;\n import org.apache.hadoop.hbase.replication.WALEntryFilter;\n@@ -162,9 +165,9 @@ private void getRegionLocations(CompletableFuture<RegionLocations> future,\n           return;\n         }\n         // check if the number of region replicas is correct, and also the primary region name\n-        // matches, and also there is no null elements in the returned RegionLocations\n+        // matches.\n         if (locs.size() == tableDesc.getRegionReplication() &&\n-          locs.size() == locs.numNonNullElements() &&\n+          locs.getDefaultRegionLocation() != null &&\n           Bytes.equals(locs.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(),\n             encodedRegionName)) {\n           future.complete(locs);\n@@ -182,16 +185,16 @@ private void replicate(CompletableFuture<Long> future, RegionLocations locs,\n       future.complete(Long.valueOf(entries.size()));\n       return;\n     }\n-    if (!Bytes.equals(locs.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(),\n-      encodedRegionName)) {\n+    RegionInfo defaultReplica = locs.getDefaultRegionLocation().getRegion();\n+    if (!Bytes.equals(defaultReplica.getEncodedNameAsBytes(), encodedRegionName)) {\n       // the region name is not equal, this usually means the region has been split or merged, so\n       // give up replicating as the new region(s) should already have all the data of the parent\n       // region(s).\n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\n           \"Skipping {} entries in table {} because located region {} is different than\" +\n             \" the original region {} from WALEdit\",\n-          tableDesc.getTableName(), locs.getDefaultRegionLocation().getRegion().getEncodedName(),\n+          tableDesc.getTableName(), defaultReplica.getEncodedName(),\n           Bytes.toStringBinary(encodedRegionName));\n       }\n       future.complete(Long.valueOf(entries.size()));\n@@ -202,24 +205,26 @@ private void replicate(CompletableFuture<Long> future, RegionLocations locs,\n     AtomicLong skippedEdits = new AtomicLong(0);\n \n     for (int i = 1, n = locs.size(); i < n; i++) {\n-      final int replicaId = i;\n-      FutureUtils.addListener(connection.replay(tableDesc.getTableName(),\n-        locs.getRegionLocation(replicaId).getRegion().getEncodedNameAsBytes(), row, entries,\n-        replicaId, numRetries, operationTimeoutNs), (r, e) -> {\n-          if (e != null) {\n-            LOG.warn(\"Failed to replicate to {}\", locs.getRegionLocation(replicaId), e);\n-            error.compareAndSet(null, e);\n-          } else {\n-            AtomicUtils.updateMax(skippedEdits, r.longValue());\n-          }\n-          if (remainingTasks.decrementAndGet() == 0) {\n-            if (error.get() != null) {\n-              future.completeExceptionally(error.get());\n+      // Do not use the elements other than the default replica as they may be null. We will fail\n+      // earlier if the location for default replica is null.\n+      final RegionInfo replica = RegionReplicaUtil.getRegionInfoForReplica(defaultReplica, i);\n+      FutureUtils\n+        .addListener(connection.replay(tableDesc.getTableName(), replica.getEncodedNameAsBytes(),\n+          row, entries, replica.getReplicaId(), numRetries, operationTimeoutNs), (r, e) -> {\n+            if (e != null) {\n+              LOG.warn(\"Failed to replicate to {}\", replica, e);\n+              error.compareAndSet(null, e);\n             } else {\n-              future.complete(skippedEdits.get());\n+              AtomicUtils.updateMax(skippedEdits, r.longValue());\n             }\n-          }\n-        });\n+            if (remainingTasks.decrementAndGet() == 0) {\n+              if (error.get() != null) {\n+                future.completeExceptionally(error.get());\n+              } else {\n+                future.complete(skippedEdits.get());\n+              }\n+            }\n+          });\n     }\n   }\n \n@@ -245,6 +250,10 @@ private void logSkipped(TableName tableName, List<Entry> entries, String reason)\n     FutureUtils.addListener(locateFuture, (locs, error) -> {\n       if (error != null) {\n         future.completeExceptionally(error);\n+      } else if (locs.getDefaultRegionLocation() == null) {\n+        future.completeExceptionally(\n+          new HBaseIOException(\"No location found for default replica of table=\" +\n+            tableDesc.getTableName() + \" row='\" + Bytes.toStringBinary(row) + \"'\"));\n       } else {\n         replicate(future, locs, tableDesc, encodedRegionName, row, entries);\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/621dc88c7940ac8ab3719872e4cc1643dcf87da8/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "sha": "2c3b19b6a45c3aa8820b49bc600b04efd7a37ef3",
                "status": "modified"
            }
        ],
        "message": "HBASE-22553 NPE in RegionReplicaReplicationEndpoint",
        "parent": "https://github.com/apache/hbase/commit/6278c98f5d0b19ff830368c47204232760e2b4ea",
        "repo": "hbase",
        "unit_tests": [
            "TestRegionReplicaReplicationEndpoint.java"
        ]
    },
    "hbase_69431c7": {
        "bug_id": "hbase_69431c7",
        "commit": "https://github.com/apache/hbase/commit/69431c75c16d8d863932815f0460322153a25dbb",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/69431c75c16d8d863932815f0460322153a25dbb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java?ref=69431c75c16d8d863932815f0460322153a25dbb",
                "deletions": 2,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.hadoop.hbase.master.MasterServices;\n import org.apache.hadoop.hbase.master.RackManager;\n import org.apache.hadoop.hbase.master.RegionPlan;\n+import org.apache.hadoop.hbase.master.assignment.RegionStates;\n import org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.Cluster.Action.Type;\n import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;\n import org.apache.hbase.thirdparty.com.google.common.base.Joiner;\n@@ -1457,8 +1458,9 @@ public ServerName randomAssignment(RegionInfo regionInfo, List<ServerName> serve\n       // In the current set of regions even if one has region replica let us go with\n       // getting the entire snapshot\n       if (this.services != null && this.services.getAssignmentManager() != null) { // for tests\n-        if (!hasRegionReplica && this.services.getAssignmentManager().getRegionStates()\n-            .isReplicaAvailableForRegion(region)) {\n+        RegionStates states = this.services.getAssignmentManager().getRegionStates();\n+        if (!hasRegionReplica && states != null &&\n+            states.isReplicaAvailableForRegion(region)) {\n           hasRegionReplica = true;\n         }\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/69431c75c16d8d863932815f0460322153a25dbb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "sha": "6cca59fa7837a0c8b2677604ef5f5e30243d224b",
                "status": "modified"
            }
        ],
        "message": "HBASE-21102 ServerCrashProcedure should select target server where no other replicas exist for the current region - addendum fixes NPE",
        "parent": "https://github.com/apache/hbase/commit/cebb725a9f4ab8dd9d3f306d21d53d6f56161c51",
        "repo": "hbase",
        "unit_tests": [
            "TestBaseLoadBalancer.java"
        ]
    },
    "hbase_70ecf18": {
        "bug_id": "hbase_70ecf18",
        "commit": "https://github.com/apache/hbase/commit/70ecf18817ef219389a9e024ff21ffb99b6615d9",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/70ecf18817ef219389a9e024ff21ffb99b6615d9/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java?ref=70ecf18817ef219389a9e024ff21ffb99b6615d9",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java",
                "patch": "@@ -149,8 +149,8 @@ public SplitLogManager(Server server, Configuration conf, Stoppable stopper,\n       Set<String> failedDeletions = Collections.synchronizedSet(new HashSet<String>());\n       SplitLogManagerDetails details =\n           new SplitLogManagerDetails(tasks, master, failedDeletions, serverName);\n-      coordination.init();\n       coordination.setDetails(details);\n+      coordination.init();\n       // Determine recovery mode\n     }\n     this.unassignedTimeout =",
                "raw_url": "https://github.com/apache/hbase/raw/70ecf18817ef219389a9e024ff21ffb99b6615d9/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java",
                "sha": "c93ecf6b639ffc45bf4c9ca015667e948df3bde3",
                "status": "modified"
            }
        ],
        "message": "HBASE-NPE when running TestSplitLogManager (Andrey Stepachev and Zhangduo)",
        "parent": "https://github.com/apache/hbase/commit/dad2474f08d201d09989e36f5cf1c25d3fa4acee",
        "repo": "hbase",
        "unit_tests": [
            "TestSplitLogManager.java"
        ]
    },
    "hbase_748aaef": {
        "bug_id": "hbase_748aaef",
        "commit": "https://github.com/apache/hbase/commit/748aaefb12dee675adc89a7fbf08790f311525fb",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/748aaefb12dee675adc89a7fbf08790f311525fb/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java?ref=748aaefb12dee675adc89a7fbf08790f311525fb",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "patch": "@@ -53,7 +53,7 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n       tmpl.setFormat(req.getParameter(\"format\"));\n     if (req.getParameter(\"filter\") != null)\n       tmpl.setFilter(req.getParameter(\"filter\"));\n-    tmpl.render(resp.getWriter(), hrs);\n+    if (hrs != null) tmpl.render(resp.getWriter(), hrs);\n   }\n \n }",
                "raw_url": "https://github.com/apache/hbase/raw/748aaefb12dee675adc89a7fbf08790f311525fb/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "sha": "f94f53f412bae5bcba8709191fb6d0b61cca3c24",
                "status": "modified"
            }
        ],
        "message": "HBASE-9294 NPE in /rs-status during RS shutdown\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1576953 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/d8ce8d050636dd7432dab8698098bb7ec3c10215",
        "repo": "hbase",
        "unit_tests": [
            "TestRSStatusServlet.java"
        ]
    },
    "hbase_81e2aba": {
        "bug_id": "hbase_81e2aba",
        "commit": "https://github.com/apache/hbase/commit/81e2aba1f9bbe592d9d9284e701af5bedad3cd56",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/81e2aba1f9bbe592d9d9284e701af5bedad3cd56/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java?ref=81e2aba1f9bbe592d9d9284e701af5bedad3cd56",
                "deletions": 9,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java",
                "patch": "@@ -210,15 +210,16 @@ public void write(ImmutableBytesWritable row, V cell)\n           if (conf.getBoolean(LOCALITY_SENSITIVE_CONF_KEY, DEFAULT_LOCALITY_SENSITIVE)) {\n             HRegionLocation loc = null;\n             String tableName = conf.get(OUTPUT_TABLE_NAME_CONF_KEY);\n-\n-            try (Connection connection = ConnectionFactory.createConnection(conf);\n-                   RegionLocator locator =\n-                     connection.getRegionLocator(TableName.valueOf(tableName))) {\n-              loc = locator.getRegionLocation(rowKey);\n-            } catch (Throwable e) {\n-              LOG.warn(\"there's something wrong when locating rowkey: \" +\n-                Bytes.toString(rowKey), e);\n-              loc = null;\n+            if (tableName != null) {\n+              try (Connection connection = ConnectionFactory.createConnection(conf);\n+                     RegionLocator locator =\n+                       connection.getRegionLocator(TableName.valueOf(tableName))) {\n+                loc = locator.getRegionLocation(rowKey);\n+              } catch (Throwable e) {\n+                LOG.warn(\"there's something wrong when locating rowkey: \" +\n+                  Bytes.toString(rowKey), e);\n+                loc = null;\n+              }\n             }\n \n             if (null == loc) {",
                "raw_url": "https://github.com/apache/hbase/raw/81e2aba1f9bbe592d9d9284e701af5bedad3cd56/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java",
                "sha": "a18377305f6d9feab9fcf2c30c1c86eb904284ea",
                "status": "modified"
            }
        ],
        "message": "HBASE-14662 Fix NPE in HFileOutputFormat2 (Heng Chen)",
        "parent": "https://github.com/apache/hbase/commit/5363f371bb7cd83902e8027b221fe7bf690f9eec",
        "repo": "hbase",
        "unit_tests": [
            "TestHFileOutputFormat2.java"
        ]
    },
    "hbase_8525e1e": {
        "bug_id": "hbase_8525e1e",
        "commit": "https://github.com/apache/hbase/commit/8525e1ee23a269ba64821d26060a3ef724e307eb",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/8525e1ee23a269ba64821d26060a3ef724e307eb/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=8525e1ee23a269ba64821d26060a3ef724e307eb",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -44,6 +44,7 @@ Release 0.21.0 - Unreleased\n                split (Cosmin Lehane via Stack)\n    HBASE-1809  NPE thrown in BoundedRangeFileInputStream\n    HBASE-1859  Misc shell fixes patch (Kyle Oba via Stack)\n+   HBASE-1865  0.20.0 TableInputFormatBase NPE\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "raw_url": "https://github.com/apache/hbase/raw/8525e1ee23a269ba64821d26060a3ef724e307eb/CHANGES.txt",
                "sha": "3df60cc6deb9fe206c30fa04129779713aca22ea",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/8525e1ee23a269ba64821d26060a3ef724e307eb/src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java?ref=8525e1ee23a269ba64821d26060a3ef724e307eb",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "patch": "@@ -269,13 +269,13 @@ public float getProgress() {\n    */\n   @Override\n   public List<InputSplit> getSplits(JobContext context) throws IOException {\n+    if (table == null) {\n+      throw new IOException(\"No table was provided.\");\n+    }\n     byte [][] startKeys = table.getStartKeys();\n     if (startKeys == null || startKeys.length == 0) {\n       throw new IOException(\"Expecting at least one region.\");\n     }\n-    if (table == null) {\n-      throw new IOException(\"No table was provided.\");\n-    }\n     int realNumSplits = startKeys.length;\n     InputSplit[] splits = new InputSplit[realNumSplits];\n     int middle = startKeys.length / realNumSplits;",
                "raw_url": "https://github.com/apache/hbase/raw/8525e1ee23a269ba64821d26060a3ef724e307eb/src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "sha": "660b27171d7f900bd3fafe31fce4300b5c4d3c4b",
                "status": "modified"
            }
        ],
        "message": "HBASE-1865 0.20.0 TableInputFormatBase NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@818651 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/dadeb0bdde6461f9fc061c1a316d85045038eca8",
        "repo": "hbase",
        "unit_tests": [
            "TestTableInputFormatBase.java"
        ]
    },
    "hbase_8b693bf": {
        "bug_id": "hbase_8b693bf",
        "commit": "https://github.com/apache/hbase/commit/8b693bf547bc055a83de6aa0f239f481961173a1",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/8b693bf547bc055a83de6aa0f239f481961173a1/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=8b693bf547bc055a83de6aa0f239f481961173a1",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -9,6 +9,8 @@ Release 0.91.0 - Unreleased\n    HBASE-3280  YouAreDeadException being swallowed in HRS getMaster\n    HBASE-3282  Need to retain DeadServers to ensure we don't allow\n                previously expired RS instances to rejoin cluster\n+   HBASE-3283  NPE in AssignmentManager if processing shutdown of RS who\n+               doesn't have any regions assigned to it\n \n   IMPROVEMENTS\n    HBASE-2001  Coprocessors: Colocate user code with regions (Mingjie Lai via",
                "raw_url": "https://github.com/apache/hbase/raw/8b693bf547bc055a83de6aa0f239f481961173a1/CHANGES.txt",
                "sha": "eae17ae9baa7ad371f81f204b9d1b038eba6b9ae",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/8b693bf547bc055a83de6aa0f239f481961173a1/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=8b693bf547bc055a83de6aa0f239f481961173a1",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -1597,16 +1597,21 @@ protected void chore() {\n     // Remove this server from map of servers to regions, and remove all regions\n     // of this server from online map of regions.\n     Set<HRegionInfo> deadRegions = null;\n+    List<HRegionInfo> rits = new ArrayList<HRegionInfo>();\n     synchronized (this.regions) {\n-      deadRegions = new TreeSet<HRegionInfo>(this.servers.remove(hsi));\n+      List<HRegionInfo> assignedRegions = this.servers.remove(hsi);\n+      if (assignedRegions == null || assignedRegions.isEmpty()) {\n+        // No regions on this server, we are done, return empty list of RITs\n+        return rits;\n+      }\n+      deadRegions = new TreeSet<HRegionInfo>(assignedRegions);\n       for (HRegionInfo region : deadRegions) {\n         this.regions.remove(region);\n       }\n     }\n     // See if any of the regions that were online on this server were in RIT\n     // If they are, normal timeouts will deal with them appropriately so\n     // let's skip a manual re-assignment.\n-    List<HRegionInfo> rits = new ArrayList<HRegionInfo>();\n     synchronized (regionsInTransition) {\n       for (RegionState region : this.regionsInTransition.values()) {\n         if (deadRegions.remove(region.getRegion())) {",
                "raw_url": "https://github.com/apache/hbase/raw/8b693bf547bc055a83de6aa0f239f481961173a1/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "6811cdc6438af677ce02985f27d830f49b032905",
                "status": "modified"
            }
        ],
        "message": "HBASE-3283 NPE in AssignmentManager if processing shutdown of RS who doesn't have any regions assigned to it\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1040302 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/2778fced7580043d58116f83badc8629b290505f",
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_8fd9db6": {
        "bug_id": "hbase_8fd9db6",
        "commit": "https://github.com/apache/hbase/commit/8fd9db6d6c7a3b702e752b7c4aa18f48215c0852",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/8fd9db6d6c7a3b702e752b7c4aa18f48215c0852/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=8fd9db6d6c7a3b702e752b7c4aa18f48215c0852",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -2298,7 +2298,7 @@ public void unassign(HRegionInfo region, boolean force, ServerName dest) {\n         // Region is not in transition.\n         // We can unassign it only if it's not SPLIT/MERGED.\n         state = regionStates.getRegionState(encodedName);\n-        if (state.isMerged() || state.isSplit()) {\n+        if (state != null && (state.isMerged() || state.isSplit())) {\n           LOG.info(\"Attempting to unassign \" + state + \", ignored\");\n           return;\n         }",
                "raw_url": "https://github.com/apache/hbase/raw/8fd9db6d6c7a3b702e752b7c4aa18f48215c0852/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "2b72fd3cbff33e1672e025db20e43bb781a29c91",
                "status": "modified"
            }
        ],
        "message": "HBASE-9554 TestOfflineMetaRebuildOverlap#testMetaRebuildOverlapFail fails due to NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1523870 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/80ab9fe15c9751ddb54039e97c96d84c4e1c841c",
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_8fe805c": {
        "bug_id": "hbase_8fe805c",
        "commit": "https://github.com/apache/hbase/commit/8fe805ce290bfd9ceffc900509b36af4928bdf34",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/8fe805ce290bfd9ceffc900509b36af4928bdf34/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=8fe805ce290bfd9ceffc900509b36af4928bdf34",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -450,6 +450,7 @@ Release 0.92.0 - Unreleased\n    HBASE-4877  TestHCM failing sporadically on jenkins and always for me on an\n                ubuntu machine\n    HBASE-4878  Master crash when splitting hlog may cause data loss (Chunhui Shen)\n+   HBASE-4945  NPE in HRegion.bulkLoadHFiles (Andrew P and Lars H)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "raw_url": "https://github.com/apache/hbase/raw/8fe805ce290bfd9ceffc900509b36af4928bdf34/CHANGES.txt",
                "sha": "4ff8cbecae4d2615ad948e26f0c60013a7029f66",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/8fe805ce290bfd9ceffc900509b36af4928bdf34/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=8fe805ce290bfd9ceffc900509b36af4928bdf34",
                "deletions": 11,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -2977,20 +2977,19 @@ public boolean bulkLoadHFiles(List<Pair<byte[], String>> familyPaths)\n               \"No such column family \" + Bytes.toStringBinary(familyName));\n           ioes.add(ioe);\n           failures.add(p);\n-        }\n-\n-        try {\n-          store.assertBulkLoadHFileOk(new Path(path));\n-        } catch (WrongRegionException wre) {\n-          // recoverable (file doesn't fit in region)\n-          failures.add(p);\n-        } catch (IOException ioe) {\n-          // unrecoverable (hdfs problem)\n-          ioes.add(ioe);\n+        } else {\n+          try {\n+            store.assertBulkLoadHFileOk(new Path(path));\n+          } catch (WrongRegionException wre) {\n+            // recoverable (file doesn't fit in region)\n+            failures.add(p);\n+          } catch (IOException ioe) {\n+            // unrecoverable (hdfs problem)\n+            ioes.add(ioe);\n+          }\n         }\n       }\n \n-\n       // validation failed, bail out before doing anything permanent.\n       if (failures.size() != 0) {\n         StringBuilder list = new StringBuilder();",
                "raw_url": "https://github.com/apache/hbase/raw/8fe805ce290bfd9ceffc900509b36af4928bdf34/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "e26e213a628728a20bd018ef98f2e9c9b1e5c21b",
                "status": "modified"
            }
        ],
        "message": "HBASE-4945  NPE in HRegion.bulkLoadHFiles (Andrew P and Lars H)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1210212 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/f547de07c84137d865ac45518d6824f8577f0cab",
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_9297767": {
        "bug_id": "hbase_9297767",
        "commit": "https://github.com/apache/hbase/commit/929776752e52e97d397930807b38f95c9bfc3c1b",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hbase/blob/929776752e52e97d397930807b38f95c9bfc3c1b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=929776752e52e97d397930807b38f95c9bfc3c1b",
                "deletions": 12,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "patch": "@@ -392,7 +392,7 @@ protected Connection createConnection(ConnectionId remoteId, final Codec codec,\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Use \" + authMethod + \" authentication for protocol \"\n-            + protocol.getSimpleName());\n+            + (protocol == null ? \"null\" : protocol.getSimpleName()));\n       }\n       reloginMaxBackoff = conf.getInt(\"hbase.security.relogin.maxbackoff\", 5000);\n       this.remoteId = remoteId;\n@@ -811,7 +811,7 @@ protected synchronized void setupIOstreams()\n                 ticket = ticket.getRealUser();\n               }\n             }\n-            boolean continueSasl = false;\n+            boolean continueSasl;\n             try {\n               if (ticket == null) {\n                 throw new NullPointerException(\"ticket is null\");\n@@ -855,7 +855,7 @@ public Boolean run() throws IOException {\n         }\n       } catch (Throwable t) {\n         failedServers.addToFailedServers(remoteId.address);\n-        IOException e = null;\n+        IOException e;\n         if (t instanceof IOException) {\n           e = (IOException)t;\n           markClosed(e);\n@@ -1007,14 +1007,16 @@ protected void readResponse() {\n             if (call != null) call.setException(re);\n           }\n         } else {\n-          Message rpcResponseType;\n-          try {\n-            // TODO: Why pb engine pollution in here in this class?  FIX.\n-            rpcResponseType =\n-              ProtobufRpcClientEngine.Invoker.getReturnProtoType(\n-                reflectionCache.getMethod(remoteId.getProtocol(), call.method.getName()));\n-          } catch (Exception e) {\n-            throw new RuntimeException(e); //local exception\n+          Message rpcResponseType = null;\n+          if (call != null){\n+            try {\n+              // TODO: Why pb engine pollution in here in this class?  FIX.\n+              rpcResponseType =\n+                ProtobufRpcClientEngine.Invoker.getReturnProtoType(\n+                  reflectionCache.getMethod(remoteId.getProtocol(), call.method.getName()));\n+            } catch (Exception e) {\n+              throw new RuntimeException(e); //local exception\n+            }\n           }\n           Message value = null;\n           if (rpcResponseType != null) {\n@@ -1474,4 +1476,4 @@ public int hashCode() {\n       return hashcode;\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/929776752e52e97d397930807b38f95c9bfc3c1b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "sha": "be60417f26db45f6e3daae1d0c3bf21389062e95",
                "status": "modified"
            }
        ],
        "message": "HBASE-8380 NPE in HBaseClient.readResponse\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1471271 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/374052f071f314b22eb37394c24407377beb5de7",
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseClient.java"
        ]
    },
    "hbase_92e5efd": {
        "bug_id": "hbase_92e5efd",
        "commit": "https://github.com/apache/hbase/commit/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=92e5efdcb6746c5a06d67efd5e5eaccc2efc242e",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -109,6 +109,7 @@ Release 0.20.0 - Unreleased\n    HBASE-1368  HBASE-1279 broke the build\n    HBASE-1264  Wrong return values of comparators for ColumnValueFilter\n                (Thomas Schneider via Andrew Purtell)\n+   HBASE-1374  NPE out of ZooKeeperWrapper.loadZooKeeperConfig\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "raw_url": "https://github.com/apache/hbase/raw/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/CHANGES.txt",
                "sha": "7be5e34ccd72f18b79c5b63074ed051843fba487",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java?ref=92e5efdcb6746c5a06d67efd5e5eaccc2efc242e",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java",
                "patch": "@@ -76,6 +76,9 @@ public static void main(String[] args) {\n   public static Properties parseZooKeeperConfig() throws IOException {\n     ClassLoader cl = HQuorumPeer.class.getClassLoader();\n     InputStream inputStream = cl.getResourceAsStream(ZOOKEEPER_CONFIG_NAME);\n+    if (inputStream == null) {\n+      throw new IOException(ZOOKEEPER_CONFIG_NAME + \" not found\");\n+    }\n     return parseConfig(inputStream);\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java",
                "sha": "9d17b36f0b15e5bdb5a8ae49d115c6a14188a62c",
                "status": "modified"
            }
        ],
        "message": "HBASE-1374 NPE out of ZooKeeperWrapper.loadZooKeeperConfig\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@771996 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/1a2a1764adad96d79b678f047c5960b8b72f8810",
        "repo": "hbase",
        "unit_tests": [
            "TestHQuorumPeer.java"
        ]
    },
    "hbase_96c6b98": {
        "bug_id": "hbase_96c6b98",
        "commit": "https://github.com/apache/hbase/commit/96c6b9815ddbc9f2589655df4ad2381af04ac9f8",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/96c6b9815ddbc9f2589655df4ad2381af04ac9f8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java?ref=96c6b9815ddbc9f2589655df4ad2381af04ac9f8",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "patch": "@@ -601,7 +601,7 @@ Path splitStoreFile(final HRegionInfo hri, final String familyName, final StoreF\n       }\n     }\n \n-    f.getReader().close(true);\n+    f.closeReader(true);\n \n     Path splitDir = new Path(getSplitsDir(hri), familyName);\n     // A reference to the bottom half of the hsf store file.",
                "raw_url": "https://github.com/apache/hbase/raw/96c6b9815ddbc9f2589655df4ad2381af04ac9f8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "sha": "0751634263ab838e9df321af31e671060abdaa7d",
                "status": "modified"
            }
        ],
        "message": "HBASE-12696 Possible NPE in HRegionFileSystem#splitStoreFile when skipStoreFileRangeCheck in splitPolicy return true(Rajeshbabu)",
        "parent": "https://github.com/apache/hbase/commit/110c5f593057366509b8480e396cc750d5fd782b",
        "repo": "hbase",
        "unit_tests": [
            "TestHRegionFileSystem.java"
        ]
    },
    "hbase_998cd16": {
        "bug_id": "hbase_998cd16",
        "commit": "https://github.com/apache/hbase/commit/998cd1642f4ae3819f18061468755bf992257a52",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/998cd1642f4ae3819f18061468755bf992257a52/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java?ref=998cd1642f4ae3819f18061468755bf992257a52",
                "deletions": 0,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java",
                "patch": "@@ -329,6 +329,9 @@ public void clearCache(final HRegionLocation location) {\n     TableName tableName = location.getRegionInfo().getTable();\n     ConcurrentMap<byte[], RegionLocations> tableLocations = getTableLocations(tableName);\n     RegionLocations rll = tableLocations.get(location.getRegionInfo().getStartKey());\n+    if (rll == null) {\n+      return;\n+    }\n     RegionLocations updatedLocations = rll.remove(location);\n     if (updatedLocations.isEmpty()) {\n       tableLocations.remove(location.getRegionInfo().getStartKey(), rll);",
                "raw_url": "https://github.com/apache/hbase/raw/998cd1642f4ae3819f18061468755bf992257a52/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java",
                "sha": "10a48ae28ac57acf57271d308a75467b865f302c",
                "status": "modified"
            }
        ],
        "message": "HBASE-10517 NPE in MetaCache.clearCache()\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/branches/hbase-10070@1567827 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/481a116e26faea7472328420469b6aa72ce378f1",
        "repo": "hbase",
        "unit_tests": [
            "TestMetaCache.java"
        ]
    },
    "hbase_9cfe7cf": {
        "bug_id": "hbase_9cfe7cf",
        "commit": "https://github.com/apache/hbase/commit/9cfe7cfa3c58e189256a83d764067612832b3daf",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/HConstants.java?ref=9cfe7cfa3c58e189256a83d764067612832b3daf",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "patch": "@@ -82,6 +82,9 @@\n   /** Cluster is fully-distributed */\n   public static final String CLUSTER_IS_DISTRIBUTED = \"true\";\n \n+  /** Default value for cluster distributed mode */  \n+  public static final String DEFAULT_CLUSTER_DISTRIBUTED = CLUSTER_IS_LOCAL;\n+\n   /** default host address */\n   public static final String DEFAULT_HOST = \"0.0.0.0\";\n ",
                "raw_url": "https://github.com/apache/hbase/raw/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "sha": "07041b5dccbb4cfc370f2e8c6ab614dfc065890e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java?ref=9cfe7cfa3c58e189256a83d764067612832b3daf",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "patch": "@@ -163,7 +163,8 @@ public static Properties parseZooCfg(Configuration conf,\n       }\n       // Special case for 'hbase.cluster.distributed' property being 'true'\n       if (key.startsWith(\"server.\")) {\n-        if (conf.get(HConstants.CLUSTER_DISTRIBUTED).equals(HConstants.CLUSTER_IS_DISTRIBUTED)\n+        if (conf.get(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED).\n+              equals(HConstants.CLUSTER_IS_DISTRIBUTED)\n             && value.startsWith(HConstants.LOCALHOST)) {\n           String msg = \"The server in zoo.cfg cannot be set to localhost \" +\n               \"in a fully-distributed setup because it won't be reachable. \" +",
                "raw_url": "https://github.com/apache/hbase/raw/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "sha": "b6fef32ed5e359d9b81acd326988a2cb1d54c203",
                "status": "modified"
            }
        ],
        "message": "HBASE-5633 NPE reading ZK config in HBase\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1304924 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/accf8ee8621010ce76169a99ce0daee76469c0af",
        "repo": "hbase",
        "unit_tests": [
            "TestZKConfig.java"
        ]
    },
    "hbase_9e628b7": {
        "bug_id": "hbase_9e628b7",
        "commit": "https://github.com/apache/hbase/commit/9e628b715d68c180a73f89b82ef8203fd66ef39d",
        "file": [
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hbase/blob/9e628b715d68c180a73f89b82ef8203fd66ef39d/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java?ref=9e628b715d68c180a73f89b82ef8203fd66ef39d",
                "deletions": 11,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "patch": "@@ -2177,24 +2177,37 @@ public long getHFilesSize() {\n   }\n \n   private long getTotalUmcompressedBytes(List<HStoreFile> files) {\n-    return files.stream().filter(f -> f != null && f.getReader() != null)\n-        .mapToLong(f -> f.getReader().getTotalUncompressedBytes()).sum();\n+    return files.stream().filter(f -> f != null).mapToLong(f -> {\n+      StoreFileReader reader = f.getReader();\n+      if (reader == null) {\n+        return 0;\n+      } else {\n+        return reader.getTotalUncompressedBytes();\n+      }\n+    }).sum();\n   }\n \n   private long getStorefilesSize(Collection<HStoreFile> files, Predicate<HStoreFile> predicate) {\n     return files.stream().filter(f -> f != null && f.getReader() != null).filter(predicate)\n-        .mapToLong(f -> f.getReader().length()).sum();\n+        .mapToLong(f -> {\n+          StoreFileReader reader = f.getReader();\n+          if (reader == null) {\n+            return 0;\n+          } else {\n+            return reader.length();\n+          }\n+        }).sum();\n   }\n \n   private long getStoreFileFieldSize(ToLongFunction<StoreFileReader> f) {\n-    return this.storeEngine.getStoreFileManager().getStorefiles().stream().filter(sf -> {\n-      if (sf.getReader() == null) {\n-        LOG.warn(\"StoreFile {} has a null Reader\", sf);\n-        return false;\n-      } else {\n-        return true;\n-      }\n-    }).map(HStoreFile::getReader).mapToLong(f).sum();\n+    return this.storeEngine.getStoreFileManager().getStorefiles().stream()\n+        .map(HStoreFile::getReader).filter(reader -> {\n+          if (reader == null) {\n+            return false;\n+          } else {\n+            return true;\n+          }\n+        }).mapToLong(f).sum();\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hbase/raw/9e628b715d68c180a73f89b82ef8203fd66ef39d/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "sha": "10bb030babe3c437ce8bc4474acd410afb78e865",
                "status": "modified"
            }
        ],
        "message": "HBASE-23159 HStore#getStorefilesSize may throw NPE",
        "parent": "https://github.com/apache/hbase/commit/73d69c6157515c28479fc78f224c8065d0c00abc",
        "repo": "hbase",
        "unit_tests": [
            "TestHStore.java"
        ]
    },
    "hbase_a4a4458": {
        "bug_id": "hbase_a4a4458",
        "commit": "https://github.com/apache/hbase/commit/a4a44587b3f2236750569f5c032b283dc77942f6",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/a4a44587b3f2236750569f5c032b283dc77942f6/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=a4a44587b3f2236750569f5c032b283dc77942f6",
                "deletions": 8,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "patch": "@@ -17,7 +17,10 @@\n  */\n package org.apache.hadoop.hbase;\n \n-import javax.annotation.Nullable;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n import java.io.File;\n import java.io.IOException;\n import java.io.OutputStream;\n@@ -44,6 +47,8 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import javax.annotation.Nullable;\n+\n import org.apache.commons.lang.RandomStringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -58,7 +63,6 @@\n import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.BufferedMutator;\n-import org.apache.hadoop.hbase.client.ClusterConnection;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.client.Consistency;\n@@ -84,6 +88,7 @@\n import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n import org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim;\n+import org.apache.hadoop.hbase.master.AssignmentManager;\n import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.RegionStates;\n import org.apache.hadoop.hbase.master.ServerManager;\n@@ -129,10 +134,6 @@\n import org.apache.zookeeper.ZooKeeper;\n import org.apache.zookeeper.ZooKeeper.States;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n-\n /**\n  * Facility for testing HBase. Replacement for\n  * old HBaseTestCase and HBaseClusterTestCase functionality.\n@@ -3767,8 +3768,11 @@ public String explainFailure() throws IOException {\n \n       @Override\n       public boolean evaluate() throws IOException {\n-        final RegionStates regionStates = getMiniHBaseCluster().getMaster()\n-            .getAssignmentManager().getRegionStates();\n+        HMaster master = getMiniHBaseCluster().getMaster();\n+        if (master == null) return false;\n+        AssignmentManager am = master.getAssignmentManager();\n+        if (am == null) return false;\n+        final RegionStates regionStates = am.getRegionStates();\n         return !regionStates.isRegionsInTransition();\n       }\n     };",
                "raw_url": "https://github.com/apache/hbase/raw/a4a44587b3f2236750569f5c032b283dc77942f6/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "sha": "006c3e71362f90732b2febbe947c9a28ec1858bd",
                "status": "modified"
            }
        ],
        "message": "Revert \"Revert \"HBASE-14909 NPE testing for RIT\"\"\nReverted the wrong patch... putting it back (revert of a revert)\n\nThis reverts commit 157a60f1b396ab9adc7f934a15352f2dbc5493a9.",
        "parent": "https://github.com/apache/hbase/commit/157a60f1b396ab9adc7f934a15352f2dbc5493a9",
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseTestingUtility.java"
        ]
    },
    "hbase_a95eb65": {
        "bug_id": "hbase_a95eb65",
        "commit": "https://github.com/apache/hbase/commit/a95eb6559d69781c20e05b88b5b9ba773328c691",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/a95eb6559d69781c20e05b88b5b9ba773328c691/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java?ref=a95eb6559d69781c20e05b88b5b9ba773328c691",
                "deletions": 6,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "patch": "@@ -151,21 +151,23 @@ private boolean requiresReplication(Optional<TableDescriptor> tableDesc, Entry e\n   private void getRegionLocations(CompletableFuture<RegionLocations> future,\n       TableDescriptor tableDesc, byte[] encodedRegionName, byte[] row, boolean reload) {\n     FutureUtils.addListener(connection.getRegionLocations(tableDesc.getTableName(), row, reload),\n-      (r, e) -> {\n+      (locs, e) -> {\n         if (e != null) {\n           future.completeExceptionally(e);\n           return;\n         }\n         // if we are not loading from cache, just return\n         if (reload) {\n-          future.complete(r);\n+          future.complete(locs);\n           return;\n         }\n         // check if the number of region replicas is correct, and also the primary region name\n-        // matches\n-        if (r.size() == tableDesc.getRegionReplication() && Bytes.equals(\n-          r.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(), encodedRegionName)) {\n-          future.complete(r);\n+        // matches, and also there is no null elements in the returned RegionLocations\n+        if (locs.size() == tableDesc.getRegionReplication() &&\n+          locs.size() == locs.numNonNullElements() &&\n+          Bytes.equals(locs.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(),\n+            encodedRegionName)) {\n+          future.complete(locs);\n         } else {\n           // reload again as the information in cache maybe stale\n           getRegionLocations(future, tableDesc, encodedRegionName, row, true);",
                "raw_url": "https://github.com/apache/hbase/raw/a95eb6559d69781c20e05b88b5b9ba773328c691/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "sha": "cc2650f803f2aa165c353c752d2ff15b14ad4960",
                "status": "modified"
            }
        ],
        "message": "HBASE-22328 NPE in RegionReplicaReplicationEndpoint",
        "parent": "https://github.com/apache/hbase/commit/6855d5837955c31f4774d42694d64a6d921bc1f5",
        "repo": "hbase",
        "unit_tests": [
            "TestRegionReplicaReplicationEndpoint.java"
        ]
    },
    "hbase_a97739b": {
        "bug_id": "hbase_a97739b",
        "commit": "https://github.com/apache/hbase/commit/a97739bd3fe6654c26057630ea5055e97a6eb487",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/a97739bd3fe6654c26057630ea5055e97a6eb487/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java?ref=a97739bd3fe6654c26057630ea5055e97a6eb487",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "patch": "@@ -63,7 +63,11 @@ public void doGet(HttpServletRequest request, HttpServletResponse response)\n     List<ServerName> servers = null;\n     Set<ServerName> deadServers = null;\n     \n-    if(master.isActiveMaster()){\n+    if(master.isActiveMaster()) {\n+      if (master.getServerManager() == null) {\n+        response.sendError(503, \"Master not ready\");\n+        return;\n+      }\n       metaLocation = getMetaLocationOrNull(master);\n       //ServerName metaLocation = master.getCatalogTracker().getMetaLocation();\n       servers = master.getServerManager().getOnlineServersList();",
                "raw_url": "https://github.com/apache/hbase/raw/a97739bd3fe6654c26057630ea5055e97a6eb487/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "sha": "49ab9d6bd80d43653032b346ec2f383e4c5c115a",
                "status": "modified"
            }
        ],
        "message": "HBASE-8975  NPE/HTTP 500 when opening the master's web UI too early\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1504955 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/581c3f2634e82343ed0d27a4a0ce5d407f445567",
        "repo": "hbase",
        "unit_tests": [
            "TestMasterStatusServlet.java"
        ]
    },
    "hbase_aba565a": {
        "bug_id": "hbase_aba565a",
        "commit": "https://github.com/apache/hbase/commit/aba565a22873439f7ee8f6d53091b3ce709fc96c",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMemcache.java?ref=aba565a22873439f7ee8f6d53091b3ce709fc96c",
                "deletions": 4,
                "filename": "src/java/org/apache/hadoop/hbase/HMemcache.java",
                "patch": "@@ -58,9 +58,7 @@\n   /**\n    * Constructor\n    */\n-  public HMemcache() {\n-    super();\n-  }\n+  public HMemcache() {}\n \n   /** represents the state of the memcache at a specified point in time */\n   static class Snapshot {\n@@ -320,7 +318,7 @@ HInternalScannerInterface getScanner(long timestamp,\n         // Generate list of iterators\n         HStoreKey firstKey = new HStoreKey(firstRow);\n         for(int i = 0; i < backingMaps.length; i++) {\n-          keyIterators[i] = (firstRow.getLength() != 0)?\n+          keyIterators[i] = (/*firstRow != null &&*/ firstRow.getLength() != 0)?\n             backingMaps[i].tailMap(firstKey).keySet().iterator():\n             backingMaps[i].keySet().iterator();\n           while(getNext(i)) {",
                "raw_url": "https://github.com/apache/hbase/raw/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "sha": "163c015136543d62b576f34adbde4ad701262830",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hbase/blob/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HRegion.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegion.java?ref=aba565a22873439f7ee8f6d53091b3ce709fc96c",
                "deletions": 12,
                "filename": "src/java/org/apache/hadoop/hbase/HRegion.java",
                "patch": "@@ -208,6 +208,7 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n \n   final int memcacheFlushSize;\n   final int blockingMemcacheSize;\n+  protected final long threadWakeFrequency;\n   private final HLocking lock = new HLocking();\n   private long desiredMaxFileSize;\n   private final long maxSequenceId;\n@@ -244,6 +245,7 @@ public HRegion(Path rootDir, HLog log, FileSystem fs, Configuration conf,\n     this.conf = conf;\n     this.regionInfo = regionInfo;\n     this.memcache = new HMemcache();\n+    this.threadWakeFrequency = conf.getLong(THREAD_WAKE_FREQUENCY, 10 * 1000);\n \n     // Declare the regionName.  This is a unique string for the region, used to \n     // build a unique filename.\n@@ -1055,24 +1057,28 @@ public long startUpdate(Text row) throws IOException {\n    * the notify.\n    */\n   private synchronized void checkResources() {\n-    if (checkCommitsSinceFlush()) {\n-      return;\n-    }\n+    boolean blocked = false;\n     \n-    LOG.warn(\"Blocking updates for '\" + Thread.currentThread().getName() +\n-      \"': Memcache size \" +\n-      StringUtils.humanReadableInt(this.memcache.getSize()) +\n-      \" is >= than blocking \" +\n-      StringUtils.humanReadableInt(this.blockingMemcacheSize) + \" size\");\n     while (!checkCommitsSinceFlush()) {\n+      if (!blocked) {\n+        LOG.info(\"Blocking updates for '\" + Thread.currentThread().getName() +\n+            \"': Memcache size \" +\n+            StringUtils.humanReadableInt(this.memcache.getSize()) +\n+            \" is >= than blocking \" +\n+            StringUtils.humanReadableInt(this.blockingMemcacheSize) + \" size\");\n+      }\n+\n+      blocked = true;\n       try {\n-        wait();\n+        wait(threadWakeFrequency);\n       } catch (InterruptedException e) {\n         // continue;\n       }\n     }\n-    LOG.warn(\"Unblocking updates for '\" + Thread.currentThread().getName() +\n-      \"'\");\n+    if (blocked) {\n+      LOG.info(\"Unblocking updates for '\" + Thread.currentThread().getName() +\n+          \"'\");\n+    }\n   }\n   \n   /*\n@@ -1635,4 +1641,4 @@ static boolean deleteRegion(FileSystem fs, Path baseDirectory,\n   public static Path getRegionDir(final Path dir, final Text regionName) {\n     return new Path(dir, new Path(HREGIONDIR_PREFIX + regionName));\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hbase/raw/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HRegion.java",
                "sha": "f7e4b55bc4cf3ae43ec07a5f904d8de64c0574be",
                "status": "modified"
            }
        ],
        "message": "HADOOP-1797 Fix NPEs in MetaScanner constructor\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@571333 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/12a62a6333dbe317af987a303e4e3c9405608ee3",
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_b5e2145": {
        "bug_id": "hbase_b5e2145",
        "commit": "https://github.com/apache/hbase/commit/b5e2145879254acb416e8414924ab9256d55bd31",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/b5e2145879254acb416e8414924ab9256d55bd31/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java?ref=b5e2145879254acb416e8414924ab9256d55bd31",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "patch": "@@ -762,7 +762,7 @@ private static Path findOrCreateJar(Class<?> my_class, FileSystem fs,\n     }\n \n     if (null == jar || jar.isEmpty()) {\n-      throw new IOException(\"Cannot locate resource for class \" + my_class.getName());\n+      return null;\n     }\n \n     LOG.debug(String.format(\"For class %s, using jar %s\", my_class.getName(), jar));\n@@ -776,6 +776,9 @@ private static Path findOrCreateJar(Class<?> my_class, FileSystem fs,\n    * @param packagedClasses map[class -> jar]\n    */\n   private static void updateMap(String jar, Map<String, String> packagedClasses) throws IOException {\n+    if (null == jar || jar.isEmpty()) {\n+      return;\n+    }\n     ZipFile zip = null;\n     try {\n       zip = new ZipFile(jar);",
                "raw_url": "https://github.com/apache/hbase/raw/b5e2145879254acb416e8414924ab9256d55bd31/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "sha": "cc66882c03aaedf21611960675482825e2c6e04d",
                "status": "modified"
            }
        ],
        "message": "HBASE-10061 TableMapReduceUtil.findOrCreateJar calls updateMap(null, ) resulting in thrown NPE (Amit Sela)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1548747 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/656b5ca9f73ee9cb806ec39fc32001fa9f32df3c",
        "repo": "hbase",
        "unit_tests": [
            "TestTableMapReduceUtil.java"
        ]
    },
    "hbase_b7d932f": {
        "bug_id": "hbase_b7d932f",
        "commit": "https://github.com/apache/hbase/commit/b7d932f4dc1d1cee73142376baaee212df9d6ae0",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/b7d932f4dc1d1cee73142376baaee212df9d6ae0/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=b7d932f4dc1d1cee73142376baaee212df9d6ae0",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -15,6 +15,7 @@ Trunk (unreleased changes)\n   BUG FIXES\r\n    HADOOP-2731 Under load, regions become extremely large and eventually cause\r\n                region servers to become unresponsive\r\n+   HADOOP-2693 NPE in getClosestRowBefore (Bryan Duxbury & Stack)\r\n \r\n   IMPROVEMENTS\r\n    HADOOP-2555 Refactor the HTable#get and HTable#getRow methods to avoid\r",
                "raw_url": "https://github.com/apache/hbase/raw/b7d932f4dc1d1cee73142376baaee212df9d6ae0/CHANGES.txt",
                "sha": "1e60afff71fd9da9c9ff74959202eecdcd388ab1",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/b7d932f4dc1d1cee73142376baaee212df9d6ae0/src/java/org/apache/hadoop/hbase/HStore.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=b7d932f4dc1d1cee73142376baaee212df9d6ae0",
                "deletions": 14,
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "patch": "@@ -1830,20 +1830,13 @@ public Text getRowKeyAtOrBefore(final Text row, final long timestamp)\n       for(int i = maparray.length - 1; i >= 0; i--) {\n         Text row_from_mapfile = \n           rowAtOrBeforeFromMapFile(maparray[i], row, timestamp);\n-\n-        // for when we have MapFile.Reader#getClosest before functionality\n-/*        Text row_from_mapfile = null;\n-        WritableComparable value = null; \n-        \n-        HStoreKey hskResult = \n-          (HStoreKey)maparray[i].getClosest(rowKey, value, true);\n-        \n-        if (hskResult != null) {\n-          row_from_mapfile = hskResult.getRow();\n-        }*/\n-                \n-/*        LOG.debug(\"Best from this mapfile was \" + row_from_mapfile);*/\n         \n+        // if the result from the mapfile is null, then we know that\n+        // the mapfile was empty and can move on to the next one.\n+        if (row_from_mapfile == null) {\n+          continue;\n+        }\n+      \n         // short circuit on an exact match\n         if (row.equals(row_from_mapfile)) {\n           return row;\n@@ -1855,7 +1848,6 @@ public Text getRowKeyAtOrBefore(final Text row, final long timestamp)\n         }\n       }\n       \n-/*      LOG.debug(\"Went searching for \" + row + \", found \" + bestSoFar);*/\n       return bestSoFar;\n     } finally {\n       this.lock.readLock().unlock();",
                "raw_url": "https://github.com/apache/hbase/raw/b7d932f4dc1d1cee73142376baaee212df9d6ae0/src/java/org/apache/hadoop/hbase/HStore.java",
                "sha": "ab5f7f1fe7f29d44504fe9d7e5b3e653f3cb66ab",
                "status": "modified"
            }
        ],
        "message": "HADOOP-2693 NPE in getClosestRowBefore\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/core/trunk/src/contrib/hbase@617724 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/3e351091b6695c6344ad382ee28db8af0627e2e5",
        "repo": "hbase",
        "unit_tests": [
            "TestHStore.java"
        ]
    },
    "hbase_b94b0e9": {
        "bug_id": "hbase_b94b0e9",
        "commit": "https://github.com/apache/hbase/commit/b94b0e9ca76f5ab2fad648dae052dce85e578628",
        "file": [
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hbase/blob/b94b0e9ca76f5ab2fad648dae052dce85e578628/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java?ref=b94b0e9ca76f5ab2fad648dae052dce85e578628",
                "deletions": 16,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "patch": "@@ -900,25 +900,28 @@ private static void updateMap(String jar, Map<String, String> packagedClasses) t\n   private static String findContainingJar(Class<?> my_class, Map<String, String> packagedClasses)\n       throws IOException {\n     ClassLoader loader = my_class.getClassLoader();\n+\n     String class_file = my_class.getName().replaceAll(\"\\\\.\", \"/\") + \".class\";\n \n-    // first search the classpath\n-    for (Enumeration<URL> itr = loader.getResources(class_file); itr.hasMoreElements();) {\n-      URL url = itr.nextElement();\n-      if (\"jar\".equals(url.getProtocol())) {\n-        String toReturn = url.getPath();\n-        if (toReturn.startsWith(\"file:\")) {\n-          toReturn = toReturn.substring(\"file:\".length());\n+    if (loader != null) {\n+      // first search the classpath\n+      for (Enumeration<URL> itr = loader.getResources(class_file); itr.hasMoreElements();) {\n+        URL url = itr.nextElement();\n+        if (\"jar\".equals(url.getProtocol())) {\n+          String toReturn = url.getPath();\n+          if (toReturn.startsWith(\"file:\")) {\n+            toReturn = toReturn.substring(\"file:\".length());\n+          }\n+          // URLDecoder is a misnamed class, since it actually decodes\n+          // x-www-form-urlencoded MIME type rather than actual\n+          // URL encoding (which the file path has). Therefore it would\n+          // decode +s to ' 's which is incorrect (spaces are actually\n+          // either unencoded or encoded as \"%20\"). Replace +s first, so\n+          // that they are kept sacred during the decoding process.\n+          toReturn = toReturn.replaceAll(\"\\\\+\", \"%2B\");\n+          toReturn = URLDecoder.decode(toReturn, \"UTF-8\");\n+          return toReturn.replaceAll(\"!.*$\", \"\");\n         }\n-        // URLDecoder is a misnamed class, since it actually decodes\n-        // x-www-form-urlencoded MIME type rather than actual\n-        // URL encoding (which the file path has). Therefore it would\n-        // decode +s to ' 's which is incorrect (spaces are actually\n-        // either unencoded or encoded as \"%20\"). Replace +s first, so\n-        // that they are kept sacred during the decoding process.\n-        toReturn = toReturn.replaceAll(\"\\\\+\", \"%2B\");\n-        toReturn = URLDecoder.decode(toReturn, \"UTF-8\");\n-        return toReturn.replaceAll(\"!.*$\", \"\");\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/b94b0e9ca76f5ab2fad648dae052dce85e578628/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "sha": "b5149415a5e6abbb9cdfaec75604bd2c6c6056b3",
                "status": "modified"
            }
        ],
        "message": "HBASE-12491 TableMapReduceUtil.findContainingJar() NPE",
        "parent": "https://github.com/apache/hbase/commit/555e78005df2b1f211e013acc5f743df9c7b80ab",
        "repo": "hbase",
        "unit_tests": [
            "TestTableMapReduceUtil.java"
        ]
    },
    "hbase_bfe27f5": {
        "bug_id": "hbase_bfe27f5",
        "commit": "https://github.com/apache/hbase/commit/bfe27f5764b6a9b1b23599e140b6c699bba572ed",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/bfe27f5764b6a9b1b23599e140b6c699bba572ed/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=bfe27f5764b6a9b1b23599e140b6c699bba572ed",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -802,6 +802,7 @@ Release 0.90.0 - Unreleased\n    HBASE-3374  Our jruby jar has *GPL jars in it; fix\n    HBASE-3343  Server not shutting down after losing log lease\n    HBASE-3381  Interrupt of a region open comes across as a successful open\n+   HBASE-3386  NPE in TableRecordReaderImpl.restart\n \n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hbase/raw/bfe27f5764b6a9b1b23599e140b6c699bba572ed/CHANGES.txt",
                "sha": "f73e9a409d5fcda41688c031400a385842c7105e",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/bfe27f5764b6a9b1b23599e140b6c699bba572ed/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java?ref=bfe27f5764b6a9b1b23599e140b6c699bba572ed",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "patch": "@@ -94,6 +94,11 @@\n   public RecordReader<ImmutableBytesWritable, Result> createRecordReader(\n       InputSplit split, TaskAttemptContext context)\n   throws IOException {\n+    if (table == null) {\n+      throw new IOException(\"Cannot create a record reader because of a\" +\n+          \" previous error. Please look at the previous logs lines from\" +\n+          \" the task's full log for more details.\");\n+    }\n     TableSplit tSplit = (TableSplit) split;\n     TableRecordReader trr = this.tableRecordReader;\n     // if no table record reader was provided use default",
                "raw_url": "https://github.com/apache/hbase/raw/bfe27f5764b6a9b1b23599e140b6c699bba572ed/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "sha": "c813c490a365c2445725af3c8122cd5dc7b329ca",
                "status": "modified"
            }
        ],
        "message": "HBASE-3386  NPE in TableRecordReaderImpl.restart\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1052051 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/94cc5237b0576d9f36f51b7142ef913cc032d9bc",
        "repo": "hbase",
        "unit_tests": [
            "TestTableInputFormatBase.java"
        ]
    },
    "hbase_c0598dc": {
        "bug_id": "hbase_c0598dc",
        "commit": "https://github.com/apache/hbase/commit/c0598dcb10f2185164548ad8a1fdf1867d6a0b34",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/c0598dcb10f2185164548ad8a1fdf1867d6a0b34/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=c0598dcb10f2185164548ad8a1fdf1867d6a0b34",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "patch": "@@ -7585,7 +7585,7 @@ private WriteEntry doWALAppend(WALEdit walEdit, Durability durability, List<UUID\n       }\n       writeEntry = walKey.getWriteEntry();\n     } catch (IOException ioe) {\n-      if (walKey != null) {\n+      if (walKey != null && walKey.getWriteEntry() != null) {\n         mvcc.complete(walKey.getWriteEntry());\n       }\n       throw ioe;",
                "raw_url": "https://github.com/apache/hbase/raw/c0598dcb10f2185164548ad8a1fdf1867d6a0b34/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "sha": "6401a8b957ef6473da0c93384c534e19c876126e",
                "status": "modified"
            }
        ],
        "message": "HBASE-19593 Possible NPE if wal is closed during waledit append.(Rajeshabbu)",
        "parent": "https://github.com/apache/hbase/commit/448ba3a78f50df2ffac874c3768e9f50d52b15f6",
        "repo": "hbase",
        "unit_tests": [
            "TestHRegion.java"
        ]
    },
    "hbase_c4911d3": {
        "bug_id": "hbase_c4911d3",
        "commit": "https://github.com/apache/hbase/commit/c4911d3230749947f107f2188b4da1615bb1fab1",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/c4911d3230749947f107f2188b4da1615bb1fab1/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java?ref=c4911d3230749947f107f2188b4da1615bb1fab1",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "patch": "@@ -154,7 +154,9 @@ public void stopReplicationService() {\n   public void join() {\n     if (this.replication) {\n       this.replicationManager.join();\n-      this.replicationSink.stopReplicationSinkServices();\n+      if (this.replicationSink != null) {\n+        this.replicationSink.stopReplicationSinkServices();\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/c4911d3230749947f107f2188b4da1615bb1fab1/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "sha": "71aa2c76ea8241a2dcc70c4610759916782be527",
                "status": "modified"
            }
        ],
        "message": "HBASE-8230 Possible NPE on regionserver abort if replication service has not been started\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1464280 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/e7bebd470650488cafd94a37462e6c1fd7833edf",
        "repo": "hbase",
        "unit_tests": [
            "ReplicationTests.java"
        ]
    },
    "hbase_c58e80f": {
        "bug_id": "hbase_c58e80f",
        "commit": "https://github.com/apache/hbase/commit/c58e80fbe6db6426e652ec363149b92f9e33fbb0",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/c58e80fbe6db6426e652ec363149b92f9e33fbb0/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java?ref=c58e80fbe6db6426e652ec363149b92f9e33fbb0",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "patch": "@@ -492,7 +492,8 @@ public Void read() {\n           sink.publishReadTiming(serverName, region, column, stopWatch.getTime());\n         } catch (Exception e) {\n           sink.publishReadFailure(serverName, region, column, e);\n-          sink.updateReadFailures(region.getRegionNameAsString(), serverName.getHostname());\n+          sink.updateReadFailures(region == null? \"NULL\": region.getRegionNameAsString(),\n+              serverName == null? \"NULL\": serverName.getHostname());\n         } finally {\n           if (rs != null) {\n             rs.close();\n@@ -1579,6 +1580,10 @@ private void createWriteTable(int numberOfServers) throws IOException {\n       try (RegionLocator regionLocator =\n                admin.getConnection().getRegionLocator(tableDesc.getTableName())) {\n         for (HRegionLocation location: regionLocator.getAllRegionLocations()) {\n+          if (location == null) {\n+            LOG.warn(\"Null location\");\n+            continue;\n+          }\n           ServerName rs = location.getServerName();\n           RegionInfo region = location.getRegion();\n           tasks.add(new RegionTask(admin.getConnection(), region, rs, (RegionStdOutSink)sink,\n@@ -1795,6 +1800,10 @@ private void monitorRegionServers(Map<String, List<RegionInfo>> rsAndRMap, Regio\n           try (RegionLocator regionLocator =\n                    this.admin.getConnection().getRegionLocator(tableDesc.getTableName())) {\n             for (HRegionLocation location : regionLocator.getAllRegionLocations()) {\n+              if (location == null) {\n+                LOG.warn(\"Null location\");\n+                continue;\n+              }\n               ServerName rs = location.getServerName();\n               String rsName = rs.getHostname();\n               RegionInfo r = location.getRegion();",
                "raw_url": "https://github.com/apache/hbase/raw/c58e80fbe6db6426e652ec363149b92f9e33fbb0/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "sha": "a967dab81750d974a570f908a02de69bd8fc4666",
                "status": "modified"
            }
        ],
        "message": "HBASE-23244 NPEs running Canary (#784)\n\nSigned-off-by: Viraj Jasani <virajjasani007@gmail.com>",
        "parent": "https://github.com/apache/hbase/commit/b8a4504a2609d6d6d26e0b839a368912b3276feb",
        "repo": "hbase",
        "unit_tests": [
            "TestCanaryTool.java"
        ]
    },
    "hbase_c74cf12": {
        "bug_id": "hbase_c74cf12",
        "commit": "https://github.com/apache/hbase/commit/c74cf12925b810b7a59c5b639834508f00054053",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/c74cf12925b810b7a59c5b639834508f00054053/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=c74cf12925b810b7a59c5b639834508f00054053",
                "deletions": 0,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "patch": "@@ -2223,6 +2223,12 @@ public int run(String[] args) throws Exception {\n         throw new IllegalArgumentException(\"Number of clients must be > 0\");\n       }\n \n+      // cmdName should not be null, print help and exit\n+      if (opts.cmdName == null) {\n+        printUsage();\n+        return errCode;\n+      }\n+\n       Class<? extends Test> cmdClass = determineCommandClass(opts.cmdName);\n       if (cmdClass != null) {\n         runTest(cmdClass, opts);",
                "raw_url": "https://github.com/apache/hbase/raw/c74cf12925b810b7a59c5b639834508f00054053/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "sha": "7f1c6408c550319b449c4c78b58845919952b284",
                "status": "modified"
            }
        ],
        "message": "HBASE-17357 FIX PerformanceEvaluation parameters parsing triggers NPE.\n\ncheck command name is not null, if null print usage and exit\n\nSigned-off-by: Michael Stack <stack@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/79018056f542cde5850b1d1fc2fe248f0007fd66",
        "repo": "hbase",
        "unit_tests": [
            "TestPerformanceEvaluation.java"
        ]
    },
    "hbase_cf3284d": {
        "bug_id": "hbase_cf3284d",
        "commit": "https://github.com/apache/hbase/commit/cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -168,6 +168,7 @@ Release 0.90.2 - Unreleased\n    HBASE-3617  NoRouteToHostException during balancing will cause Master abort\n                (Ted Yu via Stack)\n    HBASE-3668  CatalogTracker.waitForMeta can wait forever and totally stall a RS\n+   HBASE-3627  NPE in EventHandler when region already reassigned\n \n   IMPROVEMENTS\n    HBASE-3542  MultiGet methods in Thrift",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/CHANGES.txt",
                "sha": "5beea27f13e9ef0c15545e94f587b59ab32f0cc3",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.io.IOException;\n import java.net.ConnectException;\n import java.net.SocketTimeoutException;\n+import java.net.SocketException;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n import org.apache.commons.logging.Log;\n@@ -390,8 +391,11 @@ private HRegionInterface getCachedConnection(HServerAddress address)\n         throw e;\n       }\n     } catch (SocketTimeoutException e) {\n-      // We were passed the wrong address.  Return 'protocol' == null.\n+      // Return 'protocol' == null.\n       LOG.debug(\"Timed out connecting to \" + address);\n+    } catch (SocketException e) {\n+      // Return 'protocol' == null.\n+      LOG.debug(\"Exception connecting to \" + address);\n     } catch (IOException ioe) {\n       Throwable cause = ioe.getCause();\n       if (cause != null && cause instanceof EOFException) {",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "sha": "be311797c81a8894d9280cad6b77c618bedff118",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 8,
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "patch": "@@ -308,14 +308,9 @@ private static HServerAddress readLocation(HRegionInterface metaServer,\n     } catch (java.net.SocketTimeoutException e) {\n       // Treat this exception + message as unavailable catalog table. Catch it\n       // and fall through to return a null\n-    } catch (java.net.ConnectException e) {\n-      if (e.getMessage() != null &&\n-          e.getMessage().contains(\"Connection refused\")) {\n-        // Treat this exception + message as unavailable catalog table. Catch it\n-        // and fall through to return a null\n-      } else {\n-        throw e;\n-      }\n+    } catch (java.net.SocketException e) {\n+      // Treat this exception + message as unavailable catalog table. Catch it\n+      // and fall through to return a null\n     } catch (RemoteException re) {\n       IOException ioe = re.unwrapRemoteException();\n       if (ioe instanceof NotServingRegionException) {",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "sha": "6e22cf5e44997f7b258d4666a6c3d78f262d135f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 0,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -1770,6 +1770,10 @@ protected void chore() {\n                   Stat stat = new Stat();\n                   RegionTransitionData data = ZKAssign.getDataNoWatch(watcher,\n                       node, stat);\n+                  if (data == null) {\n+                    LOG.warn(\"Data is null, node \" + node + \" no longer exists\");\n+                    break;\n+                  }\n                   if (data.getEventType() == EventType.RS_ZK_REGION_OPENED) {\n                     LOG.debug(\"Region has transitioned to OPENED, allowing \" +\n                         \"watched event handlers to process\");",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "e9b2af27b6a8c96656079194719260f8ca06b037",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java",
                "patch": "@@ -87,7 +87,11 @@ public void process() throws IOException {\n \n     // If fails, just return.  Someone stole the region from under us.\n     // Calling transitionZookeeperOfflineToOpening initalizes this.version.\n-    if (!transitionZookeeperOfflineToOpening(encodedName)) return;\n+    if (!transitionZookeeperOfflineToOpening(encodedName)) {\n+      LOG.warn(\"Region was hijacked? It no longer exists, encodedName=\" +\n+        encodedName);\n+      return;\n+    }\n \n     // Open region.  After a successful open, failures in subsequent processing\n     // needs to do a close as part of cleanup.\n@@ -254,7 +258,7 @@ private boolean transitionToOpened(final HRegion r) throws IOException {\n   /**\n    * @return Instance of HRegion if successful open else null.\n    */\n-  private HRegion openRegion() {\n+  HRegion openRegion() {\n     HRegion region = null;\n     try {\n       // Instantiate the region.  This also periodically tickles our zk OPENING",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java",
                "sha": "441b48419e4b85f91a73e3b3eb54c0d96268a41b",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 5,
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java",
                "patch": "@@ -396,7 +396,8 @@ public static boolean deleteNode(ZooKeeperWatcher zkw, String regionName,\n     zkw.sync(node);\n     Stat stat = new Stat();\n     byte [] bytes = ZKUtil.getDataNoWatch(zkw, node, stat);\n-    if(bytes == null) {\n+    if (bytes == null) {\n+      // If it came back null, node does not exist.\n       throw KeeperException.create(Code.NONODE);\n     }\n     RegionTransitionData data = RegionTransitionData.fromBytes(bytes);\n@@ -674,8 +675,11 @@ public static int transitionNode(ZooKeeperWatcher zkw, HRegionInfo region,\n \n     // Read existing data of the node\n     Stat stat = new Stat();\n-    byte [] existingBytes =\n-      ZKUtil.getDataNoWatch(zkw, node, stat);\n+    byte [] existingBytes = ZKUtil.getDataNoWatch(zkw, node, stat);\n+    if (existingBytes == null) {\n+      // Node no longer exists.  Return -1. It means unsuccessful transition.\n+      return -1;\n+    }\n     RegionTransitionData existingData =\n       RegionTransitionData.fromBytes(existingBytes);\n \n@@ -762,7 +766,7 @@ public static RegionTransitionData getData(ZooKeeperWatcher zkw,\n    * @param zkw zk reference\n    * @param pathOrRegionName fully-specified path or region name\n    * @param stat object to store node info into on getData call\n-   * @return data for the unassigned node\n+   * @return data for the unassigned node or null if node does not exist\n    * @throws KeeperException if unexpected zookeeper exception\n    */\n   public static RegionTransitionData getDataNoWatch(ZooKeeperWatcher zkw,\n@@ -771,7 +775,7 @@ public static RegionTransitionData getDataNoWatch(ZooKeeperWatcher zkw,\n     String node = pathOrRegionName.startsWith(\"/\") ?\n         pathOrRegionName : getNodeName(zkw, pathOrRegionName);\n     byte [] data = ZKUtil.getDataNoWatch(zkw, node, stat);\n-    if(data == null) {\n+    if (data == null) {\n       return null;\n     }\n     return RegionTransitionData.fromBytes(data);",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java",
                "sha": "34e17b60638fc698d4bbfe1e49a09312719cd4b9",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "patch": "@@ -576,7 +576,7 @@ public static int getNumberOfChildren(ZooKeeperWatcher zkw, String znode)\n    * @param zkw zk reference\n    * @param znode path of node\n    * @param stat node status to set if node exists\n-   * @return data of the specified znode, or null if does not exist\n+   * @return data of the specified znode, or null if node does not exist\n    * @throws KeeperException if unexpected zookeeper exception\n    */\n   public static byte [] getDataNoWatch(ZooKeeperWatcher zkw, String znode,",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "sha": "08748f810272dbcc582ed381d1979fa33b238f08",
                "status": "modified"
            }
        ],
        "message": "HBASE-3627 NPE in EventHandler when region already reassigned\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1085075 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/237bc82af2992a82d0f71648f68311c4dab25f3e",
        "repo": "hbase",
        "unit_tests": [
            "TestZKUtil.java"
        ]
    },
    "hbase_d64c76f": {
        "bug_id": "hbase_d64c76f",
        "commit": "https://github.com/apache/hbase/commit/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/CHANGES.txt",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0",
                "deletions": 6,
                "filename": "CHANGES.txt",
                "patch": "@@ -256,14 +256,15 @@ Release 0.91.0 - Unreleased\n Release 0.90.4 - Unreleased\n \n   BUG FIXES\n-   HBASE-3878 Hbase client throws NoSuchElementException (Ted Yu)\n-   HBASE-3878 Hbase client throws NoSuchElementException (Ted Yu)\n-   HBASE-3881 Add disable balancer in graceful_stop.sh script\n-   HBASE-3895 Fix order of parameters after HBASE-1511\n+   HBASE-3878  Hbase client throws NoSuchElementException (Ted Yu)\n+   HBASE-3881  Add disable balancer in graceful_stop.sh script\n+   HBASE-3895  Fix order of parameters after HBASE-1511\n+   HBASE-3874  ServerShutdownHandler fails on NPE if a plan has a random\n+               region assignment\n \n   IMPROVEMENT\n-   HBASE-3882 hbase-config.sh needs to be updated so it can auto-detects the\n-              sun jre provided by RHEL6 (Roman Shaposhnik)\n+   HBASE-3882  hbase-config.sh needs to be updated so it can auto-detects the\n+               sun jre provided by RHEL6 (Roman Shaposhnik)\n \n Release 0.90.3 - Unreleased\n ",
                "raw_url": "https://github.com/apache/hbase/raw/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/CHANGES.txt",
                "sha": "d48d7d7fdf89099029f2ef59d662d3ac2b07f779",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -1995,7 +1995,9 @@ protected void chore() {\n       for (Iterator <Map.Entry<String, RegionPlan>> i =\n           this.regionPlans.entrySet().iterator(); i.hasNext();) {\n         Map.Entry<String, RegionPlan> e = i.next();\n-        if (e.getValue().getDestination().equals(sn)) {\n+        ServerName otherSn = e.getValue().getDestination();\n+        // The name will be null if the region is planned for a random assign.\n+        if (otherSn != null && otherSn.equals(sn)) {\n           // Use iterator's remove else we'll get CME\n           i.remove();\n         }",
                "raw_url": "https://github.com/apache/hbase/raw/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "7e473095b11a15233e608aa48d6c9c431ebd6aed",
                "status": "modified"
            }
        ],
        "message": "   HBASE-3874  ServerShutdownHandler fails on NPE if a plan has a random\n               region assignment\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1124477 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/cf8be18f159b21137885e8c579e415a0cc1e323e",
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_da0cc59": {
        "bug_id": "hbase_da0cc59",
        "commit": "https://github.com/apache/hbase/commit/da0cc598feab995eed12527d90805dd627674035",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hbase/blob/da0cc598feab995eed12527d90805dd627674035/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=da0cc598feab995eed12527d90805dd627674035",
                "deletions": 8,
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "patch": "@@ -17,7 +17,10 @@\n  */\n package org.apache.hadoop.hbase;\n \n-import javax.annotation.Nullable;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n import java.io.File;\n import java.io.IOException;\n import java.io.OutputStream;\n@@ -44,6 +47,8 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import javax.annotation.Nullable;\n+\n import org.apache.commons.lang.RandomStringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -58,7 +63,6 @@\n import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.BufferedMutator;\n-import org.apache.hadoop.hbase.client.ClusterConnection;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.client.Consistency;\n@@ -84,6 +88,7 @@\n import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n import org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim;\n+import org.apache.hadoop.hbase.master.AssignmentManager;\n import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.RegionStates;\n import org.apache.hadoop.hbase.master.ServerManager;\n@@ -129,10 +134,6 @@\n import org.apache.zookeeper.ZooKeeper;\n import org.apache.zookeeper.ZooKeeper.States;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n-\n /**\n  * Facility for testing HBase. Replacement for\n  * old HBaseTestCase and HBaseClusterTestCase functionality.\n@@ -3767,8 +3768,11 @@ public String explainFailure() throws IOException {\n \n       @Override\n       public boolean evaluate() throws IOException {\n-        final RegionStates regionStates = getMiniHBaseCluster().getMaster()\n-            .getAssignmentManager().getRegionStates();\n+        HMaster master = getMiniHBaseCluster().getMaster();\n+        if (master == null) return false;\n+        AssignmentManager am = master.getAssignmentManager();\n+        if (am == null) return false;\n+        final RegionStates regionStates = am.getRegionStates();\n         return !regionStates.isRegionsInTransition();\n       }\n     };",
                "raw_url": "https://github.com/apache/hbase/raw/da0cc598feab995eed12527d90805dd627674035/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "sha": "006c3e71362f90732b2febbe947c9a28ec1858bd",
                "status": "modified"
            }
        ],
        "message": "HBASE-14909 NPE testing for RIT",
        "parent": "https://github.com/apache/hbase/commit/08f90f30b385bbf314f7fb014b810d86330d60b1",
        "repo": "hbase",
        "unit_tests": [
            "TestHBaseTestingUtility.java"
        ]
    },
    "hbase_db3ba65": {
        "bug_id": "hbase_db3ba65",
        "commit": "https://github.com/apache/hbase/commit/db3ba652f88083b0b1c57b4857f11fce7ae5b131",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hbase/blob/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java?ref=db3ba652f88083b0b1c57b4857f11fce7ae5b131",
                "deletions": 4,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "patch": "@@ -1190,9 +1190,15 @@ private void receiveGlobalFailure(\n         byte[] row = e.getValue().iterator().next().getAction().getRow();\n         // Do not use the exception for updating cache because it might be coming from\n         // any of the regions in the MultiAction.\n-        if (tableName != null) {\n-          connection.updateCachedLocations(tableName, regionName, row,\n+        try {\n+          if (tableName != null) {\n+            connection.updateCachedLocations(tableName, regionName, row,\n               ClientExceptionsUtil.isMetaClearingException(t) ? null : t, server);\n+          }\n+        } catch (Throwable ex) {\n+          // That should never happen, but if it did, we want to make sure\n+          // we still process errors\n+          LOG.error(\"Couldn't update cached region locations: \" + ex);\n         }\n         for (Action<Row> action : e.getValue()) {\n           Retry retry = manageError(\n@@ -1317,8 +1323,14 @@ private void receiveMultiAction(MultiAction<Row> multiAction,\n             // Register corresponding failures once per server/once per region.\n             if (!regionFailureRegistered) {\n               regionFailureRegistered = true;\n-              connection.updateCachedLocations(\n+              try {\n+                connection.updateCachedLocations(\n                   tableName, regionName, row.getRow(), result, server);\n+              } catch (Throwable ex) {\n+                // That should never happen, but if it did, we want to make sure\n+                // we still process errors\n+                LOG.error(\"Couldn't update cached region locations: \" + ex);\n+              }\n             }\n             if (failureCount == 0) {\n               errorsByServer.reportServerError(server);\n@@ -1372,8 +1384,14 @@ private void receiveMultiAction(MultiAction<Row> multiAction,\n           // for every possible exception that comes through, however.\n           connection.clearCaches(server);\n         } else {\n-          connection.updateCachedLocations(\n+          try {\n+            connection.updateCachedLocations(\n               tableName, region, actions.get(0).getAction().getRow(), throwable, server);\n+          } catch (Throwable ex) {\n+            // That should never happen, but if it did, we want to make sure\n+            // we still process errors\n+            LOG.error(\"Couldn't update cached region locations: \" + ex);\n+          }\n         }\n         failureCount += actions.size();\n ",
                "raw_url": "https://github.com/apache/hbase/raw/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "sha": "142e2a0647fb231f3b6f2a5ef67970b1e01db045",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java?ref=db3ba652f88083b0b1c57b4857f11fce7ae5b131",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "patch": "@@ -63,6 +63,7 @@\n   private static final String MEMLOAD_BASE = \"memstoreLoad_\";\n   private static final String HEAP_BASE = \"heapOccupancy_\";\n   private static final String CACHE_BASE = \"cacheDroppingExceptions_\";\n+  private static final String UNKNOWN_EXCEPTION = \"UnknownException\";\n   private static final String CLIENT_SVC = ClientService.getDescriptor().getName();\n \n   /** A container class for collecting details about the RPC call as it percolates. */\n@@ -464,7 +465,8 @@ public void updateRpc(MethodDescriptor method, Message param, CallStats stats) {\n   }\n \n   public void incrCacheDroppingExceptions(Object exception) {\n-    getMetric(CACHE_BASE + exception.getClass().getSimpleName(),\n+    getMetric(CACHE_BASE +\n+      (exception == null? UNKNOWN_EXCEPTION : exception.getClass().getSimpleName()),\n       cacheDroppingExceptions, counterFactory).inc();\n   }\n }",
                "raw_url": "https://github.com/apache/hbase/raw/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "sha": "53a33264cb41df0cf8337abc1402f7c52f2f63b5",
                "status": "modified"
            }
        ],
        "message": "HBASE-15524 Fix NPE in client-side metrics",
        "parent": "https://github.com/apache/hbase/commit/fd5c0934b60664ecdde21a994910953339c7060d",
        "repo": "hbase",
        "unit_tests": [
            "TestMetricsConnection.java"
        ]
    },
    "hbase_dfc6399": {
        "bug_id": "hbase_dfc6399",
        "commit": "https://github.com/apache/hbase/commit/dfc63998d20a956bf7a44ed6bcd7c495801fc4bf",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hbase/blob/dfc63998d20a956bf7a44ed6bcd7c495801fc4bf/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java?ref=dfc63998d20a956bf7a44ed6bcd7c495801fc4bf",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.hadoop.hbase.mapreduce;\n \n import java.io.IOException;\n+import java.io.InterruptedIOException;\n import java.net.InetAddress;\n import java.util.ArrayList;\n import java.util.HashMap;\n@@ -31,7 +32,6 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n-import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HServerAddress;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Result;\n@@ -128,6 +128,11 @@\n     sc.setStopRow(tSplit.getEndRow());\n     trr.setScan(sc);\n     trr.setHTable(table);\n+    try {\n+      trr.initialize(tSplit, context);\n+    } catch (InterruptedException e) {\n+      throw new InterruptedIOException(e.getMessage());\n+    }\n     return trr;\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/dfc63998d20a956bf7a44ed6bcd7c495801fc4bf/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "sha": "6054b36ecf946010cc82c1c0923cf4f617e97db6",
                "status": "modified"
            }
        ],
        "message": "HBASE-6069 TableInputFormatBase#createRecordReader() doesn't initialize TableRecordReader which causes NPE (Jie Huang)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1341922 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/6a5244912a7a7ef2e1d9f4d10beee0ecc75216b6",
        "repo": "hbase",
        "unit_tests": [
            "TestTableInputFormatBase.java"
        ]
    },
    "hbase_e223639": {
        "bug_id": "hbase_e223639",
        "commit": "https://github.com/apache/hbase/commit/e2236396713cfc1cef61150a569e972c2faaca04",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/e2236396713cfc1cef61150a569e972c2faaca04/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java?ref=e2236396713cfc1cef61150a569e972c2faaca04",
                "deletions": 1,
                "filename": "hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java",
                "patch": "@@ -391,8 +391,8 @@ public void initialize() throws HBaseIOException {\n         HBASE_GROUP_LOADBALANCER_CLASS,\n         StochasticLoadBalancer.class, LoadBalancer.class);\n     internalBalancer = ReflectionUtils.newInstance(balancerKlass, config);\n-    internalBalancer.setClusterStatus(clusterStatus);\n     internalBalancer.setMasterServices(masterServices);\n+    internalBalancer.setClusterStatus(clusterStatus);\n     internalBalancer.setConf(config);\n     internalBalancer.initialize();\n   }",
                "raw_url": "https://github.com/apache/hbase/raw/e2236396713cfc1cef61150a569e972c2faaca04/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java",
                "sha": "c42c46d108ee627e8a813cbcf44e613b5d8f3067",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=e2236396713cfc1cef61150a569e972c2faaca04",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "patch": "@@ -742,8 +742,8 @@ private void finishActiveMasterInitialization(MonitoredTask status)\n     }\n \n     //initialize load balancer\n-    this.balancer.setClusterStatus(getClusterStatus());\n     this.balancer.setMasterServices(this);\n+    this.balancer.setClusterStatus(getClusterStatus());\n     this.balancer.initialize();\n \n     // Check if master is shutting down because of some issue",
                "raw_url": "https://github.com/apache/hbase/raw/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "sha": "eac2fa22f8d61abefebae80ce9873f6876822169",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java?ref=e2236396713cfc1cef61150a569e972c2faaca04",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "patch": "@@ -233,7 +233,7 @@ public synchronized void setClusterStatus(ClusterStatus st) {\n \n       updateMetricsSize(tablesCount * (functionsCount + 1)); // +1 for overall\n     } catch (Exception e) {\n-      LOG.error(\"failed to get the size of all tables, exception = \" + e.getMessage());\n+      LOG.error(\"failed to get the size of all tables\", e);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "sha": "b02aac1aadd76aab8d03cd1ea370bb7c64e8ea81",
                "status": "modified"
            }
        ],
        "message": "HBASE-16910 Avoid NPE when starting StochasticLoadBalancer (Guanghao Zhang)",
        "parent": "https://github.com/apache/hbase/commit/0ae211eb399e5524196d89af8eac1941c8b61b60",
        "repo": "hbase",
        "unit_tests": [
            "TestStochasticLoadBalancer.java"
        ]
    },
    "hbase_e86b13f": {
        "bug_id": "hbase_e86b13f",
        "commit": "https://github.com/apache/hbase/commit/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a",
                "deletions": 1,
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "patch": "@@ -718,7 +718,8 @@ private static long getSeqNumDuringOpen(final Result r, final int replicaId) {\n \n   /**\n    * Returns an HRegionLocationList extracted from the result.\n-   * @return an HRegionLocationList containing all locations for the region range\n+   * @return an HRegionLocationList containing all locations for the region range or null if\n+   *  we can't deserialize the result.\n    */\n   public static RegionLocations getRegionLocations(final Result r) {\n     if (r == null) return null;",
                "raw_url": "https://github.com/apache/hbase/raw/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "sha": "9517113fcc6763c1952dc38326a3c003c1a81159",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hbase/blob/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -54,6 +54,7 @@\n import org.apache.hadoop.hbase.HRegionLocation;\n import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.NotServingRegionException;\n+import org.apache.hadoop.hbase.RegionLocations;\n import org.apache.hadoop.hbase.RegionTransition;\n import org.apache.hadoop.hbase.Server;\n import org.apache.hadoop.hbase.ServerName;\n@@ -2760,7 +2761,13 @@ boolean waitUntilNoRegionsInTransition(final long timeout)\n     Set<ServerName> offlineServers = new HashSet<ServerName>();\n     // Iterate regions in META\n     for (Result result : results) {\n-      HRegionLocation[] locations = MetaReader.getRegionLocations(result).getRegionLocations();\n+      if (result == null && LOG.isDebugEnabled()){\n+        LOG.debug(\"null result from meta - ignoring but this is strange.\");\n+        continue;\n+      }\n+      RegionLocations rl =  MetaReader.getRegionLocations(result);\n+      if (rl == null) continue;\n+      HRegionLocation[] locations = rl.getRegionLocations();\n       if (locations == null) continue;\n       for (HRegionLocation hrl : locations) {\n         HRegionInfo regionInfo = hrl.getRegionInfo();",
                "raw_url": "https://github.com/apache/hbase/raw/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "f4075698228bf80af018ad7d4283e5a923e9c74f",
                "status": "modified"
            }
        ],
        "message": "HBASE-10957 HMaster can abort with NPE in #rebuildUserRegions (Nicolas Liochon)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/branches/hbase-10070@1590184 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/48ffa4d5e615c78f6db8f6c2beddd93460887642",
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_e95cf8f": {
        "bug_id": "hbase_e95cf8f",
        "commit": "https://github.com/apache/hbase/commit/e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hbase/blob/e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java?ref=e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "patch": "@@ -860,7 +860,13 @@ int getLeastLoadedTopServerForRegion(int region) {\n         int leastLoadedServerIndex = -1;\n         int load = Integer.MAX_VALUE;\n         for (ServerName sn : topLocalServers) {\n-          int index = serversToIndex.get(sn);\n+          if (!serversToIndex.containsKey(sn.getHostAndPort())) {\n+            continue;\n+          }\n+          int index = serversToIndex.get(sn.getHostAndPort());\n+          if (regionsPerServer[index] == null) {\n+            continue;\n+          }\n           int tempLoad = regionsPerServer[index].length;\n           if (tempLoad <= load) {\n             leastLoadedServerIndex = index;",
                "raw_url": "https://github.com/apache/hbase/raw/e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "sha": "962b241663fc320e92b62926faaea81d8435d4d7",
                "status": "modified"
            }
        ],
        "message": "HBASE-14291 NPE On StochasticLoadBalancer Balance Involving RS With No Regions",
        "parent": "https://github.com/apache/hbase/commit/902cd172f8a1ace4ad9fb35d4dfb7700fc10de21",
        "repo": "hbase",
        "unit_tests": [
            "TestBaseLoadBalancer.java"
        ]
    },
    "hbase_ea3c445": {
        "bug_id": "hbase_ea3c445",
        "commit": "https://github.com/apache/hbase/commit/ea3c445e2d73b2abcfb29f0e09c379551f86ee8f",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/ea3c445e2d73b2abcfb29f0e09c379551f86ee8f/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=ea3c445e2d73b2abcfb29f0e09c379551f86ee8f",
                "deletions": 1,
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "patch": "@@ -956,7 +956,7 @@ RegionPlan getRegionPlan(final RegionState state,\n     synchronized (this.regionPlans) {\n       existingPlan = this.regionPlans.get(encodedName);\n       if (existingPlan == null || forceNewPlan ||\n-          existingPlan.getDestination().equals(serverToExclude)) {\n+          (existingPlan != null && existingPlan.getDestination().equals(serverToExclude))) {\n         newPlan = true;\n         this.regionPlans.put(encodedName, randomPlan);\n       }",
                "raw_url": "https://github.com/apache/hbase/raw/ea3c445e2d73b2abcfb29f0e09c379551f86ee8f/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "sha": "c3fd667a1411dcfb4625507ca4b7fbfc0537efd6",
                "status": "modified"
            }
        ],
        "message": "Fix possible NPE in assign\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1042885 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/d547429a1442e48d9a3f69faaf9298b817a29967",
        "repo": "hbase",
        "unit_tests": [
            "TestAssignmentManager.java"
        ]
    },
    "hbase_f19f1d9": {
        "bug_id": "hbase_f19f1d9",
        "commit": "https://github.com/apache/hbase/commit/f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java?ref=f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f",
                "deletions": 5,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java",
                "patch": "@@ -253,10 +253,11 @@ protected void checkActiveSize() {\n       * in exclusive mode while this method (checkActiveSize) is invoked holding updatesLock\n       * in the shared mode. */\n       InMemoryFlushRunnable runnable = new InMemoryFlushRunnable();\n-      LOG.info(\"Dispatching the MemStore in-memory flush for store \" + store.getColumnFamilyName());\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(\n+          \"Dispatching the MemStore in-memory flush for store \" + store.getColumnFamilyName());\n+      }\n       getPool().execute(runnable);\n-      // guard against queuing same old compactions over and over again\n-      inMemoryFlushInProgress.set(true);\n     }\n   }\n \n@@ -277,10 +278,9 @@ void flushInMemory() throws IOException {\n     }\n     // Phase II: Compact the pipeline\n     try {\n-      if (allowCompaction.get()) {\n+      if (allowCompaction.get() && inMemoryFlushInProgress.compareAndSet(false, true)) {\n         // setting the inMemoryFlushInProgress flag again for the case this method is invoked\n         // directly (only in tests) in the common path setting from true to true is idempotent\n-        inMemoryFlushInProgress.set(true);\n         // Speculative compaction execution, may be interrupted if flush is forced while\n         // compaction is in progress\n         compactor.startCompaction();",
                "raw_url": "https://github.com/apache/hbase/raw/f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java",
                "sha": "ec5684dcda1fe49d6e5ff13eba43496244285e8d",
                "status": "modified"
            }
        ],
        "message": "HBASE-15999 NPE in MemstoreCompactor (Ram)",
        "parent": "https://github.com/apache/hbase/commit/158568e7806e461275406bc15856ba26e4660f4c",
        "repo": "hbase",
        "unit_tests": [
            "TestCompactingMemStore.java"
        ]
    },
    "hbase_f2820ea": {
        "bug_id": "hbase_f2820ea",
        "commit": "https://github.com/apache/hbase/commit/f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hbase/blob/f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java?ref=f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839",
                "deletions": 5,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "patch": "@@ -329,11 +329,16 @@ private void tryStartNewShipper(String walGroupId, PriorityBlockingQueue<Path> q\n       replicationDelay =\n           ReplicationLoad.calculateReplicationDelay(ageOfLastShippedOp, lastTimeStamp, queueSize);\n       Path currentPath = shipper.getCurrentPath();\n-      try {\n-        fileSize = getFileSize(currentPath);\n-      } catch (IOException e) {\n-        LOG.warn(\"Ignore the exception as the file size of HLog only affects the web ui\", e);\n-        fileSize = -1;\n+      fileSize = -1;\n+      if (currentPath != null) {\n+        try {\n+          fileSize = getFileSize(currentPath);\n+        } catch (IOException e) {\n+          LOG.warn(\"Ignore the exception as the file size of HLog only affects the web ui\", e);\n+        }\n+      } else {\n+        currentPath = new Path(\"NO_LOGS_IN_QUEUE\");\n+        LOG.warn(\"No replication ongoing, waiting for new log\");\n       }\n       ReplicationStatus.ReplicationStatusBuilder statusBuilder = ReplicationStatus.newBuilder();\n       statusBuilder.withPeerId(this.getPeerId())",
                "raw_url": "https://github.com/apache/hbase/raw/f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "sha": "f1b6e766877fe499a59dd6d6fd5f84c1ab9350ee",
                "status": "modified"
            }
        ],
        "message": "HBASE-21749 RS UI may throw NPE and make rs-status page inaccessible with multiwal and replication\n\nSigned-off-by: Andrew Purtell <apurtell@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/35df6147eef0baf25e127df358f6e50bbf967e2c",
        "repo": "hbase",
        "unit_tests": [
            "TestReplicationSource.java"
        ]
    },
    "hbase_f29260c": {
        "bug_id": "hbase_f29260c",
        "commit": "https://github.com/apache/hbase/commit/f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0/hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java?ref=f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0",
                "deletions": 0,
                "filename": "hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java",
                "patch": "@@ -591,6 +591,9 @@ private static WebAppContext createWebAppContext(String name,\n     ctx.setContextPath(\"/\");\n     ctx.setWar(appDir + \"/\" + name);\n     ctx.getServletContext().setAttribute(CONF_CONTEXT_ATTRIBUTE, conf);\n+    // for org.apache.hadoop.metrics.MetricsServlet\n+    ctx.getServletContext().setAttribute(\n+      org.apache.hadoop.http.HttpServer2.CONF_CONTEXT_ATTRIBUTE, conf);\n     ctx.getServletContext().setAttribute(ADMINS_ACL, adminsAcl);\n     addNoCacheFilter(ctx);\n     return ctx;",
                "raw_url": "https://github.com/apache/hbase/raw/f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0/hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java",
                "sha": "af72ab8c1e65f91310ddd479239dfdbeace4e9bf",
                "status": "modified"
            }
        ],
        "message": "HBASE-19424 Fix NPE in \"/metrics\" servlet.\n\nSigned-off-by: Apekshit Sharma <appy@apache.org>",
        "parent": "https://github.com/apache/hbase/commit/2509a150c0792e914429264453510b9028250c29",
        "repo": "hbase",
        "unit_tests": [
            "TestHttpServer.java"
        ]
    },
    "hbase_f440612": {
        "bug_id": "hbase_f440612",
        "commit": "https://github.com/apache/hbase/commit/f4406121af4ff600d2ff581e5dbe46ec34b06cca",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/f4406121af4ff600d2ff581e5dbe46ec34b06cca/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=f4406121af4ff600d2ff581e5dbe46ec34b06cca",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -139,6 +139,7 @@ Release 0.21.0 - Unreleased\n                its regions around\n    HBASE-2065  Cannot disable a table if any of its region is opening \n                at the same time\n+   HBASE-2026  NPE in StoreScanner on compaction\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "raw_url": "https://github.com/apache/hbase/raw/f4406121af4ff600d2ff581e5dbe46ec34b06cca/CHANGES.txt",
                "sha": "36591df27136788929bf6dbe97dad86e5176b692",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/f4406121af4ff600d2ff581e5dbe46ec34b06cca/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java?ref=f4406121af4ff600d2ff581e5dbe46ec34b06cca",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "patch": "@@ -255,6 +255,7 @@ public synchronized void updateReaders() throws IOException {\n \n     // Reset the state of the Query Matcher and set to top row\n     matcher.reset();\n-    matcher.setRow(heap.peek().getRow());\n+    KeyValue kv = heap.peek();\n+    matcher.setRow((kv == null ? topKey : kv).getRow());\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hbase/raw/f4406121af4ff600d2ff581e5dbe46ec34b06cca/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "sha": "d30432516294e98680fa5d4737e546cf3c0aa834",
                "status": "modified"
            }
        ],
        "message": "HBASE-2026  NPE in StoreScanner on compaction\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@894219 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/69a6ef5a9a8c09d8cf918751473240c5d77cfaae",
        "repo": "hbase",
        "unit_tests": [
            "TestStoreScanner.java"
        ]
    },
    "hbase_f6d44db": {
        "bug_id": "hbase_f6d44db",
        "commit": "https://github.com/apache/hbase/commit/f6d44db9c694281ab3df6854c83033cd7dc3d29c",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hbase/blob/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java?ref=f6d44db9c694281ab3df6854c83033cd7dc3d29c",
                "deletions": 1,
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "patch": "@@ -1130,9 +1130,12 @@ public String toString() {\n \n   /**\n    * @param k Key portion of a KeyValue.\n-   * @return Key as a String.\n+   * @return Key as a String, empty string if k is null. \n    */\n   public static String keyToString(final byte [] k) {\n+    if (k == null) { \n+      return \"\";\n+    }\n     return keyToString(k, 0, k.length);\n   }\n ",
                "raw_url": "https://github.com/apache/hbase/raw/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "sha": "06675b29d590c0106b0b9ad3fd47b63cae6984d0",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hbase/blob/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java?ref=f6d44db9c694281ab3df6854c83033cd7dc3d29c",
                "deletions": 1,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java",
                "patch": "@@ -350,7 +350,11 @@ private void printMeta(HFile.Reader reader, Map<byte[], byte[]> fileInfo)\n       }\n     }\n \n-    System.out.println(\"Mid-key: \" + Bytes.toStringBinary(reader.midkey()));\n+    try {\n+      System.out.println(\"Mid-key: \" + Bytes.toStringBinary(reader.midkey()));\n+    } catch (Exception e) {\n+      System.out.println (\"Unable to retrieve the midkey\");\n+    }\n \n     // Printing general bloom information\n     DataInput bloomMeta = reader.getGeneralBloomFilterMetadata();",
                "raw_url": "https://github.com/apache/hbase/raw/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java",
                "sha": "a7484e2c7a774e6a0dab8631beece000f4c19716",
                "status": "modified"
            }
        ],
        "message": "HBASE-9649 HFilePrettyPrinter should not throw a NPE if FirstKey or LastKey is null (Jean-Marc Spaggiari)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1526113 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/91508fd56478e73e125c2eb236338afd1c155f23",
        "repo": "hbase",
        "unit_tests": [
            "TestHFilePrettyPrinter.java"
        ]
    },
    "hbase_faefb90": {
        "bug_id": "hbase_faefb90",
        "commit": "https://github.com/apache/hbase/commit/faefb9073f388663df91d0ef2db24b00d6512519",
        "file": [
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hbase/blob/faefb9073f388663df91d0ef2db24b00d6512519/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java?ref=faefb9073f388663df91d0ef2db24b00d6512519",
                "deletions": 7,
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java",
                "patch": "@@ -211,14 +211,22 @@ protected PartitionedMobCompactionRequest select(List<FileStatus> candidates,\n     }\n     List<Path> newDelPaths = compactDelFiles(request, delFilePaths);\n     List<StoreFile> newDelFiles = new ArrayList<StoreFile>();\n-    for (Path newDelPath : newDelPaths) {\n-      StoreFile sf = new StoreFile(fs, newDelPath, conf, compactionCacheConfig, BloomType.NONE);\n-      newDelFiles.add(sf);\n+    List<Path> paths = null;\n+    try {\n+      for (Path newDelPath : newDelPaths) {\n+        StoreFile sf = new StoreFile(fs, newDelPath, conf, compactionCacheConfig, BloomType.NONE);\n+        // pre-create reader of a del file to avoid race condition when opening the reader in each\n+        // partition.\n+        sf.createReader();\n+        newDelFiles.add(sf);\n+      }\n+      LOG.info(\"After merging, there are \" + newDelFiles.size() + \" del files\");\n+      // compact the mob files by partitions.\n+      paths = compactMobFiles(request, newDelFiles);\n+      LOG.info(\"After compaction, there are \" + paths.size() + \" mob files\");\n+    } finally {\n+      closeStoreFileReaders(newDelFiles);\n     }\n-    LOG.info(\"After merging, there are \" + newDelFiles.size() + \" del files\");\n-    // compact the mob files by partitions.\n-    List<Path> paths = compactMobFiles(request, newDelFiles);\n-    LOG.info(\"After compaction, there are \" + paths.size() + \" mob files\");\n     // archive the del files if all the mob files are selected.\n     if (request.type == CompactionType.ALL_FILES && !newDelPaths.isEmpty()) {\n       LOG.info(\"After a mob compaction with all files selected, archiving the del files \"\n@@ -336,6 +344,20 @@ protected PartitionedMobCompactionRequest select(List<FileStatus> candidates,\n     return newFiles;\n   }\n \n+  /**\n+   * Closes the readers of store files.\n+   * @param storeFiles The store files to be closed.\n+   */\n+  private void closeStoreFileReaders(List<StoreFile> storeFiles) {\n+    for (StoreFile storeFile : storeFiles) {\n+      try {\n+        storeFile.closeReader(true);\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed to close the reader on store file \" + storeFile.getPath(), e);\n+      }\n+    }\n+  }\n+\n   /**\n    * Compacts a partition of selected small mob files and all the del files in a batch.\n    * @param request The compaction request.\n@@ -415,6 +437,7 @@ private void compactMobFilesInBatch(PartitionedMobCompactionRequest request,\n     }\n     // archive the old mob files, do not archive the del files.\n     try {\n+      closeStoreFileReaders(mobFilesToCompact);\n       MobUtils\n         .removeMobFiles(conf, fs, tableName, mobTableDir, column.getName(), mobFilesToCompact);\n     } catch (IOException e) {",
                "raw_url": "https://github.com/apache/hbase/raw/faefb9073f388663df91d0ef2db24b00d6512519/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java",
                "sha": "6c2ff01dd84426f3b63d0b4aac8cdc5e48deef12",
                "status": "modified"
            }
        ],
        "message": "HBASE-13855 Race in multi threaded PartitionedMobCompactor causes NPE. (Jingcheng)",
        "parent": "https://github.com/apache/hbase/commit/26893aa451215ef0395b7df16f129414b7b86c86",
        "repo": "hbase",
        "unit_tests": [
            "TestPartitionedMobCompactor.java"
        ]
    },
    "hbase_fbfe3f2": {
        "bug_id": "hbase_fbfe3f2",
        "commit": "https://github.com/apache/hbase/commit/fbfe3f29e5911146361e04f4514689e05257a491",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hbase/blob/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/HConstants.java?ref=fbfe3f29e5911146361e04f4514689e05257a491",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "patch": "@@ -77,13 +77,13 @@\n   public static final String HBASE_MASTER_LOADBALANCER_CLASS = \"hbase.master.loadbalancer.class\";\n \n   /** Cluster is standalone or pseudo-distributed */\n-  public static final String CLUSTER_IS_LOCAL = \"false\";\n+  public static final boolean CLUSTER_IS_LOCAL = false;\n \n   /** Cluster is fully-distributed */\n-  public static final String CLUSTER_IS_DISTRIBUTED = \"true\";\n+  public static final boolean CLUSTER_IS_DISTRIBUTED = true;\n \n   /** Default value for cluster distributed mode */  \n-  public static final String DEFAULT_CLUSTER_DISTRIBUTED = CLUSTER_IS_LOCAL;\n+  public static final boolean DEFAULT_CLUSTER_DISTRIBUTED = CLUSTER_IS_LOCAL;\n \n   /** default host address */\n   public static final String DEFAULT_HOST = \"0.0.0.0\";",
                "raw_url": "https://github.com/apache/hbase/raw/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "sha": "21ac4bad171c7444b41cf67dc89bcd2aa5c9db4b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java?ref=fbfe3f29e5911146361e04f4514689e05257a491",
                "deletions": 2,
                "filename": "src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "patch": "@@ -436,8 +436,8 @@ public void shutdown() {\n    * @return True if a 'local' address in hbase.master value.\n    */\n   public static boolean isLocal(final Configuration c) {\n-    final String mode = c.get(HConstants.CLUSTER_DISTRIBUTED);\n-    return mode == null || mode.equals(HConstants.CLUSTER_IS_LOCAL);\n+    boolean mode = c.getBoolean(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED);\n+    return(mode == HConstants.CLUSTER_IS_LOCAL);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hbase/raw/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "sha": "9c9c7cc441573ef872caa1fe8a7921a67894b479",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hbase/blob/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java?ref=fbfe3f29e5911146361e04f4514689e05257a491",
                "deletions": 3,
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "patch": "@@ -163,9 +163,8 @@ public static Properties parseZooCfg(Configuration conf,\n       }\n       // Special case for 'hbase.cluster.distributed' property being 'true'\n       if (key.startsWith(\"server.\")) {\n-        if (conf.get(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED).\n-              equals(HConstants.CLUSTER_IS_DISTRIBUTED)\n-            && value.startsWith(HConstants.LOCALHOST)) {\n+        boolean mode = conf.getBoolean(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED);\n+        if (mode == HConstants.CLUSTER_IS_DISTRIBUTED && value.startsWith(HConstants.LOCALHOST)) {\n           String msg = \"The server in zoo.cfg cannot be set to localhost \" +\n               \"in a fully-distributed setup because it won't be reachable. \" +\n               \"See \\\"Getting Started\\\" for more information.\";",
                "raw_url": "https://github.com/apache/hbase/raw/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "sha": "99c04bdc75734361a057ac2b4114491a1a2b26a6",
                "status": "modified"
            }
        ],
        "message": "HBASE-5638 Readability improvements on HBASE-5633: NPE reading ZK config in HBase (Matteo Bertozzi)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1307085 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hbase/commit/26de676dea8b829c4fe746da4f06bc4a4cd6c3aa",
        "repo": "hbase",
        "unit_tests": [
            "TestZKConfig.java"
        ]
    },
    "hbase_ffe4302": {
        "bug_id": "hbase_ffe4302",
        "commit": "https://github.com/apache/hbase/commit/ffe430237a44b1ebad31171bdf81ab7487ef5ce3",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hbase/blob/ffe430237a44b1ebad31171bdf81ab7487ef5ce3/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=ffe430237a44b1ebad31171bdf81ab7487ef5ce3",
                "deletions": 0,
                "filename": "hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "patch": "@@ -1189,6 +1189,7 @@ void updateValueSize(final int valueSize) {\n     }\n \n     void updateScanMetrics(final ScanMetrics metrics) {\n+      if (metrics == null) return;\n       Map<String,Long> metricsMap = metrics.getMetricsMap();\n       Long rpcCalls = metricsMap.get(ScanMetrics.RPC_CALLS_METRIC_NAME);\n       if (rpcCalls != null) {",
                "raw_url": "https://github.com/apache/hbase/raw/ffe430237a44b1ebad31171bdf81ab7487ef5ce3/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "sha": "1a72ececfcae0344534d92990be57657566fa6f6",
                "status": "modified"
            }
        ],
        "message": "HBASE-20785 NPE getting metrics in PE testing scans",
        "parent": "https://github.com/apache/hbase/commit/952bb96c8a5c1f8a34237cab970ac41101bdd870",
        "repo": "hbase",
        "unit_tests": [
            "TestPerformanceEvaluation.java"
        ]
    }
}