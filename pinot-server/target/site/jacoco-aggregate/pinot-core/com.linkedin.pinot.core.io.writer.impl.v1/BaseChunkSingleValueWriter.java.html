<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>BaseChunkSingleValueWriter.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">pinot-server</a> &gt; <a href="../index.html" class="el_bundle">pinot-core</a> &gt; <a href="index.source.html" class="el_package">com.linkedin.pinot.core.io.writer.impl.v1</a> &gt; <span class="el_source">BaseChunkSingleValueWriter.java</span></div><h1>BaseChunkSingleValueWriter.java</h1><pre class="source lang-java linenums">/**
 * Copyright (C) 2014-2016 LinkedIn Corp. (pinot-core@linkedin.com)
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.linkedin.pinot.core.io.writer.impl.v1;

import com.linkedin.pinot.core.io.compression.ChunkCompressor;
import com.linkedin.pinot.core.io.compression.ChunkCompressorFactory;
import com.linkedin.pinot.core.io.writer.SingleColumnSingleValueWriter;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


/**
 * Abstract implementation for {@link SingleColumnSingleValueWriter}
 * Base class for fixed and variable byte writer implementations.
 */
public abstract class BaseChunkSingleValueWriter implements SingleColumnSingleValueWriter {
<span class="fc" id="L36">  private static final Logger LOGGER = LoggerFactory.getLogger(BaseChunkSingleValueWriter.class);</span>

  protected static final int INT_SIZE = Integer.SIZE / Byte.SIZE;
  protected static final int LONG_SIZE = Long.SIZE / Byte.SIZE;
  protected static final int FLOAT_SIZE = Float.SIZE / Byte.SIZE;
  protected static final int DOUBLE_SIZE = Double.SIZE / Byte.SIZE;

  protected final FileChannel _dataFile;
  protected ByteBuffer _header;
  protected final ByteBuffer _chunkBuffer;
  protected final ByteBuffer _compressedBuffer;
  protected final ChunkCompressor _chunkCompressor;

  protected int _chunkSize;
  protected int _dataOffset;

  /**
   * Constructor for the class.
   *
   * @param file Data file to write into
   * @param compressionType Type of compression
   * @param totalDocs Total docs to write
   * @param numDocsPerChunk Number of docs per data chunk
   * @param chunkSize Size of chunk
   * @param sizeOfEntry Size of entry (in bytes), max size for variable byte implementation.
   * @param version Version of file
   * @throws FileNotFoundException
   */
  protected BaseChunkSingleValueWriter(File file, ChunkCompressorFactory.CompressionType compressionType, int totalDocs,
      int numDocsPerChunk, int chunkSize, int sizeOfEntry, int version)
<span class="fc" id="L66">      throws FileNotFoundException {</span>
<span class="fc" id="L67">    _chunkSize = chunkSize;</span>
<span class="fc" id="L68">    _chunkCompressor = ChunkCompressorFactory.getCompressor(compressionType);</span>

<span class="fc" id="L70">    _dataOffset = writeHeader(compressionType, totalDocs, numDocsPerChunk, sizeOfEntry, version);</span>
<span class="fc" id="L71">    _chunkBuffer = ByteBuffer.allocateDirect(chunkSize);</span>
<span class="fc" id="L72">    _compressedBuffer = ByteBuffer.allocateDirect(chunkSize * 2);</span>
<span class="fc" id="L73">    _dataFile = new RandomAccessFile(file, &quot;rw&quot;).getChannel();</span>
<span class="fc" id="L74">  }</span>

  @Override
  public void setChar(int row, char ch) {
<span class="nc" id="L78">    throw new UnsupportedOperationException();</span>
  }

  @Override
  public void setInt(int row, int i) {
<span class="nc" id="L83">    throw new UnsupportedOperationException();</span>
  }

  @Override
  public void setShort(int row, short s) {
<span class="nc" id="L88">    throw new UnsupportedOperationException();</span>
  }

  @Override
  public void setLong(int row, long l) {
<span class="nc" id="L93">    throw new UnsupportedOperationException();</span>
  }

  @Override
  public void setFloat(int row, float f) {
<span class="nc" id="L98">    throw new UnsupportedOperationException();</span>
  }

  @Override
  public void setDouble(int row, double d) {
<span class="nc" id="L103">    throw new UnsupportedOperationException();</span>
  }

  @Override
  public void setString(int row, String string) {
<span class="nc" id="L108">    throw new UnsupportedOperationException();</span>
  }

  @Override
  public void setBytes(int row, byte[] bytes) {
<span class="nc" id="L113">    throw new UnsupportedOperationException();</span>
  }

  @Override
  public void close()
      throws IOException {

    // Write the chunk if it is non-empty.
<span class="fc bfc" id="L121" title="All 2 branches covered.">    if (_chunkBuffer.position() &gt; 0) {</span>
<span class="fc" id="L122">      writeChunk();</span>
    }

    // Write the header and close the file.
<span class="fc" id="L126">    _header.flip();</span>
<span class="fc" id="L127">    _dataFile.write(_header, 0);</span>
<span class="fc" id="L128">    _dataFile.close();</span>
<span class="fc" id="L129">  }</span>

  /**
   * Helper method to write header information.
   *
   * @param compressionType Compression type for the data
   * @param totalDocs Total number of records
   * @param numDocsPerChunk Number of documents per chunk
   * @param sizeOfEntry Size of each entry
   * @param version Version of file
   * @return Size of header
   */
  private int writeHeader(ChunkCompressorFactory.CompressionType compressionType, int totalDocs, int numDocsPerChunk,
      int sizeOfEntry, int version) {
<span class="fc" id="L143">    int numChunks = (totalDocs + numDocsPerChunk - 1) / numDocsPerChunk;</span>
<span class="fc" id="L144">    int headerSize = (numChunks + 7) * INT_SIZE; // 7 items written before chunk indexing.</span>

<span class="fc" id="L146">    _header = ByteBuffer.allocateDirect(headerSize);</span>

<span class="fc" id="L148">    int offset = 0;</span>
<span class="fc" id="L149">    _header.putInt(version);</span>
<span class="fc" id="L150">    offset += INT_SIZE;</span>

<span class="fc" id="L152">    _header.putInt(numChunks);</span>
<span class="fc" id="L153">    offset += INT_SIZE;</span>

<span class="fc" id="L155">    _header.putInt(numDocsPerChunk);</span>
<span class="fc" id="L156">    offset += INT_SIZE;</span>

<span class="fc" id="L158">    _header.putInt(sizeOfEntry);</span>
<span class="fc" id="L159">    offset += INT_SIZE;</span>

<span class="pc bpc" id="L161" title="1 of 2 branches missed.">    if (version &gt; 1) {</span>
      // Write total number of docs.
<span class="fc" id="L163">      _header.putInt(totalDocs);</span>
<span class="fc" id="L164">      offset += INT_SIZE;</span>

      // Write the compressor type
<span class="fc" id="L167">      _header.putInt(compressionType.getValue());</span>
<span class="fc" id="L168">      offset += INT_SIZE;</span>

      // Start of chunk offsets.
<span class="fc" id="L171">      int dataHeaderStart = offset + INT_SIZE;</span>
<span class="fc" id="L172">      _header.putInt(dataHeaderStart);</span>
    }

<span class="fc" id="L175">    return headerSize;</span>
  }

  /**
   * Helper method to compress and write the current chunk.
   * &lt;ul&gt;
   *   &lt;li&gt; Chunk header is of fixed size, so fills out any remaining offsets for partially filled chunks. &lt;/li&gt;
   *   &lt;li&gt; Compresses (if required) and writes the chunk to the data file. &lt;/li&gt;
   *   &lt;li&gt; Updates the header with the current chunks offset. &lt;/li&gt;
   *   &lt;li&gt; Clears up the buffers, so that they can be reused. &lt;/li&gt;
   * &lt;/ul&gt;
   *
   */
  protected void writeChunk() {
    int sizeToWrite;
<span class="fc" id="L190">    _chunkBuffer.flip();</span>

    try {
<span class="fc" id="L193">      sizeToWrite = _chunkCompressor.compress(_chunkBuffer, _compressedBuffer);</span>
<span class="fc" id="L194">      _dataFile.write(_compressedBuffer, _dataOffset);</span>
<span class="fc" id="L195">      _compressedBuffer.clear();</span>
<span class="nc" id="L196">    } catch (IOException e) {</span>
<span class="nc" id="L197">      LOGGER.error(&quot;Exception caught while compressing/writing data chunk&quot;, e);</span>
<span class="nc" id="L198">      throw new RuntimeException(e);</span>
<span class="fc" id="L199">    }</span>

<span class="fc" id="L201">    _header.putInt(_dataOffset);</span>
<span class="fc" id="L202">    _dataOffset += sizeToWrite;</span>

<span class="fc" id="L204">    _chunkBuffer.clear();</span>
<span class="fc" id="L205">  }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.eclemma.org/jacoco">JaCoCo</a> 0.7.7.201606060606</span></div></body></html>