<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>RealtimeTableDataManager.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">pinot-minion</a> &gt; <a href="../index.html" class="el_bundle">pinot-core</a> &gt; <a href="index.source.html" class="el_package">com.linkedin.pinot.core.data.manager.realtime</a> &gt; <span class="el_source">RealtimeTableDataManager.java</span></div><h1>RealtimeTableDataManager.java</h1><pre class="source lang-java linenums">/**
 * Copyright (C) 2014-2016 LinkedIn Corp. (pinot-core@linkedin.com)
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.linkedin.pinot.core.data.manager.realtime;

import com.google.common.base.Preconditions;
import com.linkedin.pinot.common.Utils;
import com.linkedin.pinot.common.config.IndexingConfig;
import com.linkedin.pinot.common.config.TableConfig;
import com.linkedin.pinot.common.data.FieldSpec;
import com.linkedin.pinot.common.data.Schema;
import com.linkedin.pinot.common.metadata.ZKMetadataProvider;
import com.linkedin.pinot.common.metadata.instance.InstanceZKMetadata;
import com.linkedin.pinot.common.metadata.segment.LLCRealtimeSegmentZKMetadata;
import com.linkedin.pinot.common.metadata.segment.RealtimeSegmentZKMetadata;
import com.linkedin.pinot.common.segment.fetcher.SegmentFetcherFactory;
import com.linkedin.pinot.common.utils.CommonConstants.Segment.Realtime.Status;
import com.linkedin.pinot.common.utils.NamedThreadFactory;
import com.linkedin.pinot.common.utils.SegmentName;
import com.linkedin.pinot.common.utils.TarGzCompressionUtils;
import com.linkedin.pinot.core.data.manager.offline.AbstractTableDataManager;
import com.linkedin.pinot.core.data.manager.offline.SegmentDataManager;
import com.linkedin.pinot.core.indexsegment.IndexSegment;
import com.linkedin.pinot.core.indexsegment.columnar.ColumnarSegmentLoader;
import com.linkedin.pinot.core.realtime.impl.RealtimeSegmentStatsHistory;
import com.linkedin.pinot.core.realtime.impl.kafka.KafkaConsumerManager;
import com.linkedin.pinot.core.segment.index.loader.IndexLoadingConfig;
import com.linkedin.pinot.core.segment.index.loader.LoaderUtils;
import java.io.File;
import java.io.FilenameFilter;
import java.io.IOException;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Semaphore;
import javax.annotation.Nonnull;
import org.apache.commons.io.FileUtils;


<span class="nc" id="L53">public class RealtimeTableDataManager extends AbstractTableDataManager {</span>
<span class="nc" id="L54">  private final ExecutorService _segmentAsyncExecutorService =</span>
      Executors.newSingleThreadExecutor(new NamedThreadFactory(&quot;SegmentAsyncExecutorService&quot;));
  private SegmentBuildTimeLeaseExtender _leaseExtender;
  private RealtimeSegmentStatsHistory _statsHistory;
  private Semaphore _segmentBuildSemaphore;

  private static final String STATS_FILE_NAME = &quot;stats.ser&quot;;
  private static final String CONSUMERS_DIR = &quot;consumers&quot;;

  @Override
  protected void doInit() {
<span class="nc" id="L65">    _leaseExtender = SegmentBuildTimeLeaseExtender.create(_instanceId);</span>
<span class="nc" id="L66">    int maxParallelBuilds = _tableDataManagerConfig.getMaxParallelSegmentBuilds();</span>
<span class="nc bnc" id="L67" title="All 2 branches missed.">    if (maxParallelBuilds &gt; 0) {</span>
<span class="nc" id="L68">      _segmentBuildSemaphore = new Semaphore(maxParallelBuilds, true);</span>
    }

<span class="nc" id="L71">    File statsFile = new File(_tableDataDir, STATS_FILE_NAME);</span>
    try {
<span class="nc" id="L73">      _statsHistory = RealtimeSegmentStatsHistory.deserialzeFrom(statsFile);</span>
<span class="nc" id="L74">    } catch (IOException | ClassNotFoundException e) {</span>
<span class="nc" id="L75">      _logger.error(&quot;Error reading history object for table {} from {}&quot;, _tableName, statsFile.getAbsolutePath(), e);</span>
<span class="nc" id="L76">      File savedFile = new File(_tableDataDir, STATS_FILE_NAME + &quot;.&quot; + UUID.randomUUID());</span>
      try {
<span class="nc" id="L78">        FileUtils.moveFile(statsFile, savedFile);</span>
<span class="nc" id="L79">      } catch (IOException e1) {</span>
<span class="nc" id="L80">        _logger.error(&quot;Could not move {} to {}&quot;, statsFile.getAbsolutePath(), savedFile.getAbsolutePath(), e1);</span>
<span class="nc" id="L81">        throw new RuntimeException(e);</span>
<span class="nc" id="L82">      }</span>
<span class="nc" id="L83">      _logger.warn(&quot;Saved unreadable {} into {}. Creating a fresh instance&quot;, statsFile.getAbsolutePath(),</span>
          savedFile.getAbsolutePath());
      try {
<span class="nc" id="L86">        _statsHistory = RealtimeSegmentStatsHistory.deserialzeFrom(statsFile);</span>
<span class="nc" id="L87">      } catch (Exception e2) {</span>
<span class="nc" id="L88">        Utils.rethrowException(e2);</span>
<span class="nc" id="L89">      }</span>
<span class="nc" id="L90">    }</span>

<span class="nc" id="L92">    String consumerDirPath = getConsumerDir();</span>
<span class="nc" id="L93">    File consumerDir = new File(consumerDirPath);</span>

<span class="nc bnc" id="L95" title="All 2 branches missed.">    if (consumerDir.exists()) {</span>
<span class="nc" id="L96">      File[] segmentFiles = consumerDir.listFiles(new FilenameFilter() {</span>
        @Override
        public boolean accept(File dir, String name) {
<span class="nc bnc" id="L99" title="All 2 branches missed.">          return !name.equals(STATS_FILE_NAME);</span>
        }
      });
<span class="nc bnc" id="L102" title="All 2 branches missed.">      for (File file : segmentFiles) {</span>
<span class="nc bnc" id="L103" title="All 2 branches missed.">        if (file.delete()) {</span>
<span class="nc" id="L104">          _logger.info(&quot;Deleted old file {}&quot;, file.getAbsolutePath());</span>
        } else {
<span class="nc" id="L106">          _logger.error(&quot;Cannot delete file {}&quot;, file.getAbsolutePath());</span>
        }
      }
    }
<span class="nc" id="L110">  }</span>

  @Override
  protected void doShutdown() {
<span class="nc" id="L114">    _segmentAsyncExecutorService.shutdown();</span>
<span class="nc bnc" id="L115" title="All 2 branches missed.">    for (SegmentDataManager segmentDataManager : _segmentsMap.values()) {</span>
<span class="nc" id="L116">      segmentDataManager.destroy();</span>
<span class="nc" id="L117">    }</span>
<span class="nc" id="L118">    KafkaConsumerManager.closeAllConsumers();</span>
<span class="nc bnc" id="L119" title="All 2 branches missed.">    if (_leaseExtender != null) {</span>
<span class="nc" id="L120">      _leaseExtender.shutDown();</span>
    }
<span class="nc" id="L122">  }</span>

  public RealtimeSegmentStatsHistory getStatsHistory() {
<span class="nc" id="L125">    return _statsHistory;</span>
  }

  public Semaphore getSegmentBuildSemaphore() {
<span class="nc" id="L129">    return _segmentBuildSemaphore;</span>
  }

  public String getConsumerDir() {
<span class="nc" id="L133">    String consumerDirPath = _tableDataManagerConfig.getConsumerDir();</span>
    File consumerDir;
    // If a consumer directory has been configured, use it to create a per-table path under the consumer dir.
    // Otherwise, create a sub-dir under the table-specific data director and use it for consumer mmaps
<span class="nc bnc" id="L137" title="All 2 branches missed.">    if (consumerDirPath != null) {</span>
<span class="nc" id="L138">       consumerDir = new File(consumerDirPath, _tableName);</span>
    } else {
<span class="nc" id="L140">      consumerDirPath = _tableDataDir + File.separator + CONSUMERS_DIR;</span>
<span class="nc" id="L141">      consumerDir = new File(consumerDirPath);</span>
    }

<span class="nc bnc" id="L144" title="All 2 branches missed.">    if (!consumerDir.exists()) {</span>
<span class="nc bnc" id="L145" title="All 2 branches missed.">      if (!consumerDir.mkdirs()) {</span>
<span class="nc" id="L146">        _logger.error(&quot;Failed to create consumer directory {}&quot;, consumerDir.getAbsolutePath());</span>
      }
    }

<span class="nc" id="L150">    return consumerDir.getAbsolutePath();</span>
  }

  public void notifySegmentCommitted(RealtimeSegmentZKMetadata metadata, IndexSegment segment) {
<span class="nc" id="L154">    ZKMetadataProvider.setRealtimeSegmentZKMetadata(_propertyStore, metadata);</span>
<span class="nc" id="L155">    addSegment(segment);</span>
<span class="nc" id="L156">  }</span>

  /*
   * This call comes in one of two ways:
   * For HL Segments:
   * - We are being directed by helix to own up all the segments that we committed and are still in retention. In this case
   *   we treat it exactly like how OfflineTableDataManager would -- wrap it into an OfflineSegmentDataManager, and put it
   *   in the map.
   * - We are being asked to own up a new realtime segment. In this case, we wrap the segment with a RealTimeSegmentDataManager
   *   (that kicks off Kafka consumption). When the segment is committed we get notified via the notifySegmentCommitted call, at
   *   which time we replace the segment with the OfflineSegmentDataManager
   * For LL Segments:
   * - We are being asked to start consuming from a kafka partition.
   * - We did not know about the segment and are being asked to download and own the segment (re-balancing, or
   *   replacing a realtime server with a fresh one, maybe). We need to look at segment metadata and decide whether
   *   to start consuming or download the segment.
   */
  @Override
  public void addSegment(@Nonnull String segmentName, @Nonnull TableConfig tableConfig,
      @Nonnull IndexLoadingConfig indexLoadingConfig) throws Exception {
<span class="nc" id="L176">    RealtimeSegmentZKMetadata realtimeSegmentZKMetadata =</span>
        ZKMetadataProvider.getRealtimeSegmentZKMetadata(_propertyStore, _tableName, segmentName);
<span class="nc" id="L178">    Preconditions.checkNotNull(realtimeSegmentZKMetadata);</span>

<span class="nc" id="L180">    File indexDir = new File(_indexDir, segmentName);</span>
    // Restart during segment reload might leave segment in inconsistent state (index directory might not exist but
    // segment backup directory existed), need to first try to recover from reload failure before checking the existence
    // of the index directory and loading segment from it
<span class="nc" id="L184">    LoaderUtils.reloadFailureRecovery(indexDir);</span>
<span class="nc bnc" id="L185" title="All 4 branches missed.">    if (indexDir.exists() &amp;&amp; (realtimeSegmentZKMetadata.getStatus() == Status.DONE)) {</span>
      // segment already exists on file, and we have committed the realtime segment in ZK. Treat it like an offline segment
<span class="nc bnc" id="L187" title="All 2 branches missed.">      if (_segmentsMap.containsKey(segmentName)) {</span>
<span class="nc" id="L188">        _logger.warn(&quot;Got reload for segment already on disk {} table {}, have {}&quot;, segmentName, _tableName,</span>
            _segmentsMap.get(segmentName).getClass().getSimpleName());
<span class="nc" id="L190">        return;</span>
      }

<span class="nc" id="L193">      IndexSegment segment = ColumnarSegmentLoader.load(indexDir, indexLoadingConfig);</span>
<span class="nc" id="L194">      addSegment(segment);</span>
<span class="nc" id="L195">    } else {</span>
      // Either we don't have the segment on disk or we have not committed in ZK. We should be starting the consumer
      // for realtime segment here. If we wrote it on disk but could not get to commit to zk yet, we should replace the
      // on-disk segment next time
<span class="nc bnc" id="L199" title="All 2 branches missed.">      if (_segmentsMap.containsKey(segmentName)) {</span>
<span class="nc" id="L200">        _logger.warn(&quot;Got reload for segment not on disk {} table {}, have {}&quot;, segmentName, _tableName,</span>
            _segmentsMap.get(segmentName).getClass().getSimpleName());
<span class="nc" id="L202">        return;</span>
      }
<span class="nc" id="L204">      Schema schema = ZKMetadataProvider.getTableSchema(_propertyStore, _tableName);</span>
<span class="nc" id="L205">      Preconditions.checkNotNull(schema);</span>
<span class="nc bnc" id="L206" title="All 2 branches missed.">      if (!isValid(schema, tableConfig.getIndexingConfig())) {</span>
<span class="nc" id="L207">        _logger.error(&quot;Not adding segment {}&quot;, segmentName);</span>
<span class="nc" id="L208">        throw new RuntimeException(&quot;Mismatching schema/table config for &quot; + _tableName);</span>
      }

<span class="nc" id="L211">      InstanceZKMetadata instanceZKMetadata = ZKMetadataProvider.getInstanceZKMetadata(_propertyStore, _instanceId);</span>
      SegmentDataManager manager;
<span class="nc bnc" id="L213" title="All 2 branches missed.">      if (SegmentName.isHighLevelConsumerSegmentName(segmentName)) {</span>
<span class="nc" id="L214">        manager = new HLRealtimeSegmentDataManager(realtimeSegmentZKMetadata, tableConfig, instanceZKMetadata, this,</span>
            _indexDir.getAbsolutePath(), indexLoadingConfig, schema, _serverMetrics);
      } else {
<span class="nc" id="L217">        LLCRealtimeSegmentZKMetadata llcSegmentMetadata = (LLCRealtimeSegmentZKMetadata) realtimeSegmentZKMetadata;</span>
<span class="nc bnc" id="L218" title="All 2 branches missed.">        if (realtimeSegmentZKMetadata.getStatus().equals(Status.DONE)) {</span>
          // TODO Remove code duplication here and in LLRealtimeSegmentDataManager
<span class="nc" id="L220">          downloadAndReplaceSegment(segmentName, llcSegmentMetadata, indexLoadingConfig);</span>
<span class="nc" id="L221">          return;</span>
        }
<span class="nc" id="L223">        manager = new LLRealtimeSegmentDataManager(realtimeSegmentZKMetadata, tableConfig, instanceZKMetadata, this,</span>
            _indexDir.getAbsolutePath(), indexLoadingConfig, schema, _serverMetrics);
      }
<span class="nc" id="L226">      _logger.info(&quot;Initialize RealtimeSegmentDataManager - &quot; + segmentName);</span>
      try {
<span class="nc" id="L228">        _rwLock.writeLock().lock();</span>
<span class="nc" id="L229">        _segmentsMap.put(segmentName, manager);</span>
      } finally {
<span class="nc" id="L231">        _rwLock.writeLock().unlock();</span>
<span class="nc" id="L232">      }</span>
    }
<span class="nc" id="L234">  }</span>

  public void downloadAndReplaceSegment(@Nonnull String segmentName,
      @Nonnull LLCRealtimeSegmentZKMetadata llcSegmentMetadata, @Nonnull IndexLoadingConfig indexLoadingConfig) {
<span class="nc" id="L238">    final String uri = llcSegmentMetadata.getDownloadUrl();</span>
<span class="nc" id="L239">    File tempSegmentFolder =</span>
        new File(_indexDir, &quot;tmp-&quot; + segmentName + &quot;.&quot; + String.valueOf(System.currentTimeMillis()));
<span class="nc" id="L241">    File tempFile = new File(_indexDir, segmentName + &quot;.tar.gz&quot;);</span>
    try {
<span class="nc" id="L243">      SegmentFetcherFactory.getInstance().getSegmentFetcherBasedOnURI(uri).fetchSegmentToLocal(uri, tempFile);</span>
<span class="nc" id="L244">      _logger.info(&quot;Downloaded file from {} to {}; Length of downloaded file: {}&quot;, uri, tempFile, tempFile.length());</span>
<span class="nc" id="L245">      TarGzCompressionUtils.unTar(tempFile, tempSegmentFolder);</span>
<span class="nc" id="L246">      _logger.info(&quot;Uncompressed file {} into tmp dir {}&quot;, tempFile, tempSegmentFolder);</span>
<span class="nc" id="L247">      FileUtils.moveDirectory(tempSegmentFolder.listFiles()[0], new File(_indexDir, segmentName));</span>
<span class="nc" id="L248">      _logger.info(&quot;Replacing LLC Segment {}&quot;, segmentName);</span>
<span class="nc" id="L249">      replaceLLSegment(segmentName, indexLoadingConfig);</span>
<span class="nc" id="L250">    } catch (Exception e) {</span>
<span class="nc" id="L251">      throw new RuntimeException(e);</span>
    } finally {
<span class="nc" id="L253">      FileUtils.deleteQuietly(tempFile);</span>
<span class="nc" id="L254">      FileUtils.deleteQuietly(tempSegmentFolder);</span>
<span class="nc" id="L255">    }</span>
<span class="nc" id="L256">  }</span>

  // Replace a committed segment.
  public void replaceLLSegment(@Nonnull String segmentName, @Nonnull IndexLoadingConfig indexLoadingConfig) {
    try {
<span class="nc" id="L261">      IndexSegment indexSegment = ColumnarSegmentLoader.load(new File(_indexDir, segmentName), indexLoadingConfig);</span>
<span class="nc" id="L262">      addSegment(indexSegment);</span>
<span class="nc" id="L263">    } catch (Exception e) {</span>
<span class="nc" id="L264">      throw new RuntimeException(e);</span>
<span class="nc" id="L265">    }</span>
<span class="nc" id="L266">  }</span>

  public String getServerInstance() {
<span class="nc" id="L269">    return _instanceId;</span>
  }

  /**
   * Validate a schema against the table config for real-time record consumption.
   * Ideally, we should validate these things when schema is added or table is created, but either of these
   * may be changed while the table is already provisioned. For the change to take effect, we need to restart the
   * servers, so  validation at this place is fine.
   *
   * As of now, the following validations are done:
   * 1. Make sure that the sorted column, if specified, is not multi-valued.
   * 2. Validate the schema itself
   *
   * We allow the user to specify multiple sorted columns, but only consider the first one for now.
   * (secondary sort is not yet implemented).
   *
   * If we add more validations, it may make sense to split this method into multiple validation methods.
   * But then, we are trying to figure out all the invalid cases before we return from this method...
   *
   * @param schema
   * @param indexingConfig
   * @return true if schema is valid.
   */
  private boolean isValid(Schema schema, IndexingConfig indexingConfig) {
    // 1. Make sure that the sorted column is not a multi-value field.
<span class="nc" id="L294">    List&lt;String&gt; sortedColumns = indexingConfig.getSortedColumn();</span>
<span class="nc" id="L295">    boolean isValid = true;</span>
<span class="nc bnc" id="L296" title="All 2 branches missed.">    if (!sortedColumns.isEmpty()) {</span>
<span class="nc" id="L297">      final String sortedColumn = sortedColumns.get(0);</span>
<span class="nc bnc" id="L298" title="All 2 branches missed.">      if (sortedColumns.size() &gt; 1) {</span>
<span class="nc" id="L299">        _logger.warn(&quot;More than one sorted column configured. Using {}&quot;, sortedColumn);</span>
      }
<span class="nc" id="L301">      FieldSpec fieldSpec = schema.getFieldSpecFor(sortedColumn);</span>
<span class="nc bnc" id="L302" title="All 2 branches missed.">      if (!fieldSpec.isSingleValueField()) {</span>
<span class="nc" id="L303">        _logger.error(&quot;Cannot configure multi-valued column {} as sorted column&quot;, sortedColumn);</span>
<span class="nc" id="L304">        isValid = false;</span>
      }
    }
    // 2. We want to get the schema errors, if any, even if isValid is false;
<span class="nc bnc" id="L308" title="All 2 branches missed.">    if (!schema.validate(_logger)) {</span>
<span class="nc" id="L309">      isValid = false;</span>
    }

<span class="nc" id="L312">    return isValid;</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.eclemma.org/jacoco">JaCoCo</a> 0.7.7.201606060606</span></div></body></html>